{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trace Generation and Modeling\n",
    "\n",
    "To test our DeepValidate approach we generate a dataset of test traces from a chain of relatively simple arithmetical functions operating on a series of randomized inputs. Given the generated program traces, we train a LSTM classifier to predict whether the output will be valid or result in an error. \n",
    "\n",
    "The trace generation is performed by `output_trace.jl` which reproduces much of the functionality of `varextract.jl` with some important differences. Rather than send trace information to `stdout`, we direct the traces to a file `traces.dat`. This raw output is then processed into a CSV of traces (minus the error dumps we want to predict) and a CSV of binary (0, 1) labels indicating whether the run resulted in an error. \n",
    "\n",
    "(It must be noted that this is not possible within an IJulia notebook due to restrictions on [task switching in staged functions](https://github.com/JuliaLang/julia/issues/18568) which prevents the trace outputs from being written to a file recursively. However, this works just fine from the command line.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Cassette.overdub(ctx::TraceCtx,\n",
    "                          f,\n",
    "                          args...)\n",
    "    open(\"traces.dat\", \"a\") do file\n",
    "        write(file, string(f))\n",
    "        write(file, string(args))\n",
    "    end\n",
    "    \n",
    "    # if we are supposed to descend, we call Cassette.recurse\n",
    "    if Cassette.canrecurse(ctx, f, args...)\n",
    "        subtrace = (Any[],Any[])\n",
    "        push!(ctx.metadata[1], (f, args) => subtrace)\n",
    "        newctx = Cassette.similarcontext(ctx, metadata = subtrace)\n",
    "        retval = Cassette.recurse(newctx, f, args...)\n",
    "        # push!(ctx.metadata[2], subtrace[2])\n",
    "    else\n",
    "        retval = Cassette.fallback(ctx, f, args...)\n",
    "        push!(ctx.metadata[1], :t)\n",
    "        push!(ctx.metadata[2], retval)\n",
    "    end\n",
    "    @info \"returning\"\n",
    "    @show retval\n",
    "    return retval\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then modify our `@textset` so that it creates the `traces.dat` file and then loops through a large number of randomized runs of our arithmetic tests. Error conditions happen most often when our inputs are sufficiently close to zero, so a Normal(0,2) distribution gives us a good range of values to generate a reasonable percentage of \"bad\" traces on which to train. Empirically the share of \"bad\" traces generated is about 15-17%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"TraceExtract\" begin\n",
    "    g(x) = begin\n",
    "        y = add(x.*x, -x)\n",
    "        z = 1\n",
    "        v = y .- z\n",
    "        s = sum(v)\n",
    "        return s\n",
    "    end\n",
    "    h(x) = begin\n",
    "        z = g(x)\n",
    "        zed = sqrt(z)\n",
    "        return zed\n",
    "    end\n",
    "\n",
    "    open(\"traces.dat\", \"w\") do f\n",
    "        write(f, \"\")\n",
    "    end\n",
    "\n",
    "    seeds = rand(Normal(0,2),30000,3)\n",
    "    \n",
    "    for i=1:size(seeds,1)\n",
    "        ctx = TraceCtx(pass=ExtractPass, metadata = (Any[], Any[]))\n",
    "        try\n",
    "            result = Cassette.overdub(ctx, h, seeds[i,:])\n",
    "        catch DomainError\n",
    "            dump(ctx.metadata)\n",
    "        finally\n",
    "            open(\"traces.dat\", \"a\") do f\n",
    "                write(f, \"\\n\")\n",
    "            end\n",
    "        end\n",
    "        if i%1000 == 0\n",
    "            @info string(i)\n",
    "        end\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating our raw traces, a small amount of pre-processing is required before attempting to model around them. First, we classify our \"good\" and \"bad\" traces based on whether they have resulted in an error. \n",
    "\n",
    "We then need to strip out the actual error dump information from our \"bad\" traces, as this would too easily give away the prediction game. All traces end just before they would error, allowing the validation model to predict the that next outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = split(String(read(\"traces.dat\")), \"\\n\");\n",
    "Ys = Int.(occursin.(Ref(r\"(Base[\\S(?!\\))]+error)\"i), text));\n",
    "\n",
    "text = split.(text, Ref(r\"(Base[\\S(?!\\))]+error)\"i));\n",
    "text = [t[1] for t in text];\n",
    "\n",
    "sum(Ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save our traces and our labels our as CSV files for easy ingestion for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writedlm( \"traces.csv\",  text[1:end-1], ',')\n",
    "writedlm( \"y_results.csv\",  Ys[1:end-1], ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Classifier Model\n",
    "For our modeling, we use [Flux.jl](https://github.com/FluxML/Flux.jl) and train an LSTM encoder/decoder classifier on our traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DelimitedFiles\n",
    "using Flux\n",
    "using Flux: onehot, throttle, crossentropy, onehotbatch, params, shuffle\n",
    "using MLDataPattern: stratifiedobs\n",
    "using Base.Iterators: partition\n",
    "\n",
    "include(\"../../src/validation/utils.jl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Set up inputs for model\n",
    "#\n",
    "\n",
    "# Read lines from traces.dat text in to arrays of characters\n",
    "# Convert to onehot matrices\n",
    "\n",
    "cd(@__DIR__)\n",
    "\n",
    "text, alphabet, N = get_data(\"traces.csv\")\n",
    "stop = onehot('\\n', alphabet);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition into subsequences to input to our model\n",
    "\n",
    "seq_len = 50\n",
    "\n",
    "Xs = [collect(partition(t,seq_len)) for t in text];\n",
    "Ys = readdlm(\"y_results.csv\");\n",
    "\n",
    "dataset = [(onehotbatch(x, alphabet, '\\n'), onehot(Ys[i], unique(Ys)))\n",
    "           for i in 1:length(Ys) for x in Xs[i]] |> shuffle\n",
    "\n",
    "Ys = last.(dataset)\n",
    "\n",
    "# Pad sequences to equal lengths\n",
    "\n",
    "Xs = [hcat(x,repeat(stop,1,seq_len-size(x)[2])) for x in first.(dataset)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 972,290 items in our data. We use a train:test split of 90:10, stratified to ensure we have \n",
    "# the same share of \"bad\" and \"good\" traces in our train and test sets.\n",
    "\n",
    "(Xtrain, Ytrain), (Xtest, Ytest) = stratifiedobs((Xs, Ys), p=0.9)\n",
    "\n",
    "train = [(Xtrain[i], Ytrain[i]) for i in 1:length(Ytrain)];\n",
    "test = [(Xtest[i], Ytest[i]) for i in 1:length(Ytest)];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set up our model architecture\n",
    "\n",
    "scanner = Chain(Dense(length(alphabet), seq_len, Ïƒ), LSTM(seq_len, seq_len))\n",
    "encoder = Dense(seq_len, 2)\n",
    "\n",
    "function model(x)\n",
    "  state = scanner.([x])[end]\n",
    "  Flux.reset!(scanner)\n",
    "  softmax(encoder(state))\n",
    "end\n",
    "\n",
    "loss(tup) = crossentropy(mod(tup[1]), tup[2])\n",
    "accuracy(tup) = mean(argmax(m(tup[1])) .== argmax(tup[2]))\n",
    "\n",
    "opt = ADAM(0.01)\n",
    "ps = params(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we set up our callbacks for reporting on training progress.\n",
    "\n",
    "testacc() = mean(accuracy(t) for t in test)\n",
    "testloss() = mean(loss(t) for t in test)\n",
    "\n",
    "evalcb = () -> @show testloss(), testacc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, train!\n",
    "\n",
    "Flux.train!(loss, ps, train, opt, cb = throttle(evalcb, 10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
