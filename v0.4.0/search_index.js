var documenterSearchIndex = {"docs":
[{"location":"contributing/#Developer-Guidelines","page":"Contributing","title":"Developer Guidelines","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"This document explains the process for development on this project. We are using a PR model, so file an issue on the repo proposing your change so that the developers can discuss and provide early feedback, then make a pull request with your changes. Tag the relevant developers with their names in the comments on the PR so their attention can be called to the PR. Shorter PRs get reviewed faster and get more meaningful feedback.","category":"page"},{"location":"contributing/#Code-Style","page":"Contributing","title":"Code Style","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Docstrings on every function and type.\nUse multiple dispatch or default arguments.\nUse logging with key word arguments for example, @info(\"Inserting new vertex into graph\", vertex=v).\nCreate non-allocating versions of functions such as buildgraph!(g, edges) and allocating versions for end users such as buildgraph(edges) = buildgraph(Graph(), edges).\nUse simple function names when possible.","category":"page"},{"location":"contributing/#Testing","page":"Contributing","title":"Testing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Every file in /src should have a test in /test.\nPut tests in their own test set.\nTests are an opportunity to add usage examples.\nTravis-CI will check that tests pass.","category":"page"},{"location":"contributing/#Documentation","page":"Contributing","title":"Documentation","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Every concept should have an example in the docs.\nIf you need to add a page to the HTML docs add it as /doc/src/file.md and add the corresponding line in /doc/make.jl.\nMake sure the docs build locally before merging with master. Travis will test that the docs build so it is important to make sure you can build locally.\nInstall graphviz locally so that you can test the .dot files.","category":"page"},{"location":"contributing/#Code-of-Conduct","page":"Contributing","title":"Code of Conduct","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Be nice. Answer questions and provide feedback on PRs and Issues. Help out with what you can, and ask questions about what you don't understand.","category":"page"},{"location":"act/main/","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"act/main/","page":"Introduction","title":"Introduction","text":"title: Model Augmentation with Transformations on Categories author: James Fairbanks date: May 3, 2019 header-includes: |   \\usepackage{fullpage} ...","category":"page"},{"location":"act/main/#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"act/main/","page":"Introduction","title":"Introduction","text":"We introduce SemanticModels.jl, a practical library for scientific model augmentation based on category theoretic representations of models.","category":"page"},{"location":"act/main/#Functors-on-Models","page":"Introduction","title":"Functors on Models","text":"","category":"section"},{"location":"act/main/","page":"Introduction","title":"Introduction","text":"Given a transformation tin T and a model min M we apply that transformation to the model to define m = t(m). This transformation induces a functor phi m to m. We study what the properties of phi say about the relationship between m and m.","category":"page"},{"location":"act/main/#Preimage-Transformation","page":"Introduction","title":"Preimage Transformation","text":"","category":"section"},{"location":"act/main/#Conclusion","page":"Introduction","title":"Conclusion","text":"","category":"section"},{"location":"act/main/","page":"Introduction","title":"Introduction","text":"Category Theory provides a framework for analyzing scientific models and relationships between them\nThis framework is implemented in a practical software library scientists can use.\nCategory theory notions on functors translate to scientifically meaningful relationships between models.","category":"page"},{"location":"category/#A-Category-for-Models","page":"A Category for Models","title":"A Category for Models","text":"","category":"section"},{"location":"category/","page":"A Category for Models","title":"A Category for Models","text":"[]{.math .inline} Let [ùíû]{.math .inline} be the category of programs, where [Ob(ùíû)]{.math .inline} and [Hom(S,‚ÄÜT)={f‚ÄÖ‚à£‚ÄÖf(:‚ÄÑ:‚ÄÑS):‚ÄÑ:‚ÄÑT}]{.math .inline}. A model is a subcategory comprised of functions and types used in the code that implements that model. Then a model transformation is a function, [t]{.math .inline}, on models [M]{.math .inline} that induces a functor [F‚ÄÑ:‚ÄÑt(M)‚ÜíM]{.math .inline}. We define the model transformation as function on models because you need to compute it going from the simple model to the more complex model, but the functor goes the other way, from the complex model to the simple model.","category":"page"},{"location":"category/#Properties-of-Transformations","page":"A Category for Models","title":"Properties of Transformations","text":"","category":"section"},{"location":"category/","page":"A Category for Models","title":"A Category for Models","text":"Model transformations respect composition.","category":"page"},{"location":"category/","page":"A Category for Models","title":"A Category for Models","text":"Proof: If [t~1~(M)=M‚Ä≤]{.math .inline} there exists [F‚ÄÑ:‚ÄÑM‚Ä≤‚ÜíM]{.math .inline} and [t~2~(M‚Ä≤)‚ÄÑ=‚ÄÑM‚Ä≥]{.math .inline} there exists [G‚ÄÑ:‚ÄÑM‚Ä≥‚ÜíM‚Ä≤]{.math .inline} then [t~2~‚ÄÖ‚àò‚ÄÖt~1~]{.math .inline} induces a functor [G‚ÄÖ‚àò‚ÄÖF‚ÄÑ:‚ÄÑM‚Ä≥‚ÜíM]{.math .inline} by the definition of functor.","category":"page"},{"location":"category/#Examples","page":"A Category for Models","title":"Examples","text":"","category":"section"},{"location":"category/","page":"A Category for Models","title":"A Category for Models","text":"A simple model of gravity can be represented. Everyone knows the formula for the force of gravity, the sum of the masses divided by the double of the radius between them","category":"page"},{"location":"category/","page":"A Category for Models","title":"A Category for Models","text":"[$F = G \\frac{m_1 + m_2}{2r}$]{.math .inline}","category":"page"},{"location":"category/","page":"A Category for Models","title":"A Category for Models","text":"Oh wait, that isn't right, it has that structure, but is it the sum or the product? And is twice the radius or the square of the radius?","category":"page"},{"location":"category/","page":"A Category for Models","title":"A Category for Models","text":"A model transformation can recover the right model for us. The following figure show [M]{.math .inline} on the left and [M‚Ä≤]{.math .inline} on the right. The functor between them is shown using color.","category":"page"},{"location":"category/","page":"A Category for Models","title":"A Category for Models","text":"gravity diagram","category":"page"},{"location":"category/","page":"A Category for Models","title":"A Category for Models","text":"The functor from [M‚Ä≤]{.math .inline} to [M]{.math .inline} tells us how to transform [M‚Ä≤]{.math .inline} back into [M]{.math .inline}. We see that the only think that changed is [√ó]{.math .inline} becomes [+]{.math .inline} and [sqr]{.math .inline} becomes [dbl]{.math .inline}. This is exactly the way a programmer would describe the difference between these two models.","category":"page"},{"location":"category/","page":"A Category for Models","title":"A Category for Models","text":"We can see the notion of a fully faithful functor indicating that these models have the same structure. The functions in these two programs are in 1-1 correspondence. The notion of fully faithful functors says that for every pair of types [S,‚ÄÜT]{.math .inline} [F]{.math .inline} maps [Hom~M‚Ä≤~(S,‚ÄÜT)]{.math .inline} to [Hom~M~(F(S),F(T))]{.math .inline} with a 1-1 function. For this case [F(S)=S]{.math .inline} for all types. So this reduces to [Hom~M‚Ä≤~(S,‚ÄÜT)‚ÜíHom~M~(S,‚ÄÜT)]{.math .inline} with a 1-1 function. Based on the sizes of the [Hom]{.math .inline} sets, we see that there are only 4 possible fully faithful functors between these two models. The most obvious one is the one shown with color.","category":"page"},{"location":"category/#Isomorphism-is-too-strict","page":"A Category for Models","title":"Isomorphism is too strict","text":"","category":"section"},{"location":"category/","page":"A Category for Models","title":"A Category for Models","text":"You might think that since these diagrams look like graphs and the functors look like graph homomorphism, that graph isomorphism is a good definition of \"models with the same structure\". But this is two strict.","category":"page"},{"location":"category/","page":"A Category for Models","title":"A Category for Models","text":"In the following example the type graphs are not isomorphic, but there is a fully faithful functor between the model categories.","category":"page"},{"location":"category/#A-category-of-models","page":"A Category for Models","title":"A category of models","text":"","category":"section"},{"location":"category/","page":"A Category for Models","title":"A Category for Models","text":"Let [‚Ñ≥‚Ñ¥ùíπ]{.math .inline} represent the category of models under transformation. [Ob(‚Ñ≥‚Ñ¥ùíπ)]{.math .inline} is the set of models and [Hom~‚Ñ≥‚Ñ¥ùíπ~(M‚Ä≤,M)]{.math .inline} is the set of functors from [M‚Ä≤]{.math .inline} to M (ie. transformations from [M]{.math .inline} to [M‚Ä≤]{.math .inline}).","category":"page"},{"location":"category/#Perspective-on-Model-Selection","page":"A Category for Models","title":"Perspective on Model Selection","text":"","category":"section"},{"location":"category/","page":"A Category for Models","title":"A Category for Models","text":"Given an initial model [M]{.math .inline} a set of morphisms in [‚à™~x~Hom~‚Ñ≥‚Ñ¥ùíπ~(M,‚ÄÜx)]{.math .inline} we want to pick the [m]{.math .inline} that minisimizes some function [‚Ñì(m)]{.math .inline} which is the loss between the data and the model [m]{.math .inline}. Suppose that [T‚ÄÑ=‚ÄÑ‚ü®T~i~‚ü©]{.math .inline} is the set of transformations generated by [{T~i~‚ÄÖ‚à£‚ÄÖi‚ÄÑ‚àà‚ÄÑ‚Ñê}]{.math .inline}. We want to derive model selection algorithms that work for any [M,‚ÄÜ‚Ñì,‚ÄÜT]{.math .inline}.","category":"page"},{"location":"category/","page":"A Category for Models","title":"A Category for Models","text":"These algorithms will exploit the algebra of the transformation set, for example if the set of transformations is just a compositional monoid then the only option is to enumerate the tree of possible transformations [(List(T),‚ÄÖ+‚ÄÖ+,‚ÄÜ[])]{.math .inline} and pick the one that minimizes the loss. If the transformations have a stronger structure, such as forming a ring, and that structure is compatible with respect to the loss function, we should be able to find algorithms that exploit that structure.","category":"page"},{"location":"doublepushouts/#Double-Pushout-Rewriting","page":"Double Pushout Rewriting","title":"Double Pushout Rewriting","text":"","category":"section"},{"location":"doublepushouts/","page":"Double Pushout Rewriting","title":"Double Pushout Rewriting","text":"\\renewcommand{\\hom}[2]{{#1}\\rightarrow #2}","category":"page"},{"location":"doublepushouts/","page":"Double Pushout Rewriting","title":"Double Pushout Rewriting","text":"Cicala 2019 introducing rewriting in a category of open systems using spans and double pushouts. This can be readily applied to categories of models where a grammar of rewrite rules can be generated. When implementing this framework of metamodeling, the hardest part is to compute the result of a double push out (DPO).","category":"page"},{"location":"doublepushouts/","page":"Double Pushout Rewriting","title":"Double Pushout Rewriting","text":"\\begin{align}  l \\leftarrow & c \\rightarrow  r \\\n  \\downarrow\\hspace{1em}  &  \\downarrow \\hspace{1em} \\downarrow   \\\n l' \\leftarrow & c' \\rightarrow  r'  \\end{align}","category":"page"},{"location":"doublepushouts/","page":"Double Pushout Rewriting","title":"Double Pushout Rewriting","text":"The goal is to compute r given lcrcl. In this setting lcr is the rule, which is composed of 3 models l is the old model, r is the new model and c is the intersection. Then when we go to apply the rule to an existing model l we need to compute the model r.","category":"page"},{"location":"doublepushouts/","page":"Double Pushout Rewriting","title":"Double Pushout Rewriting","text":"Denote morphisms in this category arrows ie. c rightarrow l.","category":"page"},{"location":"doublepushouts/","page":"Double Pushout Rewriting","title":"Double Pushout Rewriting","text":"If we had an algorithm for computing pushouts in the category, then given c rightarrow l crightarrow r crightarrow c l rightarrow l we could computing l r. In the use case of model augmentation, one would already know one of lr and need to compute the other. Note that spans are reversible, so if you can compute DPOs from l to r, then you can reverse the rule and compute l from r.","category":"page"},{"location":"doublepushouts/#Petri-Net-Rewriting","page":"Double Pushout Rewriting","title":"Petri Net Rewriting","text":"","category":"section"},{"location":"doublepushouts/","page":"Double Pushout Rewriting","title":"Double Pushout Rewriting","text":"Given a petri net DPO, can we solve for the r?","category":"page"},{"location":"doublepushouts/","page":"Double Pushout Rewriting","title":"Double Pushout Rewriting","text":"The desired algorithm is ","category":"page"},{"location":"doublepushouts/","page":"Double Pushout Rewriting","title":"Double Pushout Rewriting","text":"relabel the states so that the states in the image of homcc and homcr match.\nrelabel the transitions so that the transitions in the image of homcc and homcr match.\nS_r = S_r cup S_c\nT_r = T_r cup T_c","category":"page"},{"location":"doublepushouts/","page":"Double Pushout Rewriting","title":"Double Pushout Rewriting","text":"This algorithm works (not yet proven) for applying a pushout. However, modelers often have lcr and l instead of c. So the last step of the algorithm for DPO rewriting for model augmentation requires the inference of c from (lcr) and l","category":"page"},{"location":"library/#Library-Reference","page":"Library Reference","title":"Library Reference","text":"","category":"section"},{"location":"library/","page":"Library Reference","title":"Library Reference","text":"Modules = [SemanticModels]","category":"page"},{"location":"library/#SemanticModels.SemanticModels","page":"Library Reference","title":"SemanticModels.SemanticModels","text":"SemanticModels\n\nprovides the AbstractModel type and constructors for building metamodeling tooling for categorical model representations\n\n\n\n\n\n","category":"module"},{"location":"library/#SemanticModels.AbstractModel","page":"Library Reference","title":"SemanticModels.AbstractModel","text":"AbstractModel\n\na placeholder struct to dispatch on how to parse the expression tree into a model.\n\n\n\n\n\n","category":"type"},{"location":"library/#SemanticModels.model-Union{Tuple{T}, Tuple{Type{T},Any}} where T<:AbstractModel","page":"Library Reference","title":"SemanticModels.model","text":"model(::AbstractModel, x) dig into the expression that describes a model and break it down into components. This allows you to construct a structured representation of the modeling problem at the expression level. Just like how julia modeling frameworks build structured representations of the problems in data structures. This version builds them at the expression level. The first argument is the type you want to construct, the second argument is the model structure that you want to analyze. For example\n\nmodel(PetriModel, x::Petri.Model)::PetriModel\n\n\n\n\n\n","category":"method"},{"location":"library/#Category-Theory","page":"Library Reference","title":"Category Theory","text":"","category":"section"},{"location":"library/","page":"Library Reference","title":"Library Reference","text":"The following Category Theory operations ship with SemanticModels, you can use them as templates for defining your own model classes.","category":"page"},{"location":"library/","page":"Library Reference","title":"Library Reference","text":"Modules = [SemanticModels.CategoryTheory]","category":"page"},{"location":"library/#SemanticModels.CategoryTheory.AbstractCospan","page":"Library Reference","title":"SemanticModels.CategoryTheory.AbstractCospan","text":"AbstractCospan\n\nan abstract type for representing cospans. The essential API for subtypes of AbstractCospan are\n\nleft(c)::M\nright(c)::M\npullback(c)::S\n\nwhere M is the type of morphisms in the cospan, and S is the type of span that solves the pullback defined by c. See Cospan for an example.\n\n\n\n\n\n","category":"type"},{"location":"library/#SemanticModels.CategoryTheory.AbstractMorph","page":"Library Reference","title":"SemanticModels.CategoryTheory.AbstractMorph","text":"AbstractMorph\n\nan abstract type for representing morphisms. The essential API for subtypes of AbstractMorph are\n\ndom(m)::T\ncodom(m)::T\nfunc(m)::Function\n\nwhere T is the type of the objects in the category. See FinSetMorph for an example.\n\n\n\n\n\n","category":"type"},{"location":"library/#SemanticModels.CategoryTheory.AbstractSpan","page":"Library Reference","title":"SemanticModels.CategoryTheory.AbstractSpan","text":"AbstractSpan\n\nan abstract type for representing spans. The essential API for subtypes of AbstractSpan are\n\nleft(s)::M\nright(s)::M\npushout(s)::C\n\nwhere M is the type of morphisms in the span, and C is the type of cospan that solves a pushout of span s. See Span for an example.\n\n\n\n\n\n","category":"type"},{"location":"library/#SemanticModels.CategoryTheory.Cospan","page":"Library Reference","title":"SemanticModels.CategoryTheory.Cospan","text":"Cospan{F,G} <: AbstractCospan\n\na general cospan type where types F and G are types of morphisms in the cospan\n\n\n\n\n\n","category":"type"},{"location":"library/#SemanticModels.CategoryTheory.Decorated","page":"Library Reference","title":"SemanticModels.CategoryTheory.Decorated","text":"Decorated{M,T}\n\na decoration applied to the objects of a morphism, where M is a type of morphism and type T is the category of the decoration\n\n\n\n\n\n","category":"type"},{"location":"library/#SemanticModels.CategoryTheory.FinSetMorph","page":"Library Reference","title":"SemanticModels.CategoryTheory.FinSetMorph","text":"FinSetMorph{T,F}\n\nmorphisms in the category of Finite Sets. The objects are of type UnitRange{Int}. func(m::FinSetMorph) is a function that takes Int -> Int. FinSetMorphs can be constructed from a list of numbers. For example, FinSetMorph([1,3,2,3]) is the morphism that takes 1->1, 2->3, 3->2, 4->3 on domain 1:4 with codomain 1:3. When you define a morphism from a list of integers, the codomain is inferred from the largest element of the list. The domain must always be the 1:l where l is the length of the input list.\n\n(f::FinSetMorph)(g::G) where G <: AbstractGraph\n\nlift a finite set morphism (list of integers) to a graph homomorphism by its action on the vertex set. The graph h = f(g) is defined by taking the edges of g and relabeling their src and dst according to the function of f.\n\nThis method computes a valid graph homomorphism by definition.\n\n\n\n\n\n","category":"type"},{"location":"library/#SemanticModels.CategoryTheory.Span","page":"Library Reference","title":"SemanticModels.CategoryTheory.Span","text":"Span{F,G} <: AbstractSpan\n\na general span type where types F and G are types of morphisms in the span\n\n\n\n\n\n","category":"type"},{"location":"library/#SemanticModels.CategoryTheory.:‚äî-Tuple{SemanticModels.CategoryTheory.FinSetMorph,SemanticModels.CategoryTheory.FinSetMorph}","page":"Library Reference","title":"SemanticModels.CategoryTheory.:‚äî","text":"‚äî(f::FinSetMorph, g::FinSetMorph)\n\nthe union of two morphisms in a finite set.\n\n\n\n\n\n","category":"method"},{"location":"library/#SemanticModels.CategoryTheory.pushout-Union{Tuple{SemanticModels.CategoryTheory.Span{T,T}}, Tuple{T}} where T<:SemanticModels.CategoryTheory.Decorated","page":"Library Reference","title":"SemanticModels.CategoryTheory.pushout","text":"pushout(s::Span{T, T}) where T <: Decorated\n\ntreat f,g as a decorated span and compute the pushout that is, the cospan of f=(f‚äîg) and g=(a‚äîb), with the decoration of (f‚äîg)(d)\n\n\n\n\n\n","category":"method"},{"location":"library/#SemanticModels.CategoryTheory.pushout-Union{Tuple{SemanticModels.CategoryTheory.Span{T,U}}, Tuple{U}, Tuple{T}} where U<:SemanticModels.CategoryTheory.FinSetMorph where T<:SemanticModels.CategoryTheory.FinSetMorph","page":"Library Reference","title":"SemanticModels.CategoryTheory.pushout","text":"pushout(s::Span{T,T}) where T <: FinSetMorph\n\ntreat f,g as a span and compute the pushout that is, the cospan of f=(f‚äîg) and g=(a‚äîb)\n\n\n\n\n\n","category":"method"},{"location":"library/#SemanticModels.CategoryTheory.undecorate-Union{Tuple{SemanticModels.CategoryTheory.Cospan{T,T}}, Tuple{T}} where T<:SemanticModels.CategoryTheory.Decorated","page":"Library Reference","title":"SemanticModels.CategoryTheory.undecorate","text":"undecorate(s::Copan{T,T}) where T <: Decorated\n\nremove decorations of a cospan of decorated morphisms\n\n\n\n\n\n","category":"method"},{"location":"library/#SemanticModels.CategoryTheory.undecorate-Union{Tuple{SemanticModels.CategoryTheory.Span{T,T}}, Tuple{T}} where T<:SemanticModels.CategoryTheory.Decorated","page":"Library Reference","title":"SemanticModels.CategoryTheory.undecorate","text":"undecorate(s::Span{T,T}) where T <: Decorated\n\nremove decorations of a span of decorated morphisms\n\n\n\n\n\n","category":"method"},{"location":"library/#Model-Classes","page":"Library Reference","title":"Model Classes","text":"","category":"section"},{"location":"library/","page":"Library Reference","title":"Library Reference","text":"The following model class ship with SemanticModels, you can use them as templates for defining your own model classes.","category":"page"},{"location":"library/","page":"Library Reference","title":"Library Reference","text":"Modules = [\n           SemanticModels.WiringDiagrams,\n           SemanticModels.PetriModels,\n           SemanticModels.RelOlogModels,\n           SemanticModels.OpenModels,\n           SemanticModels.OpenPetris,\n           SemanticModels.PetriCospans\n          ]","category":"page"},{"location":"library/#Index","page":"Library Reference","title":"Index","text":"","category":"section"},{"location":"library/","page":"Library Reference","title":"Library Reference","text":"","category":"page"},{"location":"slides/#Slides","page":"Slides","title":"Slides","text":"","category":"section"},{"location":"slides/#Extracting-Model-Structure-for-Improved-Semantic-Modeling","page":"Slides","title":"Extracting Model Structure for Improved Semantic Modeling","text":"","category":"section"},{"location":"slides/","page":"Slides","title":"Slides","text":"James Fairbanks, GTRI\ncomputational representations of model semantics with knowledge graphs for","category":"page"},{"location":"slides/","page":"Slides","title":"Slides","text":"metamodel reasoning.","category":"page"},{"location":"slides/#Goals","page":"Slides","title":"Goals","text":"","category":"section"},{"location":"slides/","page":"Slides","title":"Slides","text":"Extract a knowledge graph from Scientific Artifacts (code, papers, datasets)\nRepresent scientific models in a high level way, (code as data)\nBuild metamodels by combining models in hierarchical expressions using reasoning over KG (1).","category":"page"},{"location":"slides/#Running-Example:-Influenza","page":"Slides","title":"Running Example: Influenza","text":"","category":"section"},{"location":"slides/","page":"Slides","title":"Slides","text":"Modeling the cost of treating a flu season taking into account weather effects.","category":"page"},{"location":"slides/","page":"Slides","title":"Slides","text":"Seasonal temperature is a dynamical system\nFlu infectiousness is a function of temperature","category":"page"},{"location":"slides/#Running-Example:-Modeling-types","page":"Slides","title":"Running Example: Modeling types","text":"","category":"section"},{"location":"slides/","page":"Slides","title":"Slides","text":"Modeling the cost of treating a flu season taking into account weather effects.","category":"page"},{"location":"slides/","page":"Slides","title":"Slides","text":"Seasonal temperature is approximated by 2nd order linear ODE\nFlu cases is an SIR model 1st oder nonlinear ode\nMitigation cost is Linear Regression on vaccines and cases","category":"page"},{"location":"slides/#Scientific-Domain","page":"Slides","title":"Scientific Domain","text":"","category":"section"},{"location":"slides/","page":"Slides","title":"Slides","text":"We focus on Susceptible Infected Recovered model of epidemiology.","category":"page"},{"location":"slides/","page":"Slides","title":"Slides","text":"Precise, concise mathematical formulation\nDiverse class of models, ODE vs Agent based, determinstic vs stochastic\nFOSS implementations are available in all three Scientific programming languages","category":"page"},{"location":"slides/#Graph-of-SIR-Model","page":"Slides","title":"Graph of SIR Model","text":"","category":"section"},{"location":"slides/","page":"Slides","title":"Slides","text":"(Image: Graph of SIR model)","category":"page"},{"location":"slides/#Knowledge-Extraction-Architecture","page":"Slides","title":"Knowledge Extraction Architecture","text":"","category":"section"},{"location":"slides/","page":"Slides","title":"Slides","text":"(Image: Knowledge Extraction Architecture)","category":"page"},{"location":"slides/#Example-Input-Packages","page":"Slides","title":"Example Input Packages","text":"","category":"section"},{"location":"slides/","page":"Slides","title":"Slides","text":"EMOD, Epimodels, NetLogo, and FRED are established packages, given their maturity and availability of published papers citing these packages.\nPathogen and NDLib are newer packages, we expect easier to work with and more future adoption.\nTextbooks¬†[@voitfirst2012] and lecture notes[1] will be a resource for these simple models that are well characterized.","category":"page"},{"location":"slides/#Model-Representation-and-Execution","page":"Slides","title":"Model Representation and Execution","text":"","category":"section"},{"location":"slides/","page":"Slides","title":"Slides","text":"Representation of models occurs at four levels:","category":"page"},{"location":"slides/","page":"Slides","title":"Slides","text":"Executable: the level of machine or byte-code instructions\nLexical: the tradition code representation assignment,   functions, and loops\nSemantic: a declarative language or computation graph   representation with nodes linked to the knowledge graph\nHuman: a description in natural language as in a research paper   or textbook","category":"page"},{"location":"slides/#Knowledge-Graph","page":"Slides","title":"Knowledge Graph","text":"","category":"section"},{"location":"slides/","page":"Slides","title":"Slides","text":"(Image: Hypothetical Knowledge Graph Sample)","category":"page"},{"location":"slides/","page":"Slides","title":"Slides","text":"Hypothetical Knowledge Graph Sample","category":"page"},{"location":"slides/#Knowledge-Graph-Schema","page":"Slides","title":"Knowledge Graph Schema","text":"","category":"section"},{"location":"slides/","page":"Slides","title":"Slides","text":"A preliminary design for types of knowledge in our knowledge graph. (Image: Knowledge Graph Schema)","category":"page"},{"location":"slides/","page":"Slides","title":"Slides","text":"Artifacts\nComponents\nModels\nVariables\nEquations\nConcepts\nValues","category":"page"},{"location":"slides/#How-do-we-get-from-Weather-to-Cost?","page":"Slides","title":"How do we get from Weather to Cost?","text":"","category":"section"},{"location":"slides/","page":"Slides","title":"Slides","text":"(Image: How do we get from Weather to Cost?){ width=80% }","category":"page"},{"location":"slides/","page":"Slides","title":"Slides","text":"(Image: How do we get from Weather to Cost?){ width=80% }","category":"page"},{"location":"slides/","page":"Slides","title":"Slides","text":"Shortest path!","category":"page"},{"location":"slides/#How-do-we-get-from-WeatherDemographics-to-Cost?","page":"Slides","title":"How do we get from Weather+Demographics to Cost?","text":"","category":"section"},{"location":"slides/","page":"Slides","title":"Slides","text":"(Image: How do we get from Weather to Cost?){ width=80% }","category":"page"},{"location":"slides/","page":"Slides","title":"Slides","text":"Minimum ST flow!","category":"page"},{"location":"slides/#Knowledge-Graph-Reasoning-Open-Questions","page":"Slides","title":"Knowledge Graph Reasoning Open Questions","text":"","category":"section"},{"location":"slides/","page":"Slides","title":"Slides","text":"What rules for path/flow computations are necessary and sufficient for a metamodel?\nCan we implement those rules by choosing weights?\nHow do we handle uncertainty and near matches?\nHow do we determine \"necessary dependencies\" better than \"connected component\"\nWhat about supplying expert information?","category":"page"},{"location":"slides/#Infectious-Disease-Metamodel","page":"Slides","title":"Infectious Disease Metamodel","text":"","category":"section"},{"location":"slides/","page":"Slides","title":"Slides","text":"A more ambitious example of a metamodel\nRequires Agent based simulations of information diffuision and disease spread","category":"page"},{"location":"slides/","page":"Slides","title":"Slides","text":"(Image: A DAG of model dependencies)","category":"page"},{"location":"slides/#Static-vs-Dynamic-Graph","page":"Slides","title":"Static vs Dynamic Graph","text":"","category":"section"},{"location":"slides/","page":"Slides","title":"Slides","text":"Inherent tradeoff between flexibility and static analysis\nWe will build the computation graph through the execution of code\nMetaprogramming will be used to generate the executable codes","category":"page"},{"location":"slides/#Validation","page":"Slides","title":"Validation","text":"","category":"section"},{"location":"slides/","page":"Slides","title":"Slides","text":"Extraction of KG elements from artifacts\nMetamodel construction\nMetamodel quality","category":"page"},{"location":"slides/#Error-and-Residual","page":"Slides","title":"Error and Residual","text":"","category":"section"},{"location":"slides/","page":"Slides","title":"Slides","text":"Analogize the metamodel construction error and the model quality to the error and residual in numerical solvers.","category":"page"},{"location":"slides/","page":"Slides","title":"Slides","text":"Given f(x)=0 solve for x","category":"page"},{"location":"slides/","page":"Slides","title":"Slides","text":"Measure both the error and the residual.\nError mid x-x^starmid, the difference from the correct solution\nResidual mid f(x) - f(x^star)mid or the difference from quality of optimal solution","category":"page"},{"location":"slides/#Next-Steps","page":"Slides","title":"Next Steps","text":"","category":"section"},{"location":"slides/","page":"Slides","title":"Slides","text":"Incorporation of feedback today\nthe types of artifacts in scope\ndomain coverage and desired extensibility\ninclusion/exclusion of particular package(s) and/or knowledge artifact(s)\nConstruction of a proof-of-concept version of our knowledge graph and end-to-end pipeline\nTailor running example to DARPA objectives\nA automatic transformation of models at the Semantic Level","category":"page"},{"location":"slides/","page":"Slides","title":"Slides","text":"[1]: http://alun.math.ncsu.edu/wp-content/uploads/sites/2/2017/01/epidemic_notes.pdf","category":"page"},{"location":"extraction/#Knowledge-Extraction","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"","category":"section"},{"location":"extraction/#Documents","page":"Knowledge Extraction","title":"Documents","text":"","category":"section"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"SemanticModels.jl takes the opinion that the source code and documentation is more valuable than the scientific papers themselves, even though traditional scientific incentive systems focus on only the papers.","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"Since natural language text is primarily being used for context, understanding, and disambiguation of code information, we use rules-based methods to extract definitions and conceptual connections. The Automates framework developed at the University of Arizona is very helpful for writing rules-based information extraction software. We have made upstream contributions to the Automates repository.","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"The primary focus of this document is the extraction of scientific knowledge from codebases. We start by describing the natural language information extraction pipeline.","category":"page"},{"location":"extraction/#Information-Extraction-for-Semantic-Modeling","page":"Knowledge Extraction","title":"Information Extraction for Semantic Modeling","text":"","category":"section"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"To select knowledge elements that should be present in knowledge graphs, we conduct information extraction on various components of our source files, including:","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"Scientist/programmer-contributed comments within source code files.\nCode phenomena such as function names, parameters, and values.","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"Ongoing work involves building extractors for:","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"Research publications.\nDocumentation for libraries and frameworks utilized within the domains of epidemiology and information diffusion.","category":"page"},{"location":"extraction/#Information-Extraction-Pipeline","page":"Knowledge Extraction","title":"Information Extraction Pipeline","text":"","category":"section"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"Process source files including research papers, source code, and documentation files into plain text or JSON document formats.\nExtract natural language text such as docstrings and comments.\nParse source code with to identify function names and parameters.\nMatch modeling text concepts with code variables using lexical-tokens.\nRun Automates rule-based extraction on the text associated with each code concept.\nCreate knowledge elements (e.g., vertices and edges) from the tuples associated with rule matches.","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"SemanticModels has created rules to extract phenomena such as definitions of parameters. These same parameters can then be recognized within source code, beginning with lexical matching for mapping human language definitions to specific source code instantiations.","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"We are currently in the process of collecting and annotating ground truth data to use in constructing machine learning models to do information extractions based on information elements of interest that we identify in use case planning for meta-modeling related functionalities users will be able to work with.","category":"page"},{"location":"extraction/#Code","page":"Knowledge Extraction","title":"Code","text":"","category":"section"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"SemanticModels currently supports extracting knowledge from the static syntactic level information that is accessible from the source code. We use the same Julia code parser as the julia program.","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"This parser takes text representations of Julia code and returns an abstract syntax tree (AST). We then walk this AST looking for Julia program expressions that create information. For example, function definitions, variable assignments and module imports. We recurse into the function definitions to find the local variable definitions (and closures) used in implementing the functions.","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"This form of static program analysis provides a more direct way to access user defined functions. However it cannot access the type information and run time values. For this we use Cassette.jl, which is a library for context-dependent execution. SemanticModels uses a custom compiler pass to access code infomation and extract information at compile time. In addition, we use the overdub component of Cassette to build a tracer for capturing run time values. Since Julia syntax is very regular and metaprogramming is a standard (albeit advanced) practice in the Julia community, the syntax trees and CodeInfo objects are designed to be manipulated programmatically, which makes writing recursive generic syntax extraction rules straightforward.","category":"page"},{"location":"extraction/#Example","page":"Knowledge Extraction","title":"Example","text":"","category":"section"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"We can read in the file examples/epicookbook/notebooks/KeelingRohani/SISModel.jl","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"using DifferentialEquations\n\n# # Model Specifications\n# - SH num of high risk susceptible\n# - IH num of high risk infected\n# - SL num of low risk susceptible\n# - IL num of low risk infected\n# # Parameters\n# - beta represents the determines the contact and transmission rates\n# - gamma the rate at which treatment is sought\n\nfunction sis_ode(du,u,p,t)\n        SH,IH,SL,IL = u\n        betaHH,betaHL,betaLH,betaLL,gamma=p\n        du[1]=-(betaHH*IH+betaHL*IL)*SH+gamma*IH\n        du[2]=+(betaHH*IH+betaHL*IL)*SH-gamma*IH\n        du[3]=-(betaLH*IH+betaLL*IL)*SL+gamma*IL\n        du[4]=+(betaLH*IH+betaLL*IL)*SL-gamma*IL\nend\n\nparms =[10,0.1,0.1,1,1]\ninit=[0.19999,0.00001,0.799,0.001]\ntspan=tspan = (0.0,15.0)\n\nsis_prob = ODEProblem(sis_ode,init,tspan,parms)\nsis_sol = solve(sis_prob,saveat=0.1);\n\nusing Plots\nplot(sis_sol,xlabel=\"Time (Years)\",ylabel=\"Proportion of Population\")","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"and run it through the code syntactic extractor, which will produce the following information:","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"‚îå Info: script uses modules\n‚îÇ   modules =\n‚îÇ    2-element Array{Any,1}:\n‚îÇ     Any[:DifferentialEquations]\n‚îî     Any[:Plots]\n\n‚îå Info: script defines functions\n‚îÇ   funcs =\n‚îÇ    1-element Array{Any,1}:\n‚îÇ     :(sis_ode(du, u, p, t)) => quote\n‚îÇ        #= none:28 =#\n‚îÇ        (SH, IH, SL, IL) = u\n‚îÇ        #= none:29 =#\n‚îÇ        (betaHH, betaHL, betaLH, betaLL, gamma) = p\n‚îÇ        #= none:30 =#\n‚îÇ        du[1] = -((betaHH * IH + betaHL * IL)) * SH + gamma * IH\n‚îÇ        #= none:31 =#\n‚îÇ        du[2] = +((betaHH * IH + betaHL * IL)) * SH - gamma * IH\n‚îÇ        #= none:32 =#\n‚îÇ        du[3] = -((betaLH * IH + betaLL * IL)) * SL + gamma * IL\n‚îÇ        #= none:33 =#\n‚îÇ        du[4] = +((betaLH * IH + betaLL * IL)) * SL - gamma * IL\n‚îî    end\n\n‚îå Info: script defines glvariables\n‚îÇ   funcs =\n‚îÇ    5-element Array{Any,1}:\n‚îÇ        :parms => :([10, 0.1, 0.1, 1, 1])\n‚îÇ         :init => :([0.19999, 1.0e-5, 0.799, 0.001])\n‚îÇ        :tspan => :(tspan = (0.0, 15.0))\n‚îÇ     :sis_prob => :(ODEProblem(sis_ode, init, tspan, parms))\n‚îî      :sis_sol => :(solve(sis_prob, saveat=0.1))\n\n‚îå Info: sis_ode(du, u, p, t) uses modules\n‚îî   modules = 0-element Array{Any,1}\n‚îå Info: sis_ode(du, u, p, t) defines functions\n‚îî   funcs = 0-element Array{Any,1}\n‚îå Info: sis_ode(du, u, p, t) defines glvariables\n‚îÇ   funcs =\n‚îÇ    6-element Array{Any,1}:\n‚îÇ                            :((SH, IH, SL, IL)) => :u\n‚îÇ     :((betaHH, betaHL, betaLH, betaLL, gamma)) => :p\n‚îÇ                                       :(du[1]) => :(-((betaHH * IH + betaHL * IL)) * SH + gamma * IH)\n‚îÇ                                       :(du[2]) => :(+((betaHH * IH + betaHL * IL)) * SH - gamma * IH)\n‚îÇ                                       :(du[3]) => :(-((betaLH * IH + betaLL * IL)) * SL + gamma * IL)\n‚îî                                       :(du[4]) => :(+((betaLH * IH + betaLL * IL)) * SL - gamma * IL)\n‚îå Info: Edges found\n‚îî   path = \"examples/epicookbook/notebooks/KeelingRohani/SISModel.jl\"\n(:Modeling, :takes, :parms, :([10, 0.1, 0.1, 1, 1]))\n(:Modeling, :has, :parms, :prop_collection)\n(:Modeling, :takes, :init, :([0.19999, 1.0e-5, 0.799, 0.001]))\n(:Modeling, :has, :init, :prop_collection)\n(:Modeling, :structure, :tspan, :((0.0, 15.0)))\n(:Modeling, :comp, :tspan, 0.0)\n(:Modeling, :comp, :tspan, 15.0)\n(:Modeling, :output, :sis_prob, :(ODEProblem(sis_ode, init, tspan, parms)))\n(:Modeling, :input, :sis_ode, Symbol[:init, :tspan, :parms])\n(:Modeling, :output, :sis_sol, :(solve(sis_prob, saveat=0.1)))\n(:Modeling, :input, :sis_prob, Symbol[Symbol(\"saveat=0.1\")])\n(\"Modeling.sis_ode(du, u, p, t)\", :destructure, :((SH, IH, SL, IL)), :u)\n(\"Modeling.sis_ode(du, u, p, t)\", :comp, :u, :SH)\n(\"Modeling.sis_ode(du, u, p, t)\", :comp, :u, :IH)\n(\"Modeling.sis_ode(du, u, p, t)\", :comp, :u, :SL)\n(\"Modeling.sis_ode(du, u, p, t)\", :comp, :u, :IL)\n(\"Modeling.sis_ode(du, u, p, t)\", :destructure, :((betaHH, betaHL, betaLH, betaLL, gamma)), :p)\n(\"Modeling.sis_ode(du, u, p, t)\", :comp, :p, :betaHH)\n(\"Modeling.sis_ode(du, u, p, t)\", :comp, :p, :betaHL)\n(\"Modeling.sis_ode(du, u, p, t)\", :comp, :p, :betaLH)\n(\"Modeling.sis_ode(du, u, p, t)\", :comp, :p, :betaLL)\n(\"Modeling.sis_ode(du, u, p, t)\", :comp, :p, :gamma)\n(\"Modeling.sis_ode(du, u, p, t)\", :output, :(du[1]), :(-((betaHH * IH + betaHL * IL)) * SH + gamma * IH))\n(\"Modeling.sis_ode(du, u, p, t)\", :input, :(-((betaHH * IH + betaHL * IL)) * SH), Symbol[Symbol(\"gamma * IH\")])\n(\"Modeling.sis_ode(du, u, p, t)\", :output, :(du[2]), :(+((betaHH * IH + betaHL * IL)) * SH - gamma * IH))\n(\"Modeling.sis_ode(du, u, p, t)\", :input, :(+((betaHH * IH + betaHL * IL)) * SH), Symbol[Symbol(\"gamma * IH\")])\n(\"Modeling.sis_ode(du, u, p, t)\", :output, :(du[3]), :(-((betaLH * IH + betaLL * IL)) * SL + gamma * IL))\n(\"Modeling.sis_ode(du, u, p, t)\", :input, :(-((betaLH * IH + betaLL * IL)) * SL), Symbol[Symbol(\"gamma * IL\")])\n(\"Modeling.sis_ode(du, u, p, t)\", :output, :(du[4]), :(+((betaLH * IH + betaLL * IL)) * SL - gamma * IL))\n(\"Modeling.sis_ode(du, u, p, t)\", :input, :(+((betaLH * IH + betaLL * IL)) * SL), Symbol[Symbol(\"gamma * IL\")])","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"Once the extraction is complete, the knowledge graph can be stored and transmitted to scientists across many disciplines. These knowledge graphs are a compact representation of the code and text. As new papers and codes are written, they can be ingested into an online graph database providing access to many scholars.","category":"page"},{"location":"extraction/#Reconciliation-and-Disambiguation","page":"Knowledge Extraction","title":"Reconciliation and Disambiguation","text":"","category":"section"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"As our information extraction pipeline outlined above illustrates, the task of knowledge graph construction implicitly requires us to either assert or infer a crosswalk between (1) vertices extracted from text and vertices extracted from code with a common higher-level source (e.g., a published paper that is associated with source code that also includes comments); and (2) vertices (and by extension, edges) that are already present in the graph, when the combined information conveyed by the user-provided vertex name, and provided/inferred vertex type is not a sufficient guarantee of uniqueness, and/or a reliable signal of user intent (e.g., the user may seek to (1) enforce uniqueness by differentiating a new vertex, v_i, from lexically identical but semantically different vertices in V, or (2) insert v_i iff V cap_semantic v_i = emptyset, regardless of their lexical (dis)similarity).","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"When two or more knowledge artifacts share provenance (e.g., the narrative text, programmer-provided comments, and source code that, when taken in tandem, represent a single recipe in the Epicookbook), we currently consider code text and markdown/comments text as strings, and use rule based learning to associate text with code objects; these lexical matches are then parsed in an effort to extract edges of the type representation (abbreviated repr), which connect a (code) type source vertex to a (scientific) concept destination vertex.","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"We intend to extend this approach in the future by: (1) creating new syntactical parsing rules to capture additional relationships; (2) considering the ways in which information related to scope, and/or position within the program-level call graph can be informative for the purpose of co-reference resolution; and/or (3) representing both sources of text sequences as real-valued vectors, to determine whether cosine similarity and/or RNN-based approaches can help to detect co-referential lexical elements [1].","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"With respect to the question of how to best assess/resolve ambiguity surrounding the uniqueness of a vertex upon ingestion, we currently guarantee uniqueness by appending a randomly generated string to the concatenation of the (raw-text) vertex name and the (schema-consistent) vertex type. This approach biases the graph toward smaller, disconnected subgraphs, and makes it harder for us to benefit from the semantic equivalence that often exists when different text and/or code artifacts from the same domain are parsed for the purpose of ingestion.","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"We intend to develop a more nuanced approach to vertex ingestion that incorporates exogenous, domain-specific information (for example, a lookup table of parameters that are commonly used within the epidemiological literature; known model imports, etc.). We can begin by manually constructing a dataset with examples of how these known elements are represented in code, and can then train an NER model to detect such references when they occur, so that we can avoid insertion of lexically distinct but (fuzzily) semantically equivalent vertices and encourage semantically meaningful consolidation, resulting in a more connected, parsimonious graph.","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"We may also find it helpful to leverage user-provided metadata (such as source/provenance information), and/or unsupervised learning techniques, including clustering methods, for this task as the complexity of the graph grows, and/or knowledge artifacts from additional domains with potentially conflicting named entities are introduced. We may also find it helpful to compare the semantic saliency of the (graph-theoretic) neighborhood(s) that might result from either the source or destination vertex of a new edge being mapped to each of a set of feasible existing vertices; this approach could also benefit from provenance-related metadata.","category":"page"},{"location":"extraction/#Reasoning","page":"Knowledge Extraction","title":"Reasoning","text":"","category":"section"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"Once the information is extracted from the documentation and code, we can visualize the knowledge as a graph.","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"(Image: Knowledge Graph from epicookbook)","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"This knowledge graph contains all the connections we need to combine components across models. Once can view this combination as either a modification of one model by substituting components of another model, or as the automatic generation of a metamodel by synthesizing components from the knowledge graph into a single coherent model. Further theoretical analysis of metamodeling and model modification as mathematical problems is warranted to make these categories unambiguous and precisely defined.","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"Once we identify a subgraph of related components we can identify the graft point between the two models. We look for a common variable that is used in two models, specifically in a derivative calculation. We find the variable S which appears in dS and dY (as S=Y[1] and dY = derivative(Y)). The knowledge that dS, dY are derivatives comes from the background knowledge of modeling that comes from reading textbooks and general scientific knowledge, while the fact that S and Y[1] both appear in an expression mu-beta*S*I - mu*S comes from the specific documents and codebases under consideration by the metamodeler.","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"(Image: Knowledge Subgraph showing model modification)","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"This subgraph must then extend out to capture all of the relevant information such as the parameter sets encountered, the function calls that contain these variables and expressions. We have found the largest relevant subgraph for some unspecified definition of relevance. From this subgraph, a human modeler can easily instruct the SemanticModels system on how to combine the SEIRmodel and ScalingModel programs into a single model and generate a program to execute it.","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"In order to move beyond this relatively manual approach to model modification and metamodeling, it is helpful to frame each of our intended use cases as an optimization problem, in which the scientist's required unitful input(s) and/or unitful output(s) (including expected deviation from observed/expected patterns, in the case of model validation) can be formally expressed as constraints, and relevance can be objectively and quantifiably represented, so that competing feasible flows can be assessed, ranked, and returned to the scientist to augment their understanding. The specification of the objective function, choice of traversal algorithm(s), and the use of edge weights to convey algorithmically meaningful information, will vary by use case.","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"For example, the metamodeling use case, in which the scientist begins with a vector of known unitful input and a vector of unitful output whose value is unknown, can be formulated as an s-t max flow problem, with our input vertex as s, our output vertex as t, and edge weights corresponding to the frequency with which a given edge is empirically observed within a domain-specific text and code corpus. To ensure tractability at scale, we may want to consider a weighting scheme to avoid integer constraints. This approach may also help us to identify disconnected subgraphs, which, if linked by cut-crossing edges, would represent a feasible flow; the scientific insight here is that such a set of edges might represent \"missing\" functions capable of transforming the \"input\" src vertex of a cut-crossing edge with its output dst vertex. These function(s) could then be ingested or written by scientists.","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"While we intend to proceed with algorithmic development of this nature in the near term, it's worth noting that the goal of this project is to augment scientists and their workflows. As such, we envision a human-in-the-loop, semi-automated approach, in which the scientist is in control and has the ability to instruct the machine by providing information about what the scientist already knows, and what they wish to do with that knowledge (e.g., modify, combine, validate) existing models and scripts.","category":"page"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"Any API that supports augmenting scientists will require some human intervention in the reasoning and generation stages as the system must get input from the user as to the questions being asked of it. We view this to analogous to a data analyst working with a database system: a query planning system is able to optimize queries based on knowledge about the schema and data statistics, but it must still wait for a human to provide a query. In this way, even as our development efforts proceed, SemanticModels will rely upon user guidance for reasoning and generation tasks.","category":"page"},{"location":"extraction/#API-reference","page":"Knowledge Extraction","title":"API reference","text":"","category":"section"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"Modules = [SemanticModels.ExprModels.Parsers]","category":"page"},{"location":"extraction/#SemanticModels.ExprModels.Parsers.AbstractCollector","page":"Knowledge Extraction","title":"SemanticModels.ExprModels.Parsers.AbstractCollector","text":"AbstractCollector\n\nsubtypes of AbstractCollector support extracting and collecting information from input sources.\n\n\n\n\n\n","category":"type"},{"location":"extraction/#SemanticModels.ExprModels.Parsers.FuncCollector","page":"Knowledge Extraction","title":"SemanticModels.ExprModels.Parsers.FuncCollector","text":"FuncCollector{T} <: AbstractCollector\n\ncollects function definitions and names\n\n\n\n\n\n","category":"type"},{"location":"extraction/#SemanticModels.ExprModels.Parsers.MetaCollector","page":"Knowledge Extraction","title":"SemanticModels.ExprModels.Parsers.MetaCollector","text":"MetaCollector{T,U,V,W} <: AbstractCollector\n\ncollects multiple pieces of information such as\n\nexprs: expressions\nfc: functions\nvc: variable assignments\nmodc: module imports\n\n\n\n\n\n","category":"type"},{"location":"extraction/#SemanticModels.ExprModels.Parsers.defs-Tuple{Any}","page":"Knowledge Extraction","title":"SemanticModels.ExprModels.Parsers.defs","text":"defs(body)\n\ncollect the function definitions and variable assignments from a module expression.\n\n\n\n\n\n","category":"method"},{"location":"extraction/#SemanticModels.ExprModels.Parsers.findassign-Tuple{Expr,Symbol}","page":"Knowledge Extraction","title":"SemanticModels.ExprModels.Parsers.findassign","text":"findassign(expr::Expr, name::Symbol)\n\nfindassign walks the AST of expr to find the assignments to a variable called name.\n\nThis function returns a reference to the original expression so that you can modify it inplace and is intended to help users rewrite expressions for generating new models.\n\nSee also: findfunc.\n\n\n\n\n\n","category":"method"},{"location":"extraction/#SemanticModels.ExprModels.Parsers.findfunc-Tuple{Expr,Symbol}","page":"Knowledge Extraction","title":"SemanticModels.ExprModels.Parsers.findfunc","text":"findfunc(expr::Expr, name::Symbol)\n\nfindfunc walks the AST of expr to find the definition of function called name.\n\nThis function returns a reference to the original expression so that you can modify it inplace and is intended to help users rewrite the definitions of functions for generating new models.\n\n\n\n\n\n","category":"method"},{"location":"extraction/#SemanticModels.ExprModels.Parsers.funcs-Tuple{Any}","page":"Knowledge Extraction","title":"SemanticModels.ExprModels.Parsers.funcs","text":"funcs(body)\n\ncollect the function definitions from a module expression.\n\n\n\n\n\n","category":"method"},{"location":"extraction/#SemanticModels.ExprModels.Parsers.inexpr-Tuple{Any,Any}","page":"Knowledge Extraction","title":"SemanticModels.ExprModels.Parsers.inexpr","text":"inexpr(expr, x)\n\nSimple expression match; will return true if the expression x can be found inside expr.     inexpr(:(2+2), 2) == true\n\n\n\n\n\n","category":"method"},{"location":"extraction/#SemanticModels.ExprModels.Parsers.parsefile","page":"Knowledge Extraction","title":"SemanticModels.ExprModels.Parsers.parsefile","text":"parsefile(path)\n\nread in a julia source file and parse it.\n\nNote: If the top level is not a simple expression or module definition the file is wrapped in a Module named modprefix.\n\n\n\n\n\n","category":"function"},{"location":"extraction/#SemanticModels.ExprModels.Parsers.postwalk-Tuple{Any,Any}","page":"Knowledge Extraction","title":"SemanticModels.ExprModels.Parsers.postwalk","text":"postwalk(f, expr)\n\nApplies f to each node in the given expression tree, returning the result. f sees expressions after they have been transformed by the walk. See also prewalk.\n\n\n\n\n\n","category":"method"},{"location":"extraction/#SemanticModels.ExprModels.Parsers.prewalk-Tuple{Any,Any}","page":"Knowledge Extraction","title":"SemanticModels.ExprModels.Parsers.prewalk","text":"prewalk(f, expr)\n\nApplies f to each node in the given expression tree, returning the result. f sees expressions before they have been transformed by the walk, and the walk will be applied to whatever f returns. This makes prewalk somewhat prone to infinite loops; you probably want to try postwalk first.\n\n\n\n\n\n","category":"method"},{"location":"extraction/","page":"Knowledge Extraction","title":"Knowledge Extraction","text":"[1]: https://arxiv.org/abs/1803.09473","category":"page"},{"location":"news/#News","page":"News","title":"News","text":"","category":"section"},{"location":"news/#Release-v0.3.0","page":"News","title":"Release v0.3.0","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Release v0.3.0 is a major enhancement to the new model augmentation tools introduced the the last release. This includes a more robust design, easier model implementation, and more built in features to augment and compose models.","category":"page"},{"location":"news/","page":"News","title":"News","text":"New submodules\nModelTools\nSemanticModels.ModelTools.CategoryTheory the main module that contains the category theory based building blocks for model augmentation\nSemanticModels.ModelTools.CategoryTheory.AbstractMorph abstract type for representing morphisms\nSemanticModels.ModelTools.CategoryTheory.FinSetMorph morphism in the category of finite sets\nSemanticModels.ModelTools.CategoryTheory.GraphMorph morphism in the category of graphs\nSemanticModels.ModelTools.CategoryTheory.‚äî get the union of two categorically defined models or morphisms\nSemanticModels.ModelTools.CategoryTheory.Decorated a type representing a decoration applied to the objects of a morphism\nSemanticModels.ModelTools.CategoryTheory.AbstractSpan an abstract type for representing spans\nSemanticModels.ModelTools.CategoryTheory.Span general span of two morphisms\nSemanticModels.ModelTools.CategoryTheory.AbstractCospan an abstract type for representing cospans\nSemanticModels.ModelTools.CategoryTheory.Cospan general cospan of two morphisms\nSemanticModels.ModelTools.CategoryTheory.pushout solve the pushout defined by a span\nSemanticModels.ModelTools.PetriModels Extends ModelTools and the new CategoryTheory API to support models defined in Petri.jl\nSemanticModels.ModelTools.OpenModels module for defining an open model where there are defined inputs and outputs, domain and codomain\nSemanticModels.ModelTools.OpenPetris module for implementing the open petri model, and converting a PetriModel to an OpenPetri\nSemanticModels.ModelTools.OpenPetris.otimes combine two open petri models in parallel\nSemanticModels.ModelTools.OpenPetris.compose combine two open petri models in series\nNew examples\ndecorations/graphs.jl example of using the new Category Theory based Morphism API to combine graphs and Petri models\npetri/malaria.ipynb example of utilizing the new OpenModel API to combine a Lotka Volterra model and an Epidemiology model to simulate Malaria spreading between a population\npetri/rewrite_demo.jl example of using rewrite rules to augment a Petri model\npetri/rewrite.jl more detailed example of using rewrite rules to augment a Petri model and then solving the new models using both agent based models and differential equations\npetri/wiring_petri.jl example of creating a model using wiring diagrams, converting that to a Petri model, and solving\nNew docs pages\nRemoved Dubstep\nUpdates to ModelTools\nUpdates to Theory\nReplaced Flu Model walkthrough with Malaria example","category":"page"},{"location":"news/#Release-v0.2.0","page":"News","title":"Release v0.2.0","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Release v0.2.0 include an inital version of our new model augmentation tools and program analysis features.","category":"page"},{"location":"news/","page":"News","title":"News","text":"New submodules\nModelTools\nSemanticModels.ModelTools.model the main constructor for building model instances\nTools for accessing parts of expressions\nSemanticModels.ModelTools.head gets the head of an Expr or nothing for LineNumberNodes\nSemanticModels.ModelTools.bodyblock gets the body of the func\nSemanticModels.ModelTools.argslist get the array of args representing the arguments of a defined function\nSemanticModels.ModelTools.funcname get the function name from an expression object return :nothing for non function expressions\nSemanticModels.ModelTools.callsites collects func calls\nSemanticModels.ModelTools.structured extract the expressions that use structuring\nTools for modifying expressions\nSemanticModels.ModelTools.pusharg! push a new argument onto the definition of a function.\nSemanticModels.ModelTools.setarg! replace the argument in a function call\nPredicates useful for manipulating expressions\nSemanticModels.ModelTools.or or\nSemanticModels.ModelTools.and and\nSemanticModels.ModelTools.issome predicate for being neither missing or nothing\nSemanticModels.ModelTools.isblock predicate for an expression being a block node\nSemanticModels.ModelTools.isfunc predicate for an expression being a function\nSemanticModels.ModelTools.isexpr checks if expr\nSemanticModels.ModelTools.iscall checks if func call\nSemanticModels.ModelTools.isusing checks for using statement\nSemanticModels.ModelTools.isimport checks for import\nPrebuilt model types\nSemanticModels.ModelTools.AbstractModel the abstract type for generic models\nSemanticModels.ModelTools.SimpleModels.SimpleModel represents generic models with just blocks and functions\nSemanticModels.ModelTools.ExpStateModels.ExpStateModel represents a simple Agent Based Modeling (ABM) framework\nSemanticModels.ModelTools.ExpStateModels.ExpStateTransition represents ABM transitions\nSemanticModels.ModelTools.ExpODEModels.ExpODEModel represents ODEProblems as code expressions\nType Graphs\nSemanticModels.ModelTools.typegraph annotate a code expression so that when you eval it, you get the typegraph\nSemanticModels.ModelTools.@typegraph extract a typegraph from a block of code\nExamples\nagentbased.jl we define a modeling microframework for Agent Based Modeling (ABM)\nagentbased2.jl a more refined version of agentbased.jl that uses singleton types\nagentgraft.jl an example of model augmentation for ABM\nagenttype.jl example of how to reverse engineer a model structure with the typegraph function\ndefintions.jl\ndiffeq.jl\nmacromodel.jl example of using modeltools as a macro\nregression.jl basic regression script\nmonomial_regression.jl transformations on linear regression build nonlinear regression\nmultivariate_regression.jl product group to manipulate a multivariate regression model\nodegraft.jl updated version of our old graft.jl script\npseudo_polynomial_regression.jl transforms linear regression into a 1 degree of freedom polynomial regression\npolynomial_regression.jl manipulate a univariate linear regression model into polynomial regression\nworkflow.jl model synthesis with a pipeline of agentgraft.jl and polynomial_regression.jl\nNew docs pages\nModelTools\nUpdates to Theory\nUpdated examples\nRemoved Functionality\nSemanticModels.Graph, replaced by SemanticModels.ModelTools.typegraph\nParsers.edges, replaces by typegraph and MetaGraphs.edges(g)","category":"page"},{"location":"news/#Release-v0.1","page":"News","title":"Release v0.1","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Release v0.1 includes an initial version of every step in the SemanticModels pipeline. Users can now extract information, build knowledge graphs, and generate new models.","category":"page"},{"location":"news/","page":"News","title":"News","text":"The following is a summary of the most important new features and updates:","category":"page"},{"location":"news/","page":"News","title":"News","text":"New submodules\nDubstep\nSemanticModels.Dubstep.TraceCtx builds dynamic analysis traces of a model for information extraction.\nSemanticModels.Dubstep.LPCtx allows you to modify the norms used in a model.\nSemanticModels.Dubstep.GraftCtx allows grafting components of one model onto another.\nParsers\nParsers.parsefile reads in a julia source file as an expression.\nParsers.defs extracts  all of the code definitions from a module definition expression.\nParsers.edges extracts edges for the knowledge graph from code.\nGraphs\nA knowledge graph schema Knowledge Graphs.\nGraphs.insert_edges_from_jl builds a knowledge graph from extracted edges.\nExamples\ntest/transform/ode.jl shows how to perturb an ODE with overdub.\ntest/transform/varextract.jl shows how to use a compiler pass to extract dynamic analysis information.\nScripts\nbin/extract.jl extracts knowledge elements from parsed markdown and source code files.\nbin/graft.jl performs metamodeling by grafting a component of one model onto another.\nNew docs pages\nIntended Use Cases\nDubstep\nKnowledge Graphs\nKnowledge Extraction\nModel Validation with Dynamic Analysis\nSemantic Modeling Theory\nDeveloper Guidelines","category":"page"},{"location":"news/#Release-v0.0.1","page":"News","title":"Release v0.0.1","text":"","category":"section"},{"location":"news/","page":"News","title":"News","text":"Initial release with documentation and some examples designed to illustrate the inteded scope of the software.","category":"page"},{"location":"kgtypes/#Multiple-Knowledge-Graphs","page":"Multiple Knowledge Graphs","title":"Multiple Knowledge Graphs","text":"","category":"section"},{"location":"kgtypes/","page":"Multiple Knowledge Graphs","title":"Multiple Knowledge Graphs","text":"The input data is a model as defined in a script which contains 1 top level module and a main function. The default name for the main function is main but you could pass in a different name if you wanted to.","category":"page"},{"location":"kgtypes/","page":"Multiple Knowledge Graphs","title":"Multiple Knowledge Graphs","text":"Example:","category":"page"},{"location":"kgtypes/","page":"Multiple Knowledge Graphs","title":"Multiple Knowledge Graphs","text":"module Foo\nusing Roots\na = 3\nb = 4\nc = -1\nf(x,y) = c*x*y + a*x + b\n\nfunction main()\n    x0 = [0.0,0.0]\n    xstar = RootProblem(f, x0)\n    @show xstar\nend\nend #module","category":"page"},{"location":"kgtypes/","page":"Multiple Knowledge Graphs","title":"Multiple Knowledge Graphs","text":"There are a few different forms of knowledge graphs that can be extracted from codes.","category":"page"},{"location":"kgtypes/","page":"Multiple Knowledge Graphs","title":"Multiple Knowledge Graphs","text":"The type graph: Vertices are types, edges are functions between types\nVertices are functions and variables, edges represent dataflow, function references variable or function calls function.\nConceptual knowledge graph from text, vertices are concepts edges are relations between concepts.","category":"page"},{"location":"kgtypes/#Linking-KGs","page":"Multiple Knowledge Graphs","title":"Linking KGs","text":"","category":"section"},{"location":"kgtypes/","page":"Multiple Knowledge Graphs","title":"Multiple Knowledge Graphs","text":"Between different scripts we should be able to link the graph by defining an alias relation that says \"these vertices are equivalent\" and then merging the graphs.","category":"page"},{"location":"kgtypes/","page":"Multiple Knowledge Graphs","title":"Multiple Knowledge Graphs","text":"With a script we should be able to merge the types of graph by converting the type graph into its pseudodual. The pseudodual is constructed by take a type-function graph and constructing a new graph where functions and types are both vertices, if U = typeof(f(::V)) then there is a pair of edges V -> f -> U and there are edges for the functions getindex(u::U, v::V) ie (U, V) -> getindex -> typeof(u[v]) for all the values of v. These represent untupling and accessing fields of structs.","category":"page"},{"location":"kgtypes/#How-do-KGs-related-to-Use-Cases?","page":"Multiple Knowledge Graphs","title":"How do KGs related to Use Cases?","text":"","category":"section"},{"location":"kgtypes/","page":"Multiple Knowledge Graphs","title":"Multiple Knowledge Graphs","text":"The different types of knowledge graph that can be extracted can help address the use cases in different ways.","category":"page"},{"location":"kgtypes/","page":"Multiple Knowledge Graphs","title":"Multiple Knowledge Graphs","text":"Model Augmentation: we need dataflow, types, and concepts for implementing the \"frontend\" of ModelTool. This is an informative step to show a person extending SemanticModels how to implement ingestion for a new class of models. Once we have the new class of models implemented, we only need Exprs and do not necessarily need the KG to do model augmentation.\nMetamodel construction: We need the type graph for program refinement and the dataflow and concept graphs to do the metamodeling reasoning. This part will probably leverage all the graphs at run time when solving for the combined model.\nModel Validation: we need the structured representation of the model that is used in model augmentation and the trace of execution that follows the same lines as the traces used to build the dataflow and type-function graph. I don't think this needs the KG directly unless we find that we can build better models with the KG than with the trace. I think the trace is more useful because it is hierarchical and DNNs work better on trees than on general graphs.","category":"page"},{"location":"usecases/#Intended-Use-Cases","page":"Intended Use Cases","title":"Intended Use Cases","text":"","category":"section"},{"location":"usecases/","page":"Intended Use Cases","title":"Intended Use Cases","text":"Here are some use cases for SemanticModels.jl","category":"page"},{"location":"usecases/","page":"Intended Use Cases","title":"Intended Use Cases","text":"Scientific knowledge is richer than the ability to make predictions given data. Knowledge and understanding provide the ability to reason about novel scenarios.  A crucial aspect of acquiring knowledge is asking questions about the world and answering those questions with models.","category":"page"},{"location":"usecases/","page":"Intended Use Cases","title":"Intended Use Cases","text":"Suppose the model is  dudt = f_p(ut) where u is the observable, dudt is the derivative, f is a function, t is the time variable, and p is a parameter. ","category":"page"},{"location":"usecases/","page":"Intended Use Cases","title":"Intended Use Cases","text":"Scientific knowledge involves asking and answering questions about the model. For example:","category":"page"},{"location":"usecases/","page":"Intended Use Cases","title":"Intended Use Cases","text":"How does u depend on p?\nHow does u depend on f?\nHow does u depend on the implementation of f?","category":"page"},{"location":"usecases/#Counterfactuals","page":"Intended Use Cases","title":"Counterfactuals","text":"","category":"section"},{"location":"usecases/","page":"Intended Use Cases","title":"Intended Use Cases","text":"Scientists often want to run counterfactuals through a model. they have questions like: ","category":"page"},{"location":"usecases/","page":"Intended Use Cases","title":"Intended Use Cases","text":"What if the parameters were different?\nWhat if the functional form of this equation was different?\nWhat if the implementation of this function was different?","category":"page"},{"location":"usecases/","page":"Intended Use Cases","title":"Intended Use Cases","text":"The \"how\" questions can be answered by running counterfactuals of the model. In order to run counterfactuals we need to modify the code.  The current approach is for scientists to modify code writen by other scientists. This takes a long time and requires models to be converted from the modeling level to the code level,  then someone else reads the code and converts it back to the modeling level.","category":"page"},{"location":"usecases/","page":"Intended Use Cases","title":"Intended Use Cases","text":"If we could automate these transformations, we could enable scientists to spend more time  thinking about the science and less time working with code. ","category":"page"},{"location":"usecases/#Model-Code-Transformations","page":"Intended Use Cases","title":"Model-Code Transformations","text":"","category":"section"},{"location":"usecases/","page":"Intended Use Cases","title":"Intended Use Cases","text":"There are many places we could modify code in order to give it new features for modeling.","category":"page"},{"location":"usecases/","page":"Intended Use Cases","title":"Intended Use Cases","text":"Source Code, changing the source files on disk before they are parsed\nExpressions, after parsing, we could use macros or Meta.parse to get Exprs and make new ones to eval\nType System, using multiple dispatch with new types to get new behavior\nOverdubbing, Cassette.jl lets you change the definitions of functions with overdub\nContextual Tags, Cassette provides a tagging mechanism attach metadata to values\nCompiler Pass, Cassette lets you implement your own compiler passes","category":"page"},{"location":"usecases/","page":"Intended Use Cases","title":"Intended Use Cases","text":"Different code modifications will be easier at different levels of this toolchain.","category":"page"},{"location":"usecases/#Use-Cases","page":"Intended Use Cases","title":"Use Cases","text":"","category":"section"},{"location":"usecases/","page":"Intended Use Cases","title":"Intended Use Cases","text":"Answering counterfactuals\nInstrumenting code to extract additional insight\nSemantic Model Validation","category":"page"},{"location":"usecases/#Answering-Counterfactuals","page":"Intended Use Cases","title":"Answering Counterfactuals","text":"","category":"section"},{"location":"usecases/","page":"Intended Use Cases","title":"Intended Use Cases","text":"Scientists want to change 1) parameters, 2) assumptions, 3) functions, or","category":"page"},{"location":"usecases/","page":"Intended Use Cases","title":"Intended Use Cases","text":"implementations in order to determine their effects on the output of the model.","category":"page"},{"location":"usecases/","page":"Intended Use Cases","title":"Intended Use Cases","text":"Note: a paramter is an argument to the model and is intended (by the simulation author) to be changed by users. An assumption is a value in the code that could be changed, but is not exposed to the API.","category":"page"},{"location":"usecases/","page":"Intended Use Cases","title":"Intended Use Cases","text":"While making accurate predictions of measurable phenomena is a necessary condition of a scientific knowledge it is not sufficient. Scientists have knowledge that allows them to reason about novel scenarios and they do this by speculating about counterfactuals. Thus answering counterfactuals about model codes form a foundational capability of our system.","category":"page"},{"location":"usecases/#Instrumenting-Model-Code","page":"Intended Use Cases","title":"Instrumenting Model Code","text":"","category":"section"},{"location":"usecases/","page":"Intended Use Cases","title":"Intended Use Cases","text":"In order to get additional insight out of models, we want to add instrumentation into the bodies of the functions. These instrumented values will be useful for many purposes. The simplest use is to add instrumentation of additional measurements. Scientists write code for a specific purposes and do not take the time to report all possible measurements or statistics in their code. A second scientist who is trying to repurpose that software will often need to compute different values from the internal state of the algorithm in order to understand their phenomenon of interest.","category":"page"},{"location":"usecases/","page":"Intended Use Cases","title":"Intended Use Cases","text":"A simple example is a model that simulates Lotka-Volterra population dynamics and reports the average time between local maxima of predator populations. A second scientist might want to also characterize the variance or median of the time between local maxima.","category":"page"},{"location":"usecases/#Semantic-Model-Validation","page":"Intended Use Cases","title":"Semantic Model Validation","text":"","category":"section"},{"location":"usecases/","page":"Intended Use Cases","title":"Intended Use Cases","text":"One could trace the value of variables as the code  is run in order to build up a distribution of normal values that variable takes. This could be used to learn implied invariants in the code. Then when running the model in a new context, you could compare the instrumentation values to these invariants to validate if the model is working as intended in this new context.","category":"page"},{"location":"usecases/","page":"Intended Use Cases","title":"Intended Use Cases","text":"One of the main benefits of mechanistic modeling over statistical modeling is the generalization of mechanistic models to novel scenarios. It is difficult to determine when a model is being applied in a novel scenario where we can trust the output and a novel scenario that is beyond the bounds of the model's capability. By analyzing the values of the internal variables in the algorithms, we can determine whether a component of the model is operating outside of the region of inputs where it can be trusted.","category":"page"},{"location":"usecases/","page":"Intended Use Cases","title":"Intended Use Cases","text":"An example of this validation could be constructed by taking a model that uses a polynomial approximation to compute a function f(x). If this polynomial approximation has small error on a region of the input space, R then whenever x is in R, we can trust the model. But if we every run the model and evaluate the approximation on an x outside of this region, we do not know if the approximation is close, and cannot trust the model. Program analysis can help scientists to identify reasons to be sceptical of model validity.","category":"page"},{"location":"theory/#Semantic-Modeling-Theory","page":"Theory","title":"Semantic Modeling Theory","text":"","category":"section"},{"location":"theory/#What-is-a-model?","page":"Theory","title":"What is a model?","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"The goal of science is to build models of natural phenomena that explain the world. Scientists build these models by conducting data gathering phenomena and using math to represent these experiments. This sets up a balancing act between the two traits that make for a good scientific model, it must first match the data collected and also be able to explain the phenomena.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We can think of fitting the data as a regression problem:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"h^* = min_hin H ell(h(x) y)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"and the institutional process of discovery as","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"max_Hin mathcalM expl(h^*)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"where expl is the explanatory power of a class of models H. The explanatory power is some combination of generalization, parsimony, and consistency with the fundamental principles of the field.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"This formulation is notional in the current state of the art, because models are not a well parameterized space. The goal of this project is to identify subspaces that can be parameterized using algebraic structures and represent those subspace symbolically so that computers can represent them and perform optimization over model structure in addition to model parameters.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The philosophy of science Kuhn 1962 tells us that scientists don't make models in a vacuum, they are usually working within a scientific paradigm. These paradigms are often given names like \"Newton's Theory of Classicial Mechanics\" or \"Einstein's Theory of General Relativity\". Within these theories, specific scenarios are explained with models which instantiate the theory. While in scientific practice there are many types of theories, some more mathematical than others, we will focus on modeling frameworks as implementations of scientific theories and the models you can build within a framework as implementations of scientific models within that theory.","category":"page"},{"location":"theory/#Modeling-Frameworks-as-a-Paradigm-of-Paradigms","page":"Theory","title":"Modeling Frameworks as a Paradigm of Paradigms","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"Computational Science is a scientific discipline concerned with the interaction of computations, mathematics, and domains science and thus, must have a paradigm of its own. In the pre-paradigm phase scientists just wrote Fortran programs to make their models and would have been unable to articulate a theory of which programs constitute a class of models. The dominant paradigm currently is the modeling framework, where a small group of scientists write a library or design a programming language for expressing all possible models within a scientific domain and then a larger group of scientists use the library in diverse experimental or theoretical contexts. The rules of the domain are a \\\"scientific paradigm\\\" and the modeling framework is an implementation of it in software.","category":"page"},{"location":"theory/#History-of-the-Modeling-Framework-Paradigm","page":"Theory","title":"History of the Modeling Framework Paradigm","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"When scientists were first given access to computers, they wrote programs directly and every scientist doing computational work needed to write FORTRAN programs in order to implement their ideas. This worked for a while until the complexity of the algorithms needed to solve models on increasingly complex hardware exceeded the ability of scientists to become an expert in both the scientific domain and computer programming. Thus, the first modeling languages were born. Tools like MATLAB and Mathematica allowed scientists to learn a programming language that hid the complexity of computer programming from them and allowed them to focus on their domain knowledge and model construction.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Modeling languages more tightly coupled to the scientific discipline achieved great success. This includes Stan for Bayesian models, Simulink for signal processing with dataflow programs, and AML for algebraic modeling of optimization problems.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"This paradigm flourished from the late 1970s until modern systems. The flaws in the monolithic modeling language were exposed by software libraries embedded in fully featured programming languages. Python and R for scientific modeling and statistical modeling respectively after the late 2000s. As the models grew in complexity and the scientific workloads became more data driven, scientists had to do more diverse computing tasks such as downloading and cleaning data from websites, interacting with network APIs, and managing large data files.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The frameworks have been able to succeed because they separate concerns, the implementers of the framework carve out a space of scientific computing to address (for example signal flow graphs or finite element simulations) and implement the mathematical algorithms of that scientific domain. The frameworks closely match the underlying scientific paradigms that they implement. Then the modelers can proceed in what Kuhn calls, normal science whereby they make and interrogate models within the framework. When the modelers get to the point were the framework cannot handle their problems (a crisis) they expand the framework or write a new one.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Connections to Algebraic Theories\nMathematical Reasoning has been the dominant paradigm of  mathematics since Descartes and over the centuries various  subfields of mathematics have undergone crises and paradigm  shifts such as the early 20th century revolutions of formal  logic, the everyday practice of modern mathematics would be very  familiar to the early modern scholars.\nIn the field of Algebra, the study of mathematical structure,  there are various theories that represent different types of  mathematical structures. For example, groups represent systems  of multiplication with identity, and rings are systems of  addition and multiplication that respect a distributive law. The  algebraic theory provides for a types of elements and tuple of  operations that must satisfy some equations. Each theory of  algebra is a different type of number system that can be used  for multiple scientific and engineering applications. Within a  theory of algebra, there are models that instantiate the  theory with specific sets of elements and specific operations  that satisfy the equations. Practitioners of algebra study both  the theory in general and specific models in particular. Thus  theorems in group theory come primarily in two varieties, \\\"let  G be a group ...\\\" or \\\"let D_n be the dihedral group on n  points\\\" and then go on to state a consequent either for the  theory or the named models.\nWhat is a Framework?\nSoftware modeling frameworks are similar, in that they define a  theory of possible models that can be implemented with the  framework, and provide algorithms for answering queries about  those models. Since modeling frameworks are often built out of  programming languages in an ad hoc manner, they suffer flaws  of design. We propose two criteria for evaluating the quality of  a modeling framework:\ncorrectness, the framework can represent all models  within the domain paradigm it purports to implement, and\nefficiency, the framework provides efficient solvers for  real world problems of \"normal science\" within that  paradigm.\nSuch software then implements a scientific paradigm. The scope  of a paradigm is determined by the practitioners of the field.  The community of users of a framework together form a discipline  of scientific computing. A good modeling framework should have a  tight correspondence to a specific mathematical or scientific  theory. For example a system for modeling chemical systems  should use a syntax similar to the reaction network equations of  modern chemistry and be able to efficiently compute equilibria  and dynamics of complex chemical systems.\nSome frameworks are so broad that they seem to span multiple  paradigms. For example, the modeling framework MATLAB defines  its scope as numerical computing, which is so broad so as to  seem all encompassing. Since existing modeling frameworks  developed organically in response to the needs of practical  scientists and engineers, they have unclear and disputed  boundaries not unlike borders on a map of unexplored territory.  Typically, the closer a domain specific modeling language  matches the domain it implements, the more powerful the tools  for analyzing the models, at the other end of the spectrum are  general purpose programming languages, which can express any  computable model, but cannot provide insight in a structured  way. One goal of computational science and engineering is to  provide formal constructions of modeling frameworks given a  formal definition of a scientific paradigm.","category":"page"},{"location":"theory/#Problems-in-the-Modeling-Framework-Paradigm","page":"Theory","title":"Problems in the Modeling Framework Paradigm","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"As the complexity of our models grows and the diversity of our analysis of the model increases, the modeling framework paradigm faces growing problems. The acute problems of the domain are:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Problem                          Attempted Solutions   ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì   Efficient Simulation of Models   Auto Parallelizing Compilers   Correctness V&V                  Unitful Quantities and Type Systems   Model Fusion                     Networking protocols and API standardization   Model Selection                  Parameterizing the space of models and applying genetic algorithms   Data Driven Modeling             Replace deterministic components of models with ML models   Parameter Estimation             Gradient free optimization, approx bayesian computing, autograd   Optimization                     Auto differentiation through programs","category":"page"},{"location":"theory/#The-Computational-Science-of-the-Future","page":"Theory","title":"The Computational Science of the Future","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"While scientists have been using framework driven computing for decades, mathematicians have been unifying the two great pillars of modern mathematics ‚Äì Analysis, the mathematics evolved from differential and integral calculus, and Algebra, the study of mathematical structure. This grand unified theory is Category Theory, which was developed to unify Algebraic Topology, but has recently found direct application to the analysis of scientific systems.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Scientific theories like the chemical reaction networks of Petri are amenable to formalization and explanation as categories, where the algebraic and analytic structures are made explicit. With applied category providing a new generation of applied mathematics to understand scientific phenomena, and the deep connection between category theory and functional programming, a new generation of scientific software can be developed.","category":"page"},{"location":"theory/#Software-that-understands-science","page":"Theory","title":"Software that understands science","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"Just as the structure of the modeling framework forces the modeler to stay within the paradigm, ACT based modeling software will provide structure to the programs written by scientists. The benefit of ACT based software is that the paradigmatic rules will be made explicit. They set of objects in a category define the scope of concepts that can be included in a model, and the set of primitive morphisms define the possible relationships between those concepts. Thus by specifying a category, you explicitly construct the rules of the modeling paradigm.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Once made precise, this structure can be exploited by algorithms that understand the modeling framework. Modeling frameworks that form a monoidal category provide for combination of models by sequential and parallel composition. Modeling frameworks that form an adhesive category provide also the structure of double pushout (DPO) rewriting, which enables reasoning over model space by analogy.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"In order to say that algorithms understand models, we must operationalize a definition of understanding.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Rejection of ill-formed models: any modeling framework  defines a scope of possible models and must be able to detect a  program that specifies a model outside that scope. Scientists  use a lay type system when checking formulas by hand. This type  system gives all quantities a sort which allows them to check,  \\\"this formula doesn\\'t make sense, because it combines two  numbers of different sorts\\\". For example unit-checking is a  form of type checking that rejects mechanical models that add  quantities with different units, such as 5m + 3ms. Any  modeling framework worth using must have the ability to  recognize well formed models.\nExplanation of model differences: a human scientist can see  multiple scientific models and describe the differences. For  example \\\"the heat equation on a square surface with Dirichlet  boundary conditions\\\" differs from \\\"the heat equation on a  circular surface with von Neumann boundary conditions\\\" in both  surface shape and boundary conditions aspects. Scientists  working within a common paradigm have a similar  conceptualization of these aspects and how models inhabit them,  even if they do not have a formal system of describing these  aspects. An algorithm that understands models should be able to  explain such differences when presented with similar models.\nComprehension of model neighborhoods: a useful technique for  model construction is to take an existing model from the  literature and perturb it into a novel model that is adapted to  the current experimental or observational context. An algorithm  that understands models should be able to provide novel models  that are in the neighborhood of any given model.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"These three types of model comprehension are interlinked. The rejection of ill-formed models (or recognition of well-formed models) defines a scope of all possible models that is used in framing the explanation of model differences and comprehension of model neighborhoods. To describe model differences in terms of aspects is one way to parameterize model neighborhoods and then specify the relative location of two models within the implied topology.","category":"page"},{"location":"theory/#New-Problems","page":"Theory","title":"New Problems","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"If computational science is to be thought of as a discipline in its own right, it must have content beyond, \"the parts of applied math and computer science that are useful to scientists.\" We can outline several problems of interested within this discipline.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Modeling Framework Construction: Given a definition of the scientific  theory as a category, generate a domain specific language for expressing  models within that theory. Such models should be easy to write and the  system should retain enough of the problem structure to execute solution  algorithms.\nModeling Framework Analysis: Given a DSL and solver algorithm, derive a  presentation of a category. This is the inverse problem of the modeling  framework construction problem.\nCompositional solvers: If the modeling framework is compositional, for  instance based on monoidal categories, then the solution to scientific  queries about that model should be compositional. Any property of the model  should be able to be computed based on the properties of its components. For  example, a chemical equilibrium of two independent systems is a combination  of the equilibria of the two component systems. Numerical solution  algorithms that exploit this compositional structure of the models should be  faster or more accurate than naive techniques. The development of  compositional solvers should alleviate the crisis of complexity presented by  multiscale and multiphysics simulations.\nUniversal data structures for partially symbolic computing: Given  intimate connections between applied category theory and diagrammatic  languages, there should be universal data structures for representing  models. For example in any monoidal category, there is a system of wiring  diagrams that can be used to draw morphisms. Algorithms for analyzing models  that are built on manipulations of these data structures will be universal  across modeling frameworks. By developing algorithms that work with models  from multiple frameworks, we can make leverage increasingly sophisticated  algorithms while proliferating frameworks to every area of science.\nAutomated Metamodeling: A lot of scientific activity involving models  involves higher order functions on models. Model selection is a process that  takes a class of models and a data set and returns the best model from the  class given the data. Sensitivity analysis and uncertainty quantification  are modeling activities that take a model and provide another model that  computes the sensitivities or uncertainties of the given model. Once  algorithms can be built that understand the rules of a scientific paradigm,  these metamodeling activities can be automated\nModel Fusion: Given models in two different frameworks, design and implement an interface that allows you to combine those two models in a meaningful way.","category":"page"},{"location":"theory/#Categories-for-Science","page":"Theory","title":"Categories for Science","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"Dan Spivak wrote a wonderful book, CT4S, on category theory for scientists based on his lectures at MIT.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Data gathering is ubiquitous in science. Giant databases are currently being mined for unknown patterns, but in fact there are many (many) known patterns that simply have not been catalogued. Consider the well-known case of medical records. A patient‚Äôs medical history is often known by various individual doctor-offices but quite inadequately shared between them. Sharing medical records often means faxing a hand-written note or a filled-in house-created form between offices.Similarly, in science there exists substantial expertise making brilliant connections between concepts, but it is being conveyed in silos of English prose known as journal articles. Every scientific journal article has a methods section, but it is almost impossible to read a methods section and subsequently repeat the experiment‚Äîthe English language is inadequate to precisely and concisely convey what is being done","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"This is the point of our project, to mine the code and docs for the information necessary to repeat and expand scientific knowledge. Reproducible research is focused on getting the code/data to be shared and runnable with VMs/Docker etc are doing the first step. Can I repeat your analysis? We want to push that to expanding.","category":"page"},{"location":"theory/#Ologs","page":"Theory","title":"Ologs","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"Ontology logs are a diagrammatic approach to formalizing scientific methodologies. They can be used to precisely specify what a scientist is talking about (see Spivak, Kent 2012 \"Ologs: A Categorical Framework for Knowledge Representation.\").","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"An olog is composed of types (the boxes) and aspects (the edges). The labels on the edges is the name of the aspect. An aspect is valid if it is a function (1-many relation).","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"(Image: Birthday olog)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We can represent an SIR model as an olog as shown below.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"(Image: SIR olog)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Another category theory representation without the human readable names used in an olog shows a simpler representation.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"(Image: SIR Category)","category":"page"},{"location":"theory/#Models-in-the-Category-of-Types","page":"Theory","title":"Models in the Category of Types","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"All programs in a strongly typed language have a set of types and functions that map values between those types. For example the Julia program","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"a = 5.0\nb = 1\nc = 2*a\nd = b + c","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Has the types Int, Float and functions *, + which are both binary functions. These types and functions can be represented as a category, where the objects are the types and the morphisms are the functions. We refer to the input type of a function as the domain and the output type as the codomain of the function. Multi-argument functions are represented with tuple types representing their argument. For example +(a::Int,b::Int)::Int is a function + Inttimes Int - Int. These type categories are well studied in the field of Functional Programming. We apply these categories to the study of mathematical models.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"One can use a combination of static and dynamic analysis to extract this category representation from a program and use it to represent the model implemented by the code.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The most salient consequence of programming language theory is that the more information that a programmer can encode in the type system, the more helpful the programming language can be for improving performance, quality, and correctness.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We want to leverage the type system to verify the semantic integrity of a model. This is critical when pursuing automatic model modification. Model developers use any number of conventions to encode semantic constraints into their code for example, prefacing all variables that refer to time with a t, such as t_start, s_end. This semantic constraint that all variables named t_ are temporal variables is not encoded in the type system because all those variables are still floats. Another example is that vectors of different lengths are incompatible. In a compartment model, the number of initial conditions must match the number of compartments, and the number of parameters may be different. For example in an SIR model there are 3 initial conditions, SIR and there are 2 parameters beta gamma. These vectors are incompatible, you cannot perform arithmetic or comparisons on them directly. Most computational systems employed by scientists will use a runtime check on dimensions to prevent a program from crashing on malformed linear algebra. Scientists rely on this limited from of semantic integrity checking provided by the language.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Our goal is to extract and encode the maximum amount of information from scientific codes into the type system. The type system is analyzable as a category. Thus we can look at the category of types and analyze the integrety of the programs. For example if there are two types ST and two functions fg Srightarrow T such that Codom(f) = Codom(g) but Range(f) cap Range(g), then we say that the type system is ambiguous in that there are two functions that use disjoint subsets of their common codomain. In order to more fully encode program semantics into the type system, the programmer (or an automated system) should introduce new types to the program to represent these disjoint subsets. (Image: Ambiguous types in the SIR Model) Returning to the SIR model example, the .param and .initial functions both map Problem to Vector{Float} but have disjoint ranges. From our mathematical understanding of the model, we know that parameters and initial conditions are incompatible types of vectors, for one thing the output of .param is length 2 and the output of .initial is length 3. Any program analysis of the model will be hampered by the ambiguity introduced by using the same type to represent two different concepts. On the other hand, .first and .second have overlapping ranges and are comparable as times.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"(Image: Unambiguous types in the SIR Model)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"This is an example of how PL theory ideas can improve the analysis of computational models.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Teaching the type system about agent based modeling. In the example notebook /examples/agenttypes2.jl you can see how to embed model structure into the julia type system. That example uses two versions of the same agent based model of disease. In the first implementation, the agents have states represented by the julia type :Symbol with values :S, :I, :R, and in the second, more refined implementation, the agent's states are represented by the singleton types Susceptible, Infected, Recovered with values, Susceptible(), Infected(), Recovered(). The model is the same, but the type system contains more information about the execution of the model. For example the julia type system knows what the possible state transitions are based in the second implementation, while the first model has a black box of :Symbols that are not distinguishable in program analysis.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The original type graph g shows how the model works. (Image: Using Symbol values to represent states)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The refined model has a typegraph G, which includes the new singleton types as well as different tuple types.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We can establish a graph homomorphism phi G to g such that phi(v) = v for all v in V(g) cap V(G). The following figure shows this homomorphism by drawing vertices vin G with the same color as phi(v)in g. (Image: Using Symbol values to represent states) Within this typegraph we have a subgraph that contains the state transitions for the agents. We can draw this subgraph separately to show how the compiler has been taught to understand the semantics of the model. (Image: The typegraph understand the transition graph of the agents)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The embedding of model semantics into the type system enables programs to reason over the behavior of the models.","category":"page"},{"location":"theory/#Theoretical-Understanding-of-Modeling","page":"Theory","title":"Theoretical Understanding of Modeling","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"Models are mathematical objects that behave like functions, but are not well captured by the ordinary definition of mathematical function. They are also not captured by programming language functions or subroutines. Mathematical functions only care about the association between input and output values of the function, and programming languages are concerned with troublesome implementation details. We need a representation of models that is aware of the internal structure of the model, but also captures the functional nature. We also think of models as representing systems, and a fundamental property of systems is that you can compose them to get complex systems out of simples ones.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Category Theory (CT) generalizes the notion of sets and functions to discuss objects and morphisms. In our notation, a modeling framework corresponds to a category, and the models that you can express in that framework are the morphisms in that category.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Monoidal Categories are Categories that have two types of composition (circ otimes), the first corresponds to sequential processes and resembles the function composition. The second, represents parallel or independent combination of two models.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Objects: WXYZ\nMorphisms: fXrightarrow Y, gYrightarrow Z\nComposition: gcirc f = fgXrightarrow Z\nCombination: fXrightarrow Y g Wrightarrow Z implies fotimes g (Xotimes W) rightarrow (Yotimes Z)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"In this language we call the objects domains, the morphisms models. Composition is feeding the output of one model as the input to another, and fotimes g is parallel or independent combination of f and g. There are equations for circ otimes like associativity that are necessary to show that circ otimes respect each other. We consider each Monoidal Category (MC) to be an example of a modeling framework. Many existing areas of mathematical modeling were developed as MCs without knowing it, because the idea of composition and combination are very natural for decomposing physical systems into their component parts.","category":"page"},{"location":"theory/#Examples-of-Modeling-Frameworks","page":"Theory","title":"Examples of Modeling Frameworks","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"The following mathematical modeling frameworks are both widely used in science and engineering, and are examples of Monoidal Categories.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Petri Nets (Figure 1)\nBayesian Networks (Figure 2)\nMatrices and Linear Algebra\nDataflow programs used in Signal Processing\nControls Algorithms\nMachine Learning Workflows\nNeural Networks\nModels of Quantum Computation","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"(Image: Petri Nets have associated wiring diagrams)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"(Image: Bayesian Networks have associated wiring diagrams (Figure Credit: Jacobs, Kissinger, and Zanasi, 2019))","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"MCs provide you with a universal syntax called wiring diagrams. SemanticModels.jl uses a Julia package developed by E. Patterson at Stanford called Catlab to represent these wiring diagrams. Wiring diagrams provide a syntax to the modeling framework, in that one can express every model in the language of circ otimes and a list of primitives. Since we are in the world of computation, we are concerned primarily with finitely presented MCs which have a finite set of primitives.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"In order to compute with a modeling framework, it is essential to give the modeling framework a semantics, which means a functor from the set of wiring diagrams to some formulation that can be turned into a program. For example, Petri Nets are just multigraphs with attached metadata, that need to be converted into Differential Equations or Stochastic Discrete Event Simulators in order to model chemical or biological processes. Once you have this representation, you need to generate a program that solves those equations. We consider this the core of a modeling framework:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"you have an MC for describing models,\na functor that converts it to a structured knowledge representation, and\na code generation platform for creating executable code.","category":"page"},{"location":"theory/#Double-Pushout-Rewriting","page":"Theory","title":"Double Pushout Rewriting","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"The Category Theory community has been working on understanding rewriting systems in mathematics for several decades, but the most directly applicable work has been done in the last 5 years by John Baez group at UC Riverside. Daniel Cicala wrote a dissertation published in May 2019 on rewriting systems for Open Systems like Petri Nets.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Double pushout rewriting is a process of making analogies between systems for specifying transformations on those systems. Originally developed for transforming graphs according to rules, DPO rewriting has been extended to arbitrary topoi. Figure 3 shows an example of graph rewriting that says delete a loop and then applies that rule to another graph.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"(Image: Double pushout rewriting allows modelers to reason by analogy to change model structure)","category":"page"},{"location":"theory/#Model-Augmentation-in-Category-Theory","page":"Theory","title":"Model Augmentation in Category Theory","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"There is a mathematical structure called a lens that solves a hard problem in functional programming. Given a large and complex data structure, how do you manipulate a small part of it in a way consistent with the overall structure? In data processing you can think of running an SQL query to create a view, modifying rows in that view, and having the database state reflect those changes in the underlying tables. Lenses are a mathematical object for representing this situation in software engineering.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Model augmentation fits this mathematical structure perfectly. The complex data structure is the code found in a modeling program, and the view is the small fraction of that code that represents the scientific model. Augmenting a model inside a code corresponds to a lens where we extract the model, change it, and then update the code to be consistent with our changes. Figure 4 depicts the relationship between lenses and model augmentation within SemanticModels.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"(Image: Model Augmentation is the combination of lenses and double pushout rewriting)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"SemanticModels.jl currently implements model augmentation by combining lenses with MCs to extract a model from the code, apply double pushout rewriting, and then updating the code to reflect this new model. The lens laws require that transformation is composable with nice mathematical properties, thus making the system amenable to mathematical analysis. Over the coming months we will advance both the analysis of our system and its capabilities to operate on more diverse codes and models.","category":"page"},{"location":"theory/#Partially-Symbolic-Computing","page":"Theory","title":"Partially Symbolic Computing","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"We are using ModelingToolkit.jl which provides an embedded computer algebra system (CAS) to represent computations as data. This software has matured during the first phase of ASKE, and is now ready for routine use. Working with ModelingToolkit expressions is easier than working with Julia expression trees primarily because there are no conditional or iterative structures such as if statements or for loops. We believe that embedded CAS will change how scientific software is developed over the next 10 years.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"In general purpose computing, the task of generating fast code falls to compilers and is performed using general purpose techniques for code optimization. However, in scientific computing, this task is performed by highly skilled mathematicians, scientists, and engineers because the need for high performance code is so great and the complexity of the systems is so high. Scientific computing is characterized by specialists using their mathematical and scientific talent to write code faster than what a compiler can possibly produce because they know something special about the problem they are trying to solve. Examples such as the Fast Fourier Transform show what heroic effort on the part of scientific computing that specialists can produce.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"As these techniques go from highly specialized one-off tricks to general purpose techniques that can apply to many problems, they will be automated. This automation requires symbolic understanding of what the program is trying to compute. There are many opportunities for performance and accuracy improvements that can be exploited by computational techniques that have access to the mathematic representation of a problem for symbolic computing. Now that high performance dynamic code generation is a viable technology, the time is ripe for embedded CAS based modeling frameworks. These frameworks will take a data structure that represents the symbolic mathematics that the scientist wants to solve, and generate special purpose code for that problem on demand.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"(Image: The SEIRDS system can be represented as a system of ODEs (top), a Petri Net (left), or a wiring diagram (right). All of these representations are compositionally related.)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Our Petri Net framework (Figure 5) is based on partially symbolic computing and uses an embedded CAS to represent the structure of the Petri Net. When it is time to solve the model, we dynamically generate code for Differential Equations, or Stochastic Discrete Event Simulation based on the symbolic expressions. Model Augmentation operates on the symbolic formulation, which is much easier to work with than general purpose code, but compiles down to straight-line procedural code with no generic code overhead. Thus we get the benefit of a custom implementation of each Petri Net, without the labor intensive work of writing the code each time. A benefit of this approach is that no one needs to write a general simulator that can simulate any possible system. You only need to write a code generator that given a specific system, generates a simulator for that specific system. We believe that this is an easier task than writing a single simulator that is simple, concise, general, and efficient. This is because the majority of software complexity in scientific computing comes from pushing the Pareto frontier of textconcise times textgeneral times textfast. A code generator can completely abandon the requirements of concise and general code, and thus has liberty to write code that is simple and fast. The code generator must be concise and general, but does not need to be fast. By reducing the problem to writing a general code generator that generates specialized code, one can avoid high complexity scientific software.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The experience of writing a Petri Net modeling library illustrates the benefits of designing the modeling software around symbolic expressions, without implementing the entire modeling framework in a pure CAS like Mathematica. Model Augmentation requires the manipulation of the modeling code. If that code is explicitly represented as lists of symbolic expressions, these manipulations are easier than manipulating either abstract syntax trees (traditional metaprogramming) or run-time data structures that configure the model (typical in dynamic programming languages). An embedded CAS allows you to manipulate modeling expressions in the familiar language of mathematics and then generate the efficient code for implementing those formulas. Since code is more complex than formulas, it is beneficial to program at the level of formulas if possible.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We intend to continue pushing this partially symbolic computing agenda through additional frameworks such as Bayesian Networks and statistical analysis.","category":"page"},{"location":"theory/#Semantic-Representations-of-Data-Science-Programs","page":"Theory","title":"Semantic Representations of Data Science Programs","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"The work of Evan Patterson on building semantic representations of data science programs is particularly relevant to these modeling questions SRDSP. Patterson 2018","category":"page"},{"location":"theory/#Model-Augmentation","page":"Theory","title":"Model Augmentation","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"Scientists build novel models from old models and the approach provided by SemanticModels has several benefits. We think that","category":"page"},{"location":"theory/#Abstraction","page":"Theory","title":"Abstraction","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"Modeling operations have similarities across domains and we can build general model augmentations that let scientists translate operations from one domain to another. The code that defines transformations is also \"general modeling code\" so our abstraction is closed.","category":"page"},{"location":"theory/#Symbolification","page":"Theory","title":"Symbolification","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"The geometric perspective is really great for proving things about shapes, but developing algorithms requires adopting a symbolic perspectrive like algebra. Our example of polynomial regression connects here because we are able to write algorithms for model selection that leverage the symbolic nature of the transformations. In fact we can give examples of model selection in terms of ideals. The algebra of the transformation space is a place for algorithms on the model space.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Open question: Can we lift the greatest common divisor of polynomials to be the \"greatest common submodel\" for least squares regression? If so, does the euclidean algorithm for GCD give a model selection algorithm?","category":"page"},{"location":"theory/#Metaprogramming-for-Science","page":"Theory","title":"Metaprogramming for Science","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"Scientific models are so diverse that we need the full flexibility of code as input for our modeling framework. This is somewhat inherent to the scientific process. Scientists who are pushing the field in modeling are often inventing or applying new algorithms that are capable of solving those models. Also the first formulation of a new model is not the most elegant and so we need to be able to operate on ad-hoc models before we understand the class of models well enough for an elegant formulation to get added to the modeling framework.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Metaprogramming is about writing programs that write programs, so it makes sense that metamodeling is about writing models that write models. In order to write models that can generate models, there needs to be a compact and parsimonious representation of the model for algorithms to manipulate. As we have seen in writing our post-hoc modeling framework, scientific models diverse and hard to summarize, however the transformations that can be applied to a model while preserving its validity within the class of models is often much more structured than the models themselves. This is why we think that metamodels will work on these transformations instead of on the models directly.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Again we look to our polynomial regression problem, with only two transformations you can generate the entire class of polynomial regression problems from a model that computes linear regression. Algorithms that work on the polynomial regression models directly would have to manage a lot of complexity around arguments, data flow, conditional logic, I/O. But in the transformation state there is just f(x) -> xf(x) and f(x) -> f(x) + 1 which are simple transformations.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"By representing complex models as transformations of a base model, under an algebra of transformations, we are able to make metaprogramming for science much easier.","category":"page"},{"location":"theory/#Scientific-Workflows-and-Model-Synthesis","page":"Theory","title":"Scientific Workflows and Model Synthesis","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"One goal of the program is to get to the point where we can automatically infer how to combine models based on what they compute. The idea of model circuits based on signal flow graphs (see #137) is that you can statically connect models with a wiring diagram and then evaluate the diagram to compute the combined model. General DAGs are hard to compose and are typically written with either a declarative DAG language or an imperative DAG building library. The complexity of using DAG driven workflows reveals how the existing implementations lack compositionality, (how can you compose two makefiles?). By redesigning scientific workflow tools around categories of PROPS, we can achieve higher compositionality and thus more scalable scientific workflow tools.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Fong and Spivak 2018 shows how to express signal processing and controls problems in a graphical language based on categories of products and permutations category or props.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"In this category, computations can be specified with diagrams. The following figure shows a diagram representing a classical controls problem. These diagrams are shown to have the same functorial semantics as matrices. (Image: Diagram for computing PID control)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We think that the category theory approach of props is the right approach. This approach leads to diagrams with precise semantics for representing general purpose computation networks. These networks will be specified with code that combines the sum and product operation in a hierarchical expression just like regular code. Thus the code that makes the diagrams is a model that we can augment with our current Model Augmentation techniques.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"These \"model circuits\" can thus be built out of code resulting from transformations on code that builds a base circuit. Which establishes tools for creating high level transformations on circuits. We can then define the input and output wires as the modeling concepts we know and want to know and then build algorithms for solving for the circuit that gets from the inputs to the outputs. We suspect a dynamic programming approach to recursively bring the inputs and outputs closer together will solve this problem.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Algorithms that do model synthesis will need to account for the conceptual knowledge that is captured in the text and documentation of scientific software.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Once we have a mathematically sound way to represent combinations of models, we must address the practical aspects of model synthesis. We are endeavoring to augment and automate scientific workflows by meaningfully pruning the set of possible metamodels. When scientists design models, they must confront:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"known unknowns: parameters or components they are aware of, but about which there may be uncertainty regarding","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"values or best practices","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"unknown unknowns: parameters or components the scientist does not yet know about, or deems unnecessary for the","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"current modeling task","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"In a broader/more abstract sense, our potential contributions to \"AI for Science\" are related to uncertainty quantification, and we endeavor to help scientists assess and reduce both aleatoric and epistemic uncertainty. By working within a specific domain and comparing known existing models, we can help a scientist make progress on (1). By integrating across domains at semantically meaningful endpoints, we can help a scientist make progress on (2).","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Some tasks that can be automated require understanding both the existing scientific software ecosystem and what the scientist it trying to compute. For example, scientific productivity can be enhanced by answering questions like, \"If I know X and Y but want to know Z, what software tools can solve this for me, and how do I use them?\" This is a task that de novo modeling frameworks cannot answer, because the existing literature was not developed with the new framework.","category":"page"},{"location":"theory/#Up-Next","page":"Theory","title":"Up Next","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"The ideas of representing models as categories and functors that preserve different aspects of the structures we have is compelling. We believe this is the best theoretical foundation for the model augmentation and model synthesis components of SemanticModels.","category":"page"},{"location":"cookbook/#Epirecipes-Cookbook","page":"Epirecipes Cookbook","title":"Epirecipes Cookbook","text":"","category":"section"},{"location":"cookbook/","page":"Epirecipes Cookbook","title":"Epirecipes Cookbook","text":"The Epirecipes cookbook is a textbook written by various authors and maintained by Simon Frost (@sdwfrost) at the Allen Turing Institute for teaching the mathematical modeling techniques necessary for computational epidemiology.","category":"page"},{"location":"cookbook/","page":"Epirecipes Cookbook","title":"Epirecipes Cookbook","text":"We leverage this content as a corpus of instructional text written by domain experts to explain and illustrate epidemiology concepts. We learn from both the text, equations, and source code. The source code is stored in julia notebooks. We have updated these notebooks to use the Julia version 1.0 language and modern versions of the DifferentialEquations ecosystem. We have also manually annotated the corpus by adding human written documentation in the code that provides more detailed information for our information extraction tasks. This corpus is used primarily for development of our software and techniques.","category":"page"},{"location":"cookbook/","page":"Epirecipes Cookbook","title":"Epirecipes Cookbook","text":"The updated code can be found in our repo at examples/epicookbook/notebooks. Each folder refers to a chapter of the textbook and contains multiple notebooks from that chapter.","category":"page"},{"location":"cookbook/#Epicookbook-upgrade-and-annotation-methodology","page":"Epirecipes Cookbook","title":"Epicookbook upgrade and annotation methodology","text":"","category":"section"},{"location":"cookbook/","page":"Epirecipes Cookbook","title":"Epirecipes Cookbook","text":"The cookbook models were developed for Julia v0.6 and needed to be upgraded to run on Julia v1.0. As an intermediate step, they were run on Julia v0.7 to collect depreciation warnings and errors. Subsequently, these issues were fixed for compatibility with with Julia 1.0.3. The most common errors were:","category":"page"},{"location":"cookbook/","page":"Epirecipes Cookbook","title":"Epirecipes Cookbook","text":"zeros function \nzeros(tuple with size) => zeros(typecast(tuple))\nODE output new format \nFlat arrays => Array{Array{Float64,n} ,1}\nDataFrame vectors\nDataFrame => vec(DataFrame)\nRandom seed\nsrand(x) => Random.seed!(x)","category":"page"},{"location":"cookbook/","page":"Epirecipes Cookbook","title":"Epirecipes Cookbook","text":"Finally, test outputs for Julia v1.0.3 scripts are compared with the original to maintain consistency. ","category":"page"},{"location":"cookbook/","page":"Epirecipes Cookbook","title":"Epirecipes Cookbook","text":"Function definitions were made consistent by replacing variable names with Greek names to fully utilize Julia functionality. In addition, comments were added to describe variable names and function usage that were extracted from previous scripts and from the research papers that each script pertained to.","category":"page"},{"location":"approach/#Approaches","page":"Approaches","title":"Approaches","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"Architecture, Approaches, and Techniques","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"We provide a high-level overview of the open-source epidemiological modeling software packages that we have reviewed, and outlines our intended approach for extracting information from scientific papers that cite one or more of these packages. Our extraction efforts are directed toward the construction of a knowledge graph that we can traverse to reason about how to best map a set of known, unitful inputs to a set of unknown, unitful outputs via parameter modification, hyperparamter modification, and/or sequential chaining of models present in the knowledge graph. Our overarching purpose in this work is to reduce the cost of conducting incremental scientific research, and facilitate communication and knowledge integration across different research domains.","category":"page"},{"location":"approach/#Introduction","page":"Approaches","title":"Introduction","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"The ASKE program aims to extract knowledge from the body of scientific work. Our view is that the best way to prove that you have extracted knowledge is to show that you can build new models out of the components of old models. The purpose of these new models may be to improve the fidelity of the original model with respect to the phenomenon of interest or to probe the mechanistic relationships between phenomena. Another use case for adapting models to new contexts is in order to use a simulation to provide data that cannot be obtained through experimentation or observation.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"Our initial scientific modeling domain is the epidemiological study of disease spread, commonly called compartmental or SIR models. These models are compelling because the literature demonstrates the use of a repetitive model structure with many variations. The math represented therein spans both discrete and continuous equations, and the algorithms that solve these models are diverse. Additionally, this general model may apply to various national defense related phenomena, such as viruses on computer networks¬†[@cohenefficient2003] or misinformation in online media¬†[@budaklimiting2011].","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"The long term goal for our project is to reduce the labor cost of integrating models between scientists so that researchers can more efficiently build on the research of others. Such an effort is usefully informed by prior work and practices within the areas of software engineering and open source software development. Having the source code for a library or package is essential to building on it, but perhaps even more important are the affordances provided by open source licensing models and (social) software distribution systems that can significantly reduce the effort required to download others' code and streamline execution from hours to minutes. This low barrier to entry is responsible for the proliferation of open source software that we see today. By extracting knowledge from scientific software and representing that knowledge, including model semantics, in knowledge graphs, along with leveraging type systems to conduct program analysis, we aim to increase the interoperability and development of scientific models at large scale.","category":"page"},{"location":"approach/#Scientific-Domain-and-Relevant-Papers","page":"Approaches","title":"Scientific Domain and Relevant Papers","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"We have focused our initial knowledge artifact gathering efforts on the scientific domain of epidemiology broadly defined, so as to render the diffusion of both disease and information in scope. Given that our ultimate goal is to automate the extraction of calls to epidemiological modeling libraries and functions, as well as the unitful parameters contained therein, we have conducted a preliminary literature review for the purpose of: (1) identifying a subset of papers published in this domain that leverage open-source epidemiological modeling libraries, and/or agent-based simulation packages, and make their code available to other researchers; and (2) identifying causally dependent research questions that could benefit from, and/or be addressed by the modification and/or chaining of individual models, as these questions can serve as foundational test cases for the meta-models we develop.","category":"page"},{"location":"approach/#Papers-and-Libraries","page":"Approaches","title":"Papers and Libraries","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"We began the literature review and corpus construction process by identifying a representative set of open-source software (OSS) frameworks for epidemiological modeling, and/or agent-based simulation, including: NDLib, EMOD, Pathogen, NetLogo, EpiModels, and FRED. These frameworks were selected for initial consideration based on: (1) the scientific domains and/or research questions they are intended to support (specifically, disease transmission and information diffusion); (2) the programming language(s) in which they are implemented (Julia, Python, R, C++); and (3) the extent to which they have been used in peer-reviewed publications that include links to their source code. We provide a brief overview of the main components of each package below, as well as commentary on the frequency with which each package has been used in relevant published works.","category":"page"},{"location":"approach/#NDLib","page":"Approaches","title":"NDLib","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"NDLib is an open-source package developed by a research team from the Knowledge Discovery and Data Mining Laboratory (KDD-lab) at the University of Pisa, and written in Python on top of the NetworkX library. NDLib is intended to aid social scientists, computer scientists, and biologists in modeling/simulating the dynamics of diffusion processes in social, biological, and infrastructure networks [@NDlib1; @NetworkX]. NDLib includes built-in implementations of many common epidemiological models (e.g., SIR, SEIR, SEIS, etc.), as well as models of opinion dynamics (e.g., Voter, Q-Voter, Majority Rule, etc.). In addition, there are several features intended to make NDLib available to non-developer domain experts, including an abstract Network Diffusion Query Language (NDQL), an experiment server that is query-able through a RESTful API to allow for remote execution, and a web-based GUI that can be used to visualize and run epidemic simulations [@NDlib1].","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"The primary disadvantage of NDLib is that it is relatively new: the associated repository on GitHub was created in 2016, with the majority of commits beginning in 2017; two supporting software system architecture papers were published in 2017-2018 [@ndlibDocs; @NDlib1; @NDlib2]. As such, while there are several factors which bode well for future adoption (popularity of Python for data science workflows and computer science education; user-friendliness of the package, particularly for users already familiar with NetworkX, etc.), the majority of published works citing NDLib are papers written by the package authors themselves, and focus on information diffusion.","category":"page"},{"location":"approach/#Epimodels","page":"Approaches","title":"Epimodels","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"EpiModel is an R package, written by researchers at Emory University and The University of Washington, that provides tools for simulating and analyzing mathematical models of infectious disease dynamics. Supported epidemic model classes include deterministic compartmental models, stochastic individual contact models, and stochastic network models. Disease types include SI, SIR, and SIS epidemics with and without demography, with utilities available for expansion to construct and simulate epidemic models of arbitrary complexity. The network model class is based on the statistical framework of temporal exponential random graph models (ERGMs) implementated in the Statnet suite of software for R. [@JSSv084i08] The library is widely used and the source code is available. The library would make a great addition to the system we are building upon integration. EpiModels has received several grants from the National Institutes of Health (NIH) for funding its development. There are several publications utilizing the library at highly elite research journals, including PLoS ONE and Infectious Diseases, as well as the Journal of Statistical Software.","category":"page"},{"location":"approach/#NetLogo","page":"Approaches","title":"NetLogo","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"NetLogo, according to the User Manual, is a programmable modeling environment for simulating natural and social phenomena. It was authored by Uri Wilensky in 1999 and has been in continuous development ever since at the Center for Connected Learning and Computer-Based Modeling. NetLogo is particularly well suited for modeling complex systems developing over time. Modelers can give instructions to hundreds or thousands of \"agents\" all operating independently. This makes it possible to explore the connection between the micro-level behavior of individuals and the macro-level patterns that emerge from their interaction. NetLogo lets students open simulations and \"play\" with them, exploring their behavior under various conditions. It is also an authoring environment which enables students, teachers and curriculum developers to create their own models. NetLogo is simple enough for students and teachers, yet advanced enough to serve as a powerful tool for researchers in many fields. NetLogo has extensive documentation and tutorials. It also comes with the Models Library, a large collection of pre-written simulations that can be used and modified. These simulations address content areas in the natural and social sciences including biology and medicine, physics and chemistry, mathematics and computer science, and economics and social psychology. Several model-based inquiry curricula using NetLogo are available and more are under development. NetLogo is the next generation of the series of multi-agent modeling languages including StarLogo and StarLogoT. NetLogo runs on the Java Virtual Machine, so it works on all major platforms (Mac, Windows, Linux, et al). It is run as a desktop application. Command line operation is also supported. [@tisue2004netlogo; @nlweb] NetLogo has been widely used by the simulation research community at-large for well over nearly two decades. Although there is a rich literature that mentions its use, it may be more difficult to identify scripts that have been authored and that pair with published research papers using the modeling library due to the amount of time that has passed and that researcher may no longer monitor the email addresses listed on their publications for various reasons.","category":"page"},{"location":"approach/#EMOD","page":"Approaches","title":"EMOD","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"Epidemiological MODeling (EMOD) is an open-source agent-based modeling software package developed by the Institute for Disease Modeling (IDM), and written in C++ [@emodRepo; @emodDocs]. The primary use case that EMOD is intended to support is the stochastic agent-based modeling of disease transmission over space and time. EMOD has built-in support for modeling malaria, HIV, tuberculosis, sexually transmitted infections (STIs), and vector-borne diseases; in addition, a generic modeling class is provided, which can be inherited from and/or modified to support the modeling of diseases that are not explicitly supported [@emodDocs; @emodRepo].","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"The documentation provided is thorough, and the associated GitHub repo has commits starting in July 2015; the most recent commit was made in July 2018 [@emodRepo; @emodDocs]. EMOD also includes a regression test suite, so that stochastic simulation results can be compared to a reference set of results and assessed for statistical similarity within an acceptable range. In addition, EMOD leverages Message Passing Interface (MPI) to support within- and among-simulation(s)-level parallelization, and outputs results as JSON blobs. The IDM conducts research, and as such, there are a relatively large number of publications associated with the institute that leverage EMOD and make their data and code accessible. One potential drawback of EMOD relative to more generic agent-based modeling packages is that domain-wise, coverage is heavily slanted toward epidemiological models; built-in support for information diffusion models is not included.","category":"page"},{"location":"approach/#Pathogen","page":"Approaches","title":"Pathogen","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"Pathogen is an open-source epidemiological modeling package written in Julia [@pathogenRepo]. Pathogen is intended to allow researchers to model the spread of infectious disease using stochastic, individual-level simulations, and perform Bayesian inference with respect to transmission pathways [@pathogenRepo]. Pathogen includes built-in support for SEIR, SEI, SIR, and SI models, and also includes example Jupyter notebooks and methods to visualize simulation results (e.g., disease spread over a graph-based network, where vertices represent individual agents). With respect to the maturity of the package, the first commit to an alpha version of Pathogen occurred in 2015, and the master branch contains commits within the last month (e.g., November 2018) [@pathogenRepo]. Pathogen is appealing because it could be integrated into our Julia-based meta-modeling approach without incurring the overhead associated with wrapping non-Julia-based packages. However, one of the potential disadvantages of the Pathogen package is that there is no associated software or system architecture paper; as such, it is difficult to locate papers that use this package.","category":"page"},{"location":"approach/#FRED","page":"Approaches","title":"FRED","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"FRED, which stands for a Framework for Reconstructing Epidemic Dynamics, is an open-source, agent-based modeling software package written in C++, developed by the Pitt Public Health Dynamics Laboratory for the purpose of modeling the spread of disease(s) and assessing the impact of public health intervention(s) (e.g., vaccination programs, school closures, etc.) [@pittir24611; @fredRepo]. FRED is notable for its use of synthetic populations that are based on U.S. Census Data, and as such, allow for the instantiation of agents whose spatiotemporal and sociodemographic characteristics, including household membership and location, as well as income level and patterns of employment and/or school attendance, reflect the actual distribution of the population in the selected geographic area(s) within the United States [@pittir24611]. FRED is modular and paramterized to allow for support of different diseases, and the associated software paper, as well as the GitHub repository, provide clear, robust documentation for use. One advantage of FRED relative to some of the other packages we have reviewed is that it is relatively mature. Commits range from 2014-2016, and the associated software paper was published in 2013; as such, epidemiology researchers have had more time to become familiar with the software and cite it in their works [@pittir24611; @fredRepo]. A related potential disadvantage is that FRED does not appear to be under active development [@pittir24611; @fredRepo].","category":"page"},{"location":"approach/#Evaluation","page":"Approaches","title":"Evaluation","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"The packages outlined in the preceding section are all open-source, and written in Turing-complete programming languages; thus, we believe any subset of them would satisfy the open-source and complexity requirements for artifact selection outlined in the solicitation. As such, the primary dimensions along which we have evaluated and compared our initial set of packages include: (1) the frequency with which a given package has been cited in published papers that include links or references to their code; (2) the potential trend of increasing adoption/citation over the near-to-medium term; (3) the existence of thorough documentation; and (4) the feasibility of cross-platform and/or cross-domain integration.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"With respect to the selection of specific papers and associated knowledge artifacts, our intent at this point in the knowledge discovery process is to prioritize the packages outlined above based on their relative maturity, and proceed to conduct additional, augmenting bibliometric exploration in the months ahead. Our view is that EMOD, Epimodels, NetLogo, and FRED can be considered established packages, given their relative maturity and the relative availability of published papers citing these packages. Pathogen and NDLib can be considered newer packages, in that they are relatively new and lack citations, but have several positive features that bode well for an uptick in use and associated citation in the near- to medium-term. It is worth noting that while the established packages provide a larger corpus of work from which to select a set of knowledge artifacts, the newer packages are more modern, and as such, we expect them to be easier to integrate into the type of data science/meta-modeling pipelines we will develop. Additionally, we note that should the absence of published works prove to be an obstacle for a package we ultimately decide to support via integration into our framework, we are able to generate feasible examples by writing them ourselves.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"For purposes of development and testing, we will need to use simple or contrived models that are presented in a homogeneous framework. Pedagogical textbooks¬†[@voitfirst2012] and lecture notes[1] will be a resource for these simple models that are well characterized.","category":"page"},{"location":"approach/#Information-Extraction","page":"Approaches","title":"Information Extraction","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"In order to construct the knowledge graph that we will traverse to generate metamodel directed acyclic graphs (DAGs), we will begin by defining a set of knowledge artifacts and implementing (in both code and process/system design) an iterative, expert-in-the-loop knowledge extraction pipeline. The term \"knowledge artifacts\" is intended to refer to the set of open-source software packages (e.g., their code-bases), as well as a curated subset of published papers in the scientific domains of epidemiology and/or information diffusion that cite one or more of these packages and make their own code and/or data (if relevant) freely available. Our approach to the selection of packages and papers has been outlined in the preceding section, and is understood to be both iterative and flexible to the incorporation of additional criteria/constraints, and/or the inclusion/exclusion of (additional) works as the knowledge discovery process proceeds.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"Given a set of knowledge artifacts, we plan to proceed with information extraction as follows: First, we will leverage an expert system's based approach to derive rules to automatically recognize and extract relevant phenomena; see Table¬†[table:info_extract]{reference-type=\"ref\" reference=\"table:info_extract\"} for details. The rules will be built using the outputs of language parsers and applied to specific areas of source code that meet other heuristic criteria e.g. length, association with other other functions/methods. Next, we will also experiment with supervised approaches (mentioned in our proposal) and utilize information from static code analysis tools, programming language parsers, and lexical and orthographic features of the source code and documentation. For example, variables that are calculated as a result of running a for loop within code and whose characters, lexically speaking, occur within source code documentation and or associated research publications are likely related to specific models being proposed or extended in publications.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"We will also be performing natural language parsing [@manning] on research papers themselves to provide cues for how we perform information extraction on associated scripts with references to known libraries. For example, a research paper will reference a library that our system is able to reason about and extend models from and so if no known library is identified then the system will not attempt to engage in further pipeline steps. For example, a paper that leverages the EpiModels library will contain references to the EpiModels library itself and in one set of cases, reference a particular family of models e.g. \"Stochastic Individual Contact Models\". The paper will likely not mention any references to actual library functions/methods that were used but will reference particular circumstances related to using a particular model such as e.g. model parameters that were the focus of the research paper's investigation. These kinds of information will be used in supervised learning to build the case for different kinds of extractions. In order to do supervised learning, we will be developing ground truth annotations to train models with. To gain a better sense of the kinds of knowledge artifacts we will be working with, below we present an example paper that a metamodel can be built from and from whence information can be extracted to help in the creation of that metamodel.","category":"page"},{"location":"approach/#EpiModels-Example","page":"Approaches","title":"EpiModels Example","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"In [@doi:10.1111/oik.04527] the authors utilize the EpiModels library and provide scripts for running the experiments they describe. We believe this is an example of the kind of material we will be able to perform useful information extractions on to inform the development of metamodels. Figure [fig:img/covar_paper1]{reference-type=\"ref\" reference=\"fig:covar_paper1\"} is an example of script code from [@doi:10.1111/oik.04527]:","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"(Image: Example script excerpt associated with [@doi:10.1111/oik.04527] setting parameters for use in an ERGM model implemented by EpiModels library.){width=\"70%\"}","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"[[fig:covar_paper1]]{#fig:covarpaper1 label=\"fig:covarpaper1\"}","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"Table [table:info_extract]{reference-type=\"ref\" reference=\"table:info_extract\"} is a non-exhaustive list of the kinds of information extractions we are currently planning and the purposes they serve in supporting later steps:","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"Extraction Type    Description                                                                         Sources   ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì- ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì-   Code References    Creation and selection of metamodels to extend or utilize depending on user goals   Papers, Scripts   Model Parameters   Natural language variable names, function parameters                                Papers, Scripts   Function Names     Names of library model functions used to run experiments in papers                  Scripts   Library Names      Include statements to use specific libraries. Identification of libraries           Scripts","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":": Planned information extractions. A non-exhaustive list of   information extractions, their purposes, and sources.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"[[table:info_extract]]{#table:infoextract label=\"table:infoextract\"}","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"The information extractions we produce here will be included as annotations in the knowledge representations we describe next.","category":"page"},{"location":"approach/#Knowledge-Representation","page":"Approaches","title":"Knowledge Representation","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"On the topic of dimensionality / complexity reduction (in an entropic sense) and knowledge representation: (1) we will begin by passing the code associated with each knowledge artifact through static analysis tools. Static analysis tools include linters intended to help programmers debug their code and correct syntax, stylistic, and/or security-related errors. As the knowledge artifacts in our set are derived from already published works, we do not anticipate syntax errors. Rather, our objective is to use the abstract syntax trees (ASTs), call graphs, control flow graphs, and/or dependency graphs that are produced during static analysis to extract both discrete model instantiation(s) (along with arguments, which can be mapped back to parameters which may have associated metadata, including required object type and units), as well as sequential function call information.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"The former can be thought of as contributing a connected subgraph to the knowledge graph, such that G_i subseteq G, in which model classes and variable data/unit types are represented as vertices and connected by directed \"requires/accepts\" edges. The latter can be thought of as contributing information about the mathematical and/or domain-specific legality and/or frequency with which a given subset of model types can be sequentially linked; this information can be used to weight edges connecting model nodes in the knowledge graph.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"The knowledge graph approach will help identify relevant pieces of context. For example the domain of a scientific paper or code will be necessary for correct resolution of scientific terms which are used to refer to multiple phenomena in different contexts. For example, in a paper about biological cell signalling pathways the term \"death\" is likely to refer to the death of individual cells, while in a paper about disease prevalence in at-risk populations, the same term is likely referring to the death of individual people. This will be further complicated by figurative language in the expository aspects of paper where \"death\" might be used as a metaphor when a cultural behavior or meme \"dies out\" because people stop spreading the behavior to their social contacts.","category":"page"},{"location":"approach/#Schema-Design","page":"Approaches","title":"Schema Design","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"We will represent the information extracted from the artifacts using a knowledge graph. And while knowledge graphs are very flexible in how they represent data, it helps to have a schema describing the vertex and edge types along with the metadata that will be stored on the vertices and edges.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"In our initial approach, the number of libraries that models can be implemented with will be small enough that schema design can be done by hand. We expect that this schema will evolve as features are added to the system, but remain mostly stable as new libraries, models, papers, and artifacts are added.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"When a new paper/code comes in, we will extract edges and vertices automatically with code which represents those edges and vertices in the predefined schema.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"Many of the connections will be from artifacts to their components, which will connect to concepts. When papers are connected to other papers, they are connected indirectly (e.g., via other vertices), except for edges that represent citations directly between papers.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"(Image: An example of the knowledge graph illustrating the nature of the schema.[]{label=\"fig:schema.\"})","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"It is an open question for this research whether the knowledge graph should contain models with the parameters bound to values, or the general concept of a model with parameters available for instantiation. Our initial approach will be to model both the general concept of a model such as HookesLawModel along with the specific instantiation HookesLawModel{k=5.3} from specific artifacts.","category":"page"},{"location":"approach/#Data-Sets-in-the-Knowledge-Graph","page":"Approaches","title":"Data Sets in the Knowledge Graph","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"A big component of how papers refer to the same physical phenomenon is that they use the same data sets. These common datasets which become benchmarks that are leveraged widely in the research community are highly concentrated in a small number of widely cited papers. This is good for our task because we know that if two papers use the same dataset then they are talking about the same phenomenon.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"The most direct overlap of datasets is to go through papers that provide the canonical source for that dataset. But we can also think of similarity of datasets in terms of the schema(s) of the datasets. This requires a more detailed dataset representation than just the column names commonly found on CSV files. Google's open dataset search has done a lot of the work necessary for annotating the semantics for features of datasets. The DataDeps.jl system includes programmatic ways to access this information for many of the common open science data access protocols[2] By linking dataset feature (column) names to knowledge graph concepts, we will be able to compare datasets for similarity and conceptual overlap. The fact that two models are connected to the same dataset(s) or concept(s) is an important indicator that the two models are compatible or interchangeable.","category":"page"},{"location":"approach/#Schema.org","page":"Approaches","title":"Schema.org","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"Schema.org is one of the largest and most diverse knowledge graph systems.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"It includes virtually no coverage of scientific concepts. There are no schema.org nodes for Variable, Function, Equation. The most relevant schema.org concepts are given in the following list.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"https://schema.org/ScholarlyArticle\nhttps://schema.org/SoftwareSourceCode.\nhttps://schema.org/ComputerLanguage\nhttps://schema.org/variableMeasured\nhttps://meta.schema.org/Property\nhttps://schema.org/DataFeedItem\nhttps://schema.org/Quantity which has more specific types\nhttps://schema.org/Distance\nhttps://schema.org/Duration\nhttps://schema.org/Energy\nhttps://schema.org/Mass","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"The focus of schema.org is driven by its adoption in the web document community. Schema.org concepts are used for tagging documents in order for search engines or automated information extraction systems to find structured information in documents. Often it is catalogue or indexing sites that use schema.org concepts to describe the items or documents in their collections.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"The lack of coverage for scientific concepts is surprising given that we think of academic research on publication mining to be focused on their own fields, for example papers about mining bibliographic databases often use examples of database researchers themselves.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"You could model the relationships between papers using this schema.org schema. But that takes place at the bibliometric level instead of the the model semantics level. There are no entries for expressing that these two papers solve the same equation. Or model the same physical phenomena. Of course schema.org is organized so that everything can be expressed as a https://schema.org/Thing, but there is no explicit representation for these concepts. There is a Schema.org schema for heath and life science https://health-lifesci.schema.org/. As we define the schema of our knowledge graph, we will link up with the schema.org concepts as much as possible and could add an extension to the schema.org in order to represent scientific concepts.","category":"page"},{"location":"approach/#Model-Representation-and-Execution","page":"Approaches","title":"Model Representation and Execution","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"Representation of models occurs at four levels:","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"Executable: the level of machine or byte-code instructions\nLexical: the tradition code representation assignment,   functions, and loops\nSemantic: a declarative language or computation graph   representation with nodes linked to the knowledge graph\nHuman: a description in natural language as in a research paper   or textbook","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"The current method of scientific knowledge extraction is to take a Human level description and have a graduate student build a Lexical level description by reading papers and implementing new codes. We aim to introduce the Semantic level which is normally stored only in the brains of human scientists, but must be explicitly represented in machines in order to automate scientific knowledge extraction. A scientific model represented at the Semantic level will be easy to modify computationally and be describable for the automatic description generation component. The Semantic level representation of a model is a computation DAG. One possible description is to represent the DAG in a human-friendly way, such as in Figure¬†[fig:flu]{reference-type=\"ref\" reference=\"fig:flu\"}.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"(Image: An example pipeline and knowledge graph elements for a flu response model.[]{label=\"fig:flu\"})","category":"page"},{"location":"approach/#Scientific-Workflows-(Pipelines)","page":"Approaches","title":"Scientific Workflows (Pipelines)","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"Our approach will need to be differentiated from scientific workflow managers that are based on conditional evaluation tools like Make. Some examples include Make for scientists, Scipipe, and the Galaxy project. These scientific workflows focus on representing the relationships between intermediate data products without getting into the model semantics. While scientific workflow managers are a useful tool for organizing the work of a scientist, they do not have a particularly detailed representation of the modeling tasks. Workflow tools generally accept the UNIX wisdom that text is the universal interface and communicate between programs using files on disk or in memory pipes, sockets, or channels that contain lines of text.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"Our approach will track a higher fidelity representation of the model semantics in order to enable computational reasoning over the viability of combined models. Ideas from static analysis of computer programs will enable better verification of metamodels before we run them.","category":"page"},{"location":"approach/#Metamodels-as-Computation-Graphs","page":"Approaches","title":"Metamodels as Computation Graphs","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"Our position is that if you have a task currently solved with a general purpose programming language, you cannot replace that solution with anything less powerful than a general purpose programming language. The set of scientific modeling codes is just too diverse, with each part a custom solution, to be solved with a limited scope solution like a declarative model specification. Thus we embed our solution into the general purpose programming language Julia.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"We use high level programming techniques such as abstract types and multiple dispatch in order to create a hierarchical structure to represent a model composed of sub-models. These hierarchies can lead to static or dynamic DAGs of models. Every system that relies on building an execution graph and then executing it finds the need for dynamically generated DAGs at some point. For sufficiently complicated systems, the designer does not know the set of nodes and dependencies until execution has started. Examples include recursive usage of the make build tool, which lead to techniques such as cmake, Luigi, and Airflow, and Deep Learning which has both static and dynamic computation graph implementations for example TensorFlow and PyTorch. There is a tradeoff between the static analysis that helps optimize and validate static representations and the ease of use of dynamic representations. We will explore this tradeoff as we implement the system.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"For a thorough example how to use our library to build a metamodel see the notebook FluExample.ipynb. This example uses Julia types system to build a model DAG that represents all of the component models in a machine readable form. This DAG is represented in Figure¬†[fig:flu]{reference-type=\"ref\" reference=\"fig:flu\"}. Code snippets and rendered plots appear in the notebook.","category":"page"},{"location":"approach/#Metamodel-Constraints","page":"Approaches","title":"Metamodel Constraints","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"When assembling a metamodel, it is important to eliminate possible combinations of models that are scientifically or logically invalid. One type of constraint is provided by units and dimensional analysis. Our flu example pipeline uses Unitful.jl to represent the quantities in the models including Csdperson for Celsius, second, day, and person. While Csd are SI defined units that come with Unitful.jl, person is a user defined type that was created for this model. These unit constraints enable a dynamic analysis tool (the Julia runtime system) to invalidate combinations of models that fail to use unitful numbers correctly, i.e., in accordance with the rules of dimensional analysis taught in high school chemistry and physics. In order to make rigorous combinations of models, more information will need to be captured about the component models. It is necessary but not sufficient for a metamodel to be dimensionally consistent. We will investigate the additional constraints necessary to check metamodels for correctness.","category":"page"},{"location":"approach/#Metamodel-Transformations","page":"Approaches","title":"Metamodel Transformations","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"Metamodel transformations describe high-level operations the system will perform based on the user's request and the information available to it in conjunction with using a particular set of open source libraries; examples of these are as follows:","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"utilize an existing metamodel and modifying parameters;\nmodifying the functional form in a model such as adding terms to an  equation\nchanging the structure of the metamodel by modifying the structure  of the computation graph\nintroducing new nodes to the model[3]","category":"page"},{"location":"approach/#Types","page":"Approaches","title":"Types","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"This project leverages the Julia type system and code generation toolchain extensively.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"Many Julia libraries define and abstract interface for representing the problems they can solve for example","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"DifferentialEquations.jl   https://github.com/JuliaDiffEq/DiffEqBase.jl defines   DiscreteProblem, ODEProblem, SDEProblem, DAEProblem which   represent different kinds of differential equation models that can   be used to represent physical phenomena. Higher level concepts such   as a MonteCarloProblem can be composed of subproblems in order to   represent more complex computations. For example a   MonteCarloProblem can be used to represent situations where the   parameters or initial conditions of an ODEProblem are random   variables, and a scientist aims to interrogate the distribution of   the solution to the ODE over that distribution of input.\nMathematicalSystems.jl   https://juliareach.github.io/MathematicalSystems.jl/latest/lib/types.html   defines an interface for dynamical systems and controls such as   LinearControlContinuousSystem and   ConstrainedPolynomialContinuousSystem which can be used to   represent Dynamical Systems including hybrid systems which combine   discrete and continuous phenomena. Hybrid systems are of particular   interest to scientists examining complex phenomena at the interface   of human designed systems and natural phenomena.\nAnother library for dynamical systems includes   https://juliadynamics.github.io/DynamicalSystems.jl/, which takes   a timeseries and physics approach to dynamical systems as compared   to the engineering and controls approach taken in   MathematicalSystems.jl.\nMADs http://madsjulia.github.io/Mads.jl/ offers a modeling   framework that supports many of the model analysis and decision   support tasks that will need to be performed on metamodels that we   create.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"Each of these libraries will need to be integrated into the system by understanding the types that are used to represent problems and developing constraints for how to create hierarchies of problems that fit together. We think that the number of libraries that the system understands will be small enough that the developers can do a small amount of work per library to integrate it into the system, but that the number of papers will be too large for manual tasks per paper.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"When a new paper or code snippet is ingested by the system, we may need to generate new leaf types for that paper automatically.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"By hooking into the Julia type system we are able to use multidispatch to reprogram existing functions. Our approach takes this to the next level by using Cassette.jl contexts and overdubbing to reprogram code to provide new functionality without changing the architecture of the existing software.","category":"page"},{"location":"approach/#User-Interface","page":"Approaches","title":"User Interface","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"Our system is used by expert scientists who want to reduce their time spent writing code and plumbing models together. As an input it would take a set of things known or measured by the scientist and a set of variables or quantities of interest that are unknown. The output of the program is a program that calculates the unknowns as a function of the known input(s) provided by the user, potentially with holes that require expert knowledge to fill in.","category":"page"},{"location":"approach/#Generating-new-models","page":"Approaches","title":"Generating new models","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"We will use metaprogramming to build a library that takes data structures, derived partially using information previously extracted from research publication and associated scripts, which represent models as input and transform and combine them into new models, then generates executable code based on the these new, potentially larger models.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"One foreseeable technical risk is that the Julia compiler and type inference mechanism could be overwhelmed by the number of methods and types that our system defines. In a fully static language like C++ the number of types defined in a program is fixed at compile time and the compile burden is paid once for many executions of the program. In a fully dynamic language like Python, there is no compilation time and the cost of type checking is paid at run time. However, in Julia, there is both compile time analysis and run time type computations.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"In Julia, changing the argument types to a function causes a round of LLVM compilation for the new method of that function. When using Unitful numbers in calculations, changes to the units of the numbers create new types and thus additional compile time overhead. This overhead is necessary to provide unitful numbers that are no slower for calculations than primitive number types provided by the processor. As we push more information into the type system, this trade-off of additional compiler overhead will need to be managed.","category":"page"},{"location":"approach/#Validation","page":"Approaches","title":"Validation","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"There are many steps to this process and at each step there is a different process for validating the system.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"Extraction of knowledge elements from artifacts: we will need to   assess the accuracy of knowledge elements extracted from text, code   and documentation to ensure that the knowledge graph is correct.   This will require some manual annotation of data from artifacts and   quality measures such as precision and recall. The precision is the   number of edges in the knowledge graph that are correct, and the   recall is the fraction of correct edges that were recovered by the   information extraction approach.\nMetamodel construction: once we have a knowledge graph, we will   need to ensure that combinations of metamodels are valid, and   optimal. We will aim to produce the simpliest metamodel that relates   the queried concepts this will be measured in terms of number of   metamodel nodes, number of metamodel dependency onnections, number   of adjustment or transformation functions. We will design test cases   that increase in complexity from pipelines with no need to transform   variables, to pipelines with variable transformations, to directed   acyclic graphs (DAGs).\nModel Accuracy: as the metamodels are combinations of models that   are imperfect, there will be compounding error within the metamodel.   We will need to validate that our metamodel execution engine does   not add error unnecessarily. This will involve numerical accuracy   related to finite precision arithmetic, as well as statistical   accuracy related to the ability to learn parameters from data.   Additionally, since we are by necessity doing some amount of domain   adaptation when reusing models, we will need to quantify the domain   adaptation error generated by applying a model developed for one   context in a different context. These components of errors can be   thought of as compounding loss in a signal processing system where   each component of the design introduces loss with a different   response to the input.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"Our view is to analogize the metamodel construction error and the model accuracy to the error and residual in numerical solvers. For a given root finding problem, such as f(x)=0 solve for x the most common way to measure the quality of the solution is to measure both the error and the residual. The error is defined as mid x-x^starmid, which is the difference from the correct solution in the domain of x and the residual is mid f(x) - f(x^star)mid or the difference from the correct solution in the codomain. We will frame our validation in terms of error and residual, where the error is how close did we get to the best metamodel, and residual is the difference between the observed versus predicted phenomena.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"These techniques need to generate simple, explainable models for physical phenomena that are easy for scientists to generate, probe, and understand, while being the best possible model of the phenomena under investigation.","category":"page"},{"location":"approach/#Next-Steps","page":"Approaches","title":"Next Steps","text":"","category":"section"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"Our intended path forward following the review of this report is as follows:","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"Incorporation of feedback received from DARPA PM, including  information related to: the types of papers we consider to be in  scope (e.g., those with and without source code); domain coverage  and desired extensibility; expressed preference for  inclusion/exclusion of particular package(s) and/or knowledge  artifact(s).\nConstruction of a proof-of-concept version of our knowledge graph  and end-to-end pipeline, in which we begin with a motivating example  and supporting documents (e.g., natural language descriptions of the  research questions and mathematical relationships modeled; source  code), show how these documents can be used to construct a knowledge  graph, and show how traversal of this knowledge graph can  approximately reproduce a hand-written Julia meta-modeling pipeline.  The flu example outlined above is our intended motivating example,  although we are open to tailoring this motivating example to  domain(s) and/or research questions that are of interest to DARPA.\nA feature of the system not yet explored is automatic transformation  of models at the Semantic Level. These transformations will be  developed in accordance with interface expectations from downstream  consumers including the TA2 performers.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"Executing on this proof-of-concept deliverable will allow us to experience the iterative development and research life-cycle that end-users of our system will ultimately participate in. We anticipate that this process will help us to identify gaps in our knowledge and framing of the problem at hand, and/or shortcomings in our methodological approach that we can enhance through the inclusion of curated domain-expert knowledge (e.g., to supplement the lexical nodes and edges we are able to extract from source code). In addition, we expect the differences between our hand-produced meta-model and our system-produced meta-model to be informative and interpretable as feedback which can help us to improve the system architecture and associated user experience. It's also worth noting that over the medium term, we anticipate that holes in the knowledge graph (e.g., missing vertices and/or edges; missing conversion steps to go from one unit of analysis to another, etc.) may help us to highlight areas where either additional research, and/or expert human input is needed.","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"[1]: http://alun.math.ncsu.edu/wp-content/uploads/sites/2/2017/01/epidemic_notes.pdf","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"[2]: http://white.ucc.asn.au/DataDeps.jl/latest/z20-for-pkg-devs.html#Registering-a-DataDep-1","category":"page"},{"location":"approach/","page":"Approaches","title":"Approaches","text":"[3]: new model nodes must first be ingested into the system in order to be made available to users.","category":"page"},{"location":"malaria/#Malaria-Example","page":"Malaria","title":"Malaria Example","text":"","category":"section"},{"location":"malaria/","page":"Malaria","title":"Malaria","text":"The following example illustrates how you can ingest two simple scientific models and perform a metamodeling or model modification task on the using the SemanticModels system to end up with a much more complex system.","category":"page"},{"location":"malaria/","page":"Malaria","title":"Malaria","text":"Our two models are a Lotka Volterra model to simulate predation and an SIR model to simulate disease spread. We want to compose these two models together, to produce a new model how predation between birds and mosquitos in a system and the spread of Malaria between mosquitos and susceptible humans affect one another.","category":"page"},{"location":"malaria/","page":"Malaria","title":"Malaria","text":"The full example notebook can be found at malaria.jl","category":"page"},{"location":"workflow/#Workflow","page":"Workflow","title":"Workflow","text":"","category":"section"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"SemanticModels provides the functionality for model augmentation. With SemanticModels, you treat models as symmetric monoidal categories and metaprogram on them using morphisms in their category with the CategoryTheory module.","category":"page"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"Introduce a new class of models to analyze by writing a struct to represent models from class mathcalC along with a constructor model(::C,...) to build the struct.\nDefine a morphism from FinSet to mathcalC as fuction (f::FinSetMorph)(g::G) where G <: C\nDefine the disjoin union between two models of class mathcalC as ‚äî(g::C, h::C)\nDefine models of class mathcalC as a decoration on a finite set, and use pushout and double pushouts to transform the model","category":"page"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"To extend this new class of models to support composition with open systems:","category":"page"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"Introduce a new class of OpenModel along with a constructor that extends OpenModel{V, C}\nDefine otimes and compose for the new OpenModel\nConvert models of class mathcalC to OpenModel{V, C} with a defined domain and codomain, and do model composition","category":"page"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"Under this workflow SemanticModels is more of a framework than a library, but it is extensible and can be used to take real world modeling methods and build a modeling framework around it, rather than building a modeling framework and then porting the models into the framework.","category":"page"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"See the examples folder for usage of how to build model types and use transformations for common metamodeling tasks. A complete API can be found at Library Reference.","category":"page"},{"location":"workflow/#Examples","page":"Workflow","title":"Examples","text":"","category":"section"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"The following examples are found in the folder SemanticModels/examples as julia files that can be viewed as notebooks with jupytext or as rendered HTML pages in the docs.","category":"page"},{"location":"workflow/#Model-Augmentation","page":"Workflow","title":"Model Augmentation","text":"","category":"section"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"These examples illustrate model augmentation with SemanticModels","category":"page"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"rewrite_demo.jl\ngraphs.jl","category":"page"},{"location":"workflow/#Algebraic-Model-Transformation","page":"Workflow","title":"Algebraic Model Transformation","text":"","category":"section"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"These examples illustrate how model transformations can be algebraic structures and how to exploit that to develop new models","category":"page"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"monomial_regression.jl\nmultivariate_regression.jl\npseudopolynomialregression.jl\npolynomial_regression.jl","category":"page"},{"location":"workflow/#Model-Synthesis","page":"Workflow","title":"Model Synthesis","text":"","category":"section"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"The workflow example combines agentgraft.jl and polynomial_regression.jl to build a modeling pipeline. This is the most important example for understanding the power of SemanticModels for model augmentation and synthesis.","category":"page"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"workflow.jl","category":"page"},{"location":"workflow/#Pre-hoc-vs-post-hoc-frameworks","page":"Workflow","title":"Pre hoc vs post hoc frameworks","text":"","category":"section"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"A normal modeling framework, is a software package that defines a set of modeling constructs for representing problems and a set of algorithms that solve those problem.","category":"page"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"A typical modeling framework is developed when:","category":"page"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"A library author (LA) decides to write a library for solving models of a specific class mathcalC\nLA develops a DSL for representing models in mathcalC\nLA develops solvers for models in mathcalC\nScientist (S) uses LA's macros to write new models and pass them to the solvers\nS publishes many great papers with the awesome framework","category":"page"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"ModelingToolkit.jl is a framework for building DSLs for expressing mathematical models of scientific phenomena. And so you could think of it as a meta-DSL a language for describing languages that describe models. Their workflow is:","category":"page"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"A library author (LA) decides to write a library for solving models of a specific class mathcalC\nLA develops a DSL for representing models in mathcalC using ModelingToolkit (MT).\nLA develops solvers for models in mathcalC using the intermediate representations provided by MT.\nScientist (S) uses LA's macros to write new models and pass them to the solvers\nS publishes many great papers with the awesome framework","category":"page"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"This is a great idea and I hope it succeeds because it will revolutionize how people develop scientific software and really benefit many communities.","category":"page"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"One of the assumptions of the SemanticModels is that we can't make scientists use a modeling language. This is reasonable because the really interesting models are pushing the boundaries of the solvers and the libraries, so if you have to change the modeling language every time you add a novel model, what is the modeling language getting you?","category":"page"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"Another key idea inspiring SemanticModels is that every software library introduces a miniature DSL for using that library. You have to set up the problem in some way, pass the parameters and options to to the solver, and then interpret the solution. These miniDSLs form through idiomatic usage instead of through an explicit representation like ModelingToolkit provides.","category":"page"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"SemanticModels actually can address this as the inverse problem of ModelingToolkit. We are saying, given a corpus of usage for a given library, what is the implicit DSL that users have developed?","category":"page"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"Our workflow is:","category":"page"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"Identify a widely used library\nExtend SemanticModels by implementing the couple necessary category theory functions in terms of the library\nBuild a DSL for that class of problems\nNew researchers and AI scientists can use the new DSL for representing the novel models\nGenerate new models in the DSL using transformations and augmentations that are valid in the DSL.","category":"page"},{"location":"workflow/","page":"Workflow","title":"Workflow","text":"In this line of inquiry the DSL plays the role of the \"structured semantic representation\" of the model. We could use ModelingToolkit DSLs as the backend.","category":"page"},{"location":"validation/#Model-Validation-with-Dynamic-Analysis","page":"Validation","title":"Model Validation with Dynamic Analysis","text":"","category":"section"},{"location":"validation/#Looking-Forward","page":"Validation","title":"Looking Forward","text":"","category":"section"},{"location":"validation/","page":"Validation","title":"Validation","text":"We pursued a novel approach to model validation based on LSTM models of program traces, this was supposed to use the Cassette based traces to build probabilistic models of program failure. This did not work because of the long term dependencies between information in the trace and the failure condition that would happen much later. While we see no impediment to successful modeling of program failure based on traces, we are attempting a prerequisite task of building an autoencoder of Julia code snippets that will enable learning latent space embeddings of programs. These latent space embeddings will assist with future model augmentation, synthesis, and validation tasks because we will be able to solve clustering, classification, and nearest neighbor search in this latent space that captures the meaning of julia program snippets.","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"We also believe that the existing work on model augmentation in particular the metaprogramming on models necessary to implement the typegraph functionality will enable faster development of novel validation techniques.","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"Validation of scientific models is a type of program verification, but is complicated by the fact that there are no global explicit rules about what defines a valid scientific models. In a local sense many disciplines of science have developed rules for valid computations. For example unit checking and dimensional analysis and conservation of physical laws. Dimensional analysis provides rules for arithmetic of unitful numbers. The rules of dimensional analysis are \"you can add numbers if the units match, when you multiply/divide, the powers of the units add/subtract.\" Many physical computations obey conservation rules that provide a form of program verification. Based on a law of physics such as \"The total mass of the system is constant,\" one can build a program checker that instruments a program with the ability to audit the fact that sum(mass, system[t]) == sum(mass, system[t0]), these kinds of checks may be expressed in codes.","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"We can use Cassette.jl to implement a context for validating these computations. The main difficulty is converting the human language expressed rule into a mathematical test for correctness. A data driven method is outlined below.","category":"page"},{"location":"validation/#DeepValidate","page":"Validation","title":"DeepValidate","text":"","category":"section"},{"location":"validation/","page":"Validation","title":"Validation","text":"There are an intractable number of permutations of valid deep learning model architectures, each providing different levels of performance (both in terms of efficiency and accuracy of output) on different datasets. One current predicament in the field is an inability to rigorously define an optimal architecture starting from the types of inputs and outputs; preferred solutions are instead chosen based on empirical processes of trial and error. In light of this, it has become common to start deep learning model efforts from architectures established by previous research, especially ones which have been adopted by a significant portion of the deep learning community, for similar tasks, and then tweak and modify hyper parameters as necessary. We adopt this typical approach, beginning with a standard architecture and leaving open the possibility of optimizing the architecture as training progresses. ","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"Given that our deep learning task in this instance is relatively straightforward and supportive of the overall thrust of this project rather than central to it, we adopt a common and well tested <a href='http://www.bioinf.jku.at/publications/older/2604.pdf'>long short-term memory</a> (LSTM) recurrent neural network (RNN) architecture for our variable-length sequence classification task. LSTM models have a rich history of success in complex natural language processing tasks, specifically where <a href='https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8'>comprehension</a> and classification of computer programming code is concernd, and they remain the most popular and <a href='https://www.microsoft.com/en-us/research/wp-content/uploads/2016/04/Intent.pdf'>effective</a> approach to these tasks. Our base model will use binary cross entropy as its cost function given our task is one of binary classification, and an Adam optimizer for training optimization. Introduced in 2014, the <a href='https://arxiv.org/abs/1412.6980'>Adam</a> optimization algorithm generally remains the most robust and efficient back propagation optimization method in deep learning. Input traces are first tokenized as indices representing specific functions, variables, and types in vocabularies compiled from our modeling corpus, while values are passed as is. These sequences are fed in to a LSTM layer which reads each token/value in sequence and calculates activations on them, while also passing values ahead in the chain subject to functions which either pass, strengthen or ‚Äúforget‚Äù the memory value. ","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"As mentioned, this LSTM RNN model is written using Julia‚Äôs Flux.jl package, with a similar architecture to the standard language classification model:","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"scanner = Chain(Dense(length(alphabet), seq_len, œÉ), LSTM(seq_len, seq_len))\nencoder = Dense(seq_len, 2)\n\nfunction model(x)\n  state = scanner.([x])[end]\n  Flux.reset!(scanner)\n  softmax(encoder(state))\nend\n\nloss(tup) = crossentropy(mod(tup[1]), tup[2])\naccuracy(tup) = mean(argmax(m(tup[1])) .== argmax(tup[2]))\n\ntestacc() = mean(accuracy(t) for t in test)\ntestloss() = mean(loss(t) for t in test)\n\nopt = ADAM(0.01)\nps = params(mod)\nevalcb = () -> @show testloss(), testacc()\n\nFlux.train!(loss, ps, train, opt, cb = throttle(evalcb, 10))","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"In the above example, outputs of the LSTM layer are subject to 50% dropout as a regularization measure to avoid over-fitting, and then fed to a densely connected neural network layer for computation of non-linear feature functions. Outputs of this dense layer are normalized for each batch of training data as another regularization measure to constrict extreme weights. This sequence of dropout-dense-normalization layers is repeated once more to add depth to the non-linear features learned by the model. Finally, a softmax activation function is calculated on the outputs and a binary classification is completed on each case in our dataset. ","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"To train this DeepValidate model, we execute the following steps:","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"Collect a sample of known ‚Äúgood‚Äù inputs matched with their corresponding ‚Äúgood‚Äù outputs, and a sample of known ‚Äúbad‚Äù inputs matched with their corresponding ‚Äúbad‚Äù outputs.\n‚ÄúGood‚Äù here is defined as: given these input(s), the model output(s)/prediction(s) correspond to expected or observed","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"empirical reality, within an acceptable error tolerance.     + Edge cases to note but not heavily consider at this point:         1. For ‚Äúgood‚Äù input to ‚Äúbad‚Äù output, we can just corrupt the ‚Äúgood‚Äù inputs at various points along the computation.         1. If assumption that code is correct and does not contain bugs holds, then it is ok to assume we will not observe ‚Äúbad‚Äù input to ‚Äúgood‚Äù output. ","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"Run the simulation to collect a sample of known good outputs.\nInstrument the code to log all SSA assignments from the function calls\nTrain an RNN on the sequence of [(func, var, val)...] where the labels are ‚Äúgood input vs bad input‚Äù\nBy definition, any SSA ‚Äúsentence‚Äù generated by a known ‚Äúgood‚Äù input is assumed to be ‚Äúgood‚Äù; thus, these labels essentially propagate down. \nPartial evaluations of the RNN model and output can point to ‚Äúwhere things went wrong.\" Specifically, layer-wise relevance propagation can be employed to identify the most powerful factors in input sequences, as well as their valence (good input, bad input) for case by case error analysis in deep learning models. This approach was effectively extended to LSTM RNN models by Arras et al. (http://www.aclweb.org/anthology/W17-5221) in 2017. ","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"In step 1: for an analytically tractable model, we can generate an arbitrarily large collection of known good and bad inputs.","category":"page"},{"location":"validation/#Required-Data-Format","page":"Validation","title":"Required Data Format","text":"","category":"section"},{"location":"validation/","page":"Validation","title":"Validation","text":"We need to build a Tensorflow.jl or Flux.jl RNN model that will work on sequences [(func, var, val, type)] and produce labels of good/bad","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"Traces will be communicated 1 trace per file\nEach line is a tuple module.func, var,val,type with quotes as necessary for CSV storage. \nThe files will be organized into folders program/{good,bad}/tracenumber.txt\nTraces will be variable length.","category":"page"},{"location":"validation/#Datasets-for-empirical-validation","page":"Validation","title":"Datasets for empirical validation","text":"","category":"section"},{"location":"validation/","page":"Validation","title":"Validation","text":"Observed empirical reality is represented in selected real world epidemiological datasets covering multiple collection efforts in different environments. These datasets have been identified as promising candidates for demonstrating how modeling choices can affect the quality of models and how ranges of variables can change when one moves between environments or contexts with the same types of data. ","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"To ensure a broad selection of types of epidemiological model options during examination of this data, we will combine key disease case data with various environmental data sources covering weather, demography, healthcare infrastructure, and travel patterns wherever possible. These datasets will be of varying levels of geographic and temporal granularity, but always at least monthly in order to model seasonal variations in infected populations. ","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"The validation datasets cover three main disease topics: 1.\tInfluenza cases in the United States: The Centers for Disease Control and Prevention (CDC) maintains publicly available data containing weekly influenza activity levels per state (https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html). This weekly data is provided for all states from the 2010-2011 to 2018-2019 flu seasons, comprising over 23,000 rows with columns indicating percentage of influenza-like-illnesses, raw case count, number of providers and total numbers of patients for each state in each week of each year. A sample of the data is presented below for reference This data will be supplemented by monthly influenza vaccine reports provided by the CDC (https://www.cdc.gov/flu/fluvaxview/coverage-1718estimates.htm) for different age ranges (6 months ‚Äì 4 years of age, 5-12 years of age, 13-17 years of age, 18-49 years of age, and 50 ‚Äì 64 years of age). In addition, data is split by different demographic groups (Caucasian, African American and Hispanic). This data is downloaded directly into .csv dataset from the above cited webpage. For application to our weekly datasets, weekly values can be interpolated based on the monthly aggregates. ","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"<center>","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"REGION YEAR WEEK %UNWEIGHTED ILI ILITOTAL NUM. OF PROVIDERS TOTAL PATIENTS\nAlabama 2010 40 2.13477 249 35 11664\nAlaska 2010 40 0.875146 15 7 1714\nArizona 2010 40 0.674721 172 49 25492","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"</center>","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"2.\tZika virus cases in the Americas: This data catalogues 108,000 reported cases of Zika, along with their report date and country/city (for geo-spatial location). This dataset is provided by the publicly available Zika Data Repository (https://github.com/cdcepi/zika) hosted on Github. One dozen countries throughout the Americas are included, as well as two separate Caribbean U.S. territories (Puerto Rico and U.S. Virgin Islands). A sample of the data is presented below for reference:","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"<center>","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"report_date location data_field value\n6/2/18 Mexico-Guanajuato weeklyzikaconfirmed 3\n6/2/18 Mexico-Guerrero weeklyzikaconfirmed 0\n6/2/18 Mexico-Hidalgo weeklyzikaconfirmed 5\n6/2/18 Mexico-Jalisco weeklyzikaconfirmed 21","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"</center>","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"3.\tDengue fever cases in select countries around the world: The Pan-American Health Organization reports weekly dengue fever case levels in 53 countries throughout the Americas and at sub-national geographic units in Brazil, covering years 2014-2019. This data is available at http://www.paho.org/data/index.php/en/mnu-topics/indicadores-dengue-en/dengue-nacional-en/252-dengue-pais-ano-en.html and a sample of the data is presented below for reference:","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"<center>","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"Geo Type Year Confirmed Deaths Week Incidence Rate Population X 1000 Severe Dengue Total Cases\nCura√ßao  2018 0 0 52 0 162 0 0\nHonduras DEN 1,2,3 2018 Null 3 52 84.34 9,417 1,172 7,942\nArgentina DEN 1 2018 1,175 0 52 4.09 44,689 0 1,829\nAruba DEN 2018 75 Null 52 70.75 106 Null 75\nMexico DEN 1,2,3,4 2018 12,706 45 52 60.13 130,759 858 78,621","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"</center>","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"Our supplementary environmental datasets will be variably sourced depending on the target geographies, but will include: 1.\tWeather: Historical weather data aggregated to the target geography and unit of time. This data is pulled directly from the Global Historical Climate Network (GHCN), an integrated database of climate summaries from land surface stations across the globe that have been subjected to a common suite of quality assurance reviews, and updated hourly. This database is maintained by the National Oceanic and Atmospheric Agency (NOAA) at https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-ghcn. A variety of weather and climate indicators are available, but for completeness of coverage and relevance, we will target high/low/mean temperatures and total precipitation data for each geography and time period.  2.\tDemography: Demographic information such as total population, population density, population share by age, gender, education level, by target geography. For the United States, American Community Survey data is conveniently from the IPUMS repositories (https://usa.ipums.org/usa/) and also includes highly relevant additional variables such as health insurance coverage. Basic demographic data is available for international geographies as well through national statistics office websites (such as the Department of Statistics for Singapore at https://www.singstat.gov.sg/) or international governmental organizations (such as the World Bank for Bangladesh at http://microdata.worldbank.org/index.php/catalog/2562/sampling). These may be less current and frequently updated, especially in less developed countries, but should still serve as reasonable approximations for our purposes. Some variables such as health coverage or access to healthcare may be more sparsely available internationally.","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"Similarly, for the United States influenza data, we will include reports and estimates of flu vaccination rates (sourced from CDC https://www.cdc.gov/flu/fluvaxview/coverage-1718estimates.htm). Rates over time within years can be interpolated from CDC estimates.\nAs one potential outcome variable in flu modeling, we leverage recent research on costs of seasonal influenza outbreaks by different population breaks (https://www.ncbi.nlm.nih.gov/pubmed/29801998). ","category":"page"},{"location":"validation/","page":"Validation","title":"Validation","text":"3.\tMobility: Airline Network News and Analysis (ANNA) provides monthly passenger traffic numbers from hundreds of major airports around the world (over 300 in Europe, over 250 in the Americas, and over 50 in the rest of the world) updated weekly and dating back to 2014 (https://www.anna.aero/databases/). These will be aggregated by geographic unit and time period to model external population flows as an additional disease vector. For application to weekly datasets, weekly numbers can be interpolated based on the monthly aggregates.  4.\tOnline indicators: Online activity proxies such as Google Trends data on disease relevant searches. This type of data has been shown to be useful in modeling and predicting disease outbreaks in recent years, and may be of interest for our own models. These data are no longer actively updated or maintained, but historical data covering most of the time periods and geographies of interest are available at https://www.google.org/flutrends/about/      a.\tSimilarly, keyword searches on Twitter for ‚ÄòDengue/aegypti‚Äô and or ‚ÄòInfluenza/Flu‚Äô can be used to supplement our datasets. These will be GPS tagged and stored by Twitter for each returned tweet. If GPS is not provided we use the location the user reported to twitter during their user registration. This data provides spatially localized social messaging that can be mapped to the Dengue Fever and Influenza/Flu case datasets provided above, by assigning each GPS tagged tweet to its most likely state (Influenza/Flu) or country (Dengue). These would then be aggregated to the time level (weekly, monthly or yearly) for comparison to the Flu and Dengue Fever databases.","category":"page"},{"location":"","page":"SemanticModels.jl","title":"SemanticModels.jl","text":"(Image: SemanticModels.jl)","category":"page"},{"location":"","page":"SemanticModels.jl","title":"SemanticModels.jl","text":"CurrentModule = SemanticModels","category":"page"},{"location":"","page":"SemanticModels.jl","title":"SemanticModels.jl","text":"SemanticModels is a system for representing scientific knowledge inherent to scientific model structure. Our philosophy is that over the next few decades, the adoption of computation as a first class pillar of scientific thought will be complete, and scientists will do a majority of their thinking about and communicating of ideas in the form of writing and using code. Attempts to teach machines science based on reading texts intended for human consumption is overwhelming, so we use text written for computers as a starting point. This involves extracting meaning from code, and reconciling such information with exogenous sources of information about the world.","category":"page"},{"location":"","page":"SemanticModels.jl","title":"SemanticModels.jl","text":"Scientists typically write procedural code based on libraries for solving mathematical models. When this procedural code is expressed in data-oriented pipelines or workflows, such workflows have limited composability. The most mature scientific field in terms of data-oriented workflows is bioinformatics, where practicing informaticists spend a great deal of time plumbing together procedural scripts and adapting data formats. Automatic adaptation of modeling codes requires a semantic understanding of the model that the code implements/computes. SemanticModels.jl is intended to augment scientists' modeling capabilities by extracting semantic information and facilitating different types of model manipulation and generation.","category":"page"},{"location":"","page":"SemanticModels.jl","title":"SemanticModels.jl","text":"We focus on three problems:","category":"page"},{"location":"","page":"SemanticModels.jl","title":"SemanticModels.jl","text":"Model augmentation: taking an existing model and modifying its components to add features or make comparisons.\nMetamodel construction: combining models or components of models to automatically generate scientific computing workflows.\nModel verification: given a model, corpus of previous applications of that model, and an input to the model, detect if the model is properly functioning.","category":"page"},{"location":"","page":"SemanticModels.jl","title":"SemanticModels.jl","text":"SemanticModels leverages technology from program analysis and natural language processing in order to build a knowledge graph representing the connections between elements of code (variables, values, functions, and expressions) and elements of scientific understanding (concepts, terms, relations). This knowledge graph supports reasoning about how to modify models, construct metamodels, and verify models.","category":"page"},{"location":"","page":"SemanticModels.jl","title":"SemanticModels.jl","text":"The most mature aspects of the library at this point are Knowledge Extraction and modification (Workflow).","category":"page"},{"location":"#Table-of-Contents","page":"SemanticModels.jl","title":"Table of Contents","text":"","category":"section"},{"location":"","page":"SemanticModels.jl","title":"SemanticModels.jl","text":"Pages = [\n     \"index.md\",\n     \"usecases.md\",\n     \"news.md\",\n     \"workflow.md\",\n     \"example.md\",\n     \"malaria.md\",\n     \"theory.md\",\n     \"extraction.md\",\n     \"validation.md\",\n     \"library.md\",\n     \"kgtypes.md\",\n     \"approach.md\",\n     \"slides.md\",\n     \"contributing.md\"]\nDepth = 3","category":"page"},{"location":"","page":"SemanticModels.jl","title":"SemanticModels.jl","text":"This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Agreement No. HR00111990008.","category":"page"},{"location":"example/#Getting-Started-Example","page":"Example","title":"Getting Started Example","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"The following example should help you understand the goals of this project. The goal of this example is to illustrate how you can ingest two scientific models and perform a metamodeling or model modification task on the using the SemanticModels system.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Our two models are an SEIR model that has 4 subpopulations (SEIR) and a ScalingModel has 2 subpopulations (SI). The ScalingModel has a population growth parameter to approximate a changing population size. We want to graft the population growth component of the ScalingModel onto the SEIR model, to produce a new model with novel capabilities.","category":"page"},{"location":"example/#Extraction","page":"Example","title":"Extraction","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"The script bin/extract.jl can extract a knowledge graph from code and documentation.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"For example the SEIR model is described in the following Julia implementation.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"module SEIRmodel\nusing DifferentialEquations\n\n#Susceptible-exposed-infected-recovered model function\nfunction seir_ode(dY,Y,p,t)\n    #Infected per-Capita Rate\n    Œ≤ = p[1]\n    #Incubation Rate\n    œÉ = p[2]\n    #Recover per-capita rate\n    Œ≥ = p[3]\n    #Death Rate\n    Œº = p[4]\n\n    #Susceptible Individual\n    S = Y[1]\n    #Exposed Individual\n    E = Y[2]\n    #Infected Individual\n    I = Y[3]\n    #Recovered Individual\n    #R = Y[4]\n\n    dY[1] = Œº-Œ≤*S*I-Œº*S\n    dY[2] = Œ≤*S*I-(œÉ+Œº)*E\n    dY[3] = œÉ*E - (Œ≥+Œº)*I\nend\n\n#Pram (Infected Rate, Incubation Rate, Recover Rate, Death Rate)\npram=[520/365,1/60,1/30,774835/(65640000*365)]\n#Initialize Param(Susceptible Individuals, Exposed Individuals, Infected Individuals)\ninit=[0.8,0.1,0.1]\ntspan=(0.0,365.0)\n\nseir_prob = ODEProblem(seir_ode,init,tspan,pram)\n\nsol=solve(seir_prob);\n\nusing Plots\n\nva = VectorOfArray(sol.u)\ny = convert(Array,va)\nR = ones(size(sol.t))' - sum(y,dims=1);\n\nplot(sol.t,[y',R'],xlabel=\"Time\",ylabel=\"Proportion\")\nend","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We can extract out a knowledge graph that covers this model along with an Scaling Model from examples/epicookbook/src/ScalingModel.jl","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"julia> include(\"extract.jl\")\n‚îå Info: Graph created from markdown has v vertices and e edges.\n‚îÇ   v = 0\n‚îî   e = 0\n‚îå Info: Parsing julia script\n‚îî   file = \"../examples/epicookbook/src/ScalingModel.jl\"\ns = \"# -*- coding: utf-8 -*-\\n# ---\\n# jupyter:\\n#   jupytext:\\n#     text_representation:\\n#       extension: .jl\\n#       format_name: light\\n#       format_version: '1.3'\\n#       jupytext_version: 0.8.6\\n#   kernelspec:\\n#     display_name: Julia 1.0.3\\n#     language: julia\\n#     name: julia-1.0\\n# ---\\n\\nmodule ScalingModel\\nusing DifferentialEquations\\n\\nfunction micro_1(du, u, parms, time)\\n    # PARAMETER DEFS\\n    # Œ≤ transmition rate\\n    # r net population growth rate\\n    # Œº hosts' natural mortality rate\\n    # Œö population size\\n    # Œ± disease induced mortality rate\\n\\n    Œ≤, r, Œº, K, Œ± = parms\\n    dS = r*(1-S/K)*S - Œ≤*S*I\\n    dI = Œ≤*S*I-(Œº+Œ±)*I\\n    du = [dS,dI]\\nend\\n\\n# +\\n# PARAMETER DEFS\\n# w and m are used to define the other parameters allometrically\\n\\nw = 1;\\nm = 10;\\nŒ≤ = 0.0247*m*w^0.44;\\nr = 0.6*w^-0.27;\\nŒº = 0.4*w^-0.26;\\nK = 16.2*w^-0.7;\\nŒ± = (m-1)*Œº;\\n# -\\n\\nparms = [Œ≤,r,Œº,K,Œ±];\\ninit = [K,1.];\\ntspan = (0.0,10.0);\\n\\nsir_prob = ODEProblem(micro_1,init,tspan,parms)\\n\\nsir_sol = solve(sir_prob);\\n\\nusing Plots\\n\\nplot(sir_sol,xlabel=\\\"Time\\\",ylabel=\\\"Number\\\")\\n\\nm = [5,10,20,40]\\nws = 10 .^collect(range(-3,length = 601,3))\\nŒ≤s = zeros(601,4)\\nfor i = 1:4\\n    Œ≤s[:,i] = 0.0247*m[i]*ws.^0.44\\nend\\nplot(ws,Œ≤s,xlabel=\\\"Weight\\\",ylabel=\\\"\\\\\\\\beta_min\\\", xscale=:log10,yscale=:log10, label=[\\\"m = 5\\\" \\\"m = 10\\\" \\\"m = 20\\\" \\\"m = 40\\\"],lw=3)\\n\\nend\\n\"\n[ Info: unknown expr type for metacollector\nexpr = :(function micro_1(du, u, parms, time)\n      #= none:27 =#\n      (Œ≤, r, Œº, K, Œ±) = parms\n      #= none:28 =#\n      dS = r * (1 - S / K) * S - Œ≤ * S * I\n      #= none:29 =#\n      dI = Œ≤ * S * I - (Œº + Œ±) * I\n      #= none:30 =#\n      du = [dS, dI]\n  end)\n[ Info: unknown expr type for metacollector\nexpr = :(plot(sir_sol, xlabel=\"Time\", ylabel=\"Number\"))\n[ Info: unknown expr type for metacollector\nexpr = :(for i = 1:4\n      #= none:62 =#\n      Œ≤s[:, i] = 0.0247 * m[i] * ws .^ 0.44\n  end)\n[ Info: unknown expr type for metacollector\nexpr = :(plot(ws, Œ≤s, xlabel=\"Weight\", ylabel=\"\\\\beta_min\", xscale=:log10, yscale=:log10, label=[\"m = 5\" \"m = 10\" \"m = 20\" \"m = 40\"], lw=3))\n‚îå Info: script uses modules\n‚îÇ   modules =\n‚îÇ    2-element Array{Any,1}:\n‚îÇ     Any[:DifferentialEquations]\n‚îî     Any[:Plots]\n‚îå Info: script defines functions\n‚îÇ   funcs =\n‚îÇ    1-element Array{Any,1}:\n‚îÇ     :(micro_1(du, u, parms, time)) => quote\n‚îÇ        #= none:27 =#\n‚îÇ        (Œ≤, r, Œº, K, Œ±) = parms\n‚îÇ        #= none:28 =#\n‚îÇ        dS = r * (1 - S / K) * S - Œ≤ * S * I\n‚îÇ        #= none:29 =#\n‚îÇ        dI = Œ≤ * S * I - (Œº + Œ±) * I\n‚îÇ        #= none:30 =#\n‚îÇ        du = [dS, dI]\n‚îî    end\n‚îå Info: script defines glvariables\n‚îÇ   funcs =\n‚îÇ    15-element Array{Any,1}:\n‚îÇ            :w => 1\n‚îÇ            :m => 10\n‚îÇ            :Œ≤ => :(0.0247 * m * w ^ 0.44)\n‚îÇ            :r => :(0.6 * w ^ -0.27)\n‚îÇ            :Œº => :(0.4 * w ^ -0.26)\n‚îÇ            :K => :(16.2 * w ^ -0.7)\n‚îÇ            :Œ± => :((m - 1) * Œº)\n‚îÇ        :parms => :([Œ≤, r, Œº, K, Œ±])\n‚îÇ         :init => :([K, 1.0])\n‚îÇ        :tspan => :((0.0, 10.0))\n‚îÇ     :sir_prob => :(ODEProblem(micro_1, init, tspan, parms))\n‚îÇ      :sir_sol => :(solve(sir_prob))\n‚îÇ            :m => :([5, 10, 20, 40])\n‚îÇ           :ws => :(10 .^ collect(range(-3, length=601, 3)))\n‚îî           :Œ≤s => :(zeros(601, 4))\nfuncdefs = Any[:(micro_1(du, u, parms, time))=>quote\n    #= none:27 =#\n    (Œ≤, r, Œº, K, Œ±) = parms\n    #= none:28 =#\n    dS = r * (1 - S / K) * S - Œ≤ * S * I\n    #= none:29 =#\n    dI = Œ≤ * S * I - (Œº + Œ±) * I\n    #= none:30 =#\n    du = [dS, dI]\nend]\n‚îå Info: local scope definitions\n‚îÇ   subdefs =\n‚îÇ    1-element Array{Any,1}:\n‚îî     :(micro_1(du, u, parms, time)) => MetaCollector{FuncCollector{Array{Any,1}},Array{Any,1},Array{Any,1},Array{Any,1}}(Any[:((Œ≤, r, Œº, K, Œ±) = parms), :(dS = r * (1 - S / K) * S - Œ≤ * S * I), :(dI = Œ≤ * S * I - (Œº + Œ±) * I), :(du = [dS, dI])], FuncCollector{Array{Any,1}}(Any[]), Any[:((Œ≤, r, Œº, K, Œ±))=>:parms, :dS=>:(r * (1 - S / K) * S - Œ≤ * S * I), :dI=>:(Œ≤ * S * I - (Œº + Œ±) * I), :du=>:([dS, dI])], Any[])\n‚îå Info: micro_1(du, u, parms, time) uses modules\n‚îî   modules = 0-element Array{Any,1}\n‚îå Info: micro_1(du, u, parms, time) defines functions\n‚îî   funcs = 0-element Array{Any,1}\n‚îå Info: micro_1(du, u, parms, time) defines variables\n‚îÇ   funcs =\n‚îÇ    4-element Array{Any,1}:\n‚îÇ     :((Œ≤, r, Œº, K, Œ±)) => :parms\n‚îÇ                    :dS => :(r * (1 - S / K) * S - Œ≤ * S * I)\n‚îÇ                    :dI => :(Œ≤ * S * I - (Œº + Œ±) * I)\n‚îî                    :du => :([dS, dI])\n‚îå Info: Making edges\n‚îî   scope = :ScalingModel\n(var, val) = (:((Œ≤, r, Œº, K, Œ±)), :parms)\n(var, val) = (:dS, :(r * (1 - S / K) * S - Œ≤ * S * I))\n(var, val) = (:dI, :(Œ≤ * S * I - (Œº + Œ±) * I))\n(var, val) = (:du, :([dS, dI]))\n‚îå Info: Making edges\n‚îî   scope = \"ScalingModel.micro_1(du, u, parms, time)\"\n(var, val) = (:((Œ≤, r, Œº, K, Œ±)), :parms)\n(var, val) = (:dS, :(r * (1 - S / K) * S - Œ≤ * S * I))\n(var, val) = (:dI, :(Œ≤ * S * I - (Œº + Œ±) * I))\n(var, val) = (:du, :([dS, dI]))\n‚îå Info: Edges found\n‚îî   path = \"../examples/epicookbook/src/ScalingModel.jl\"\n[ Info: The input graph contains 0 unique vertices\n‚îå Info: The input edge list refers to 26 unique vertices.\n‚îî   nv = 26\n‚îå Info: The size of the intersection of these two sets is: 0.\n‚îî   nv = 0\n‚îå Info: src vertex ScalingModel was not in G, and has been inserted.\n‚îî   vname = \"ScalingModel\"\n‚îå Info: dst vertex (Œ≤, r, Œº, K, Œ±) was not in G, and has been inserted.\n‚îî   vname = \"(Œ≤, r, Œº, K, Œ±)\"\n[ Info: Inserting directed edge of type destructure from ScalingModel to (Œ≤, r, Œº, K, Œ±).\n‚îå Info: dst vertex parms was not in G, and has been inserted.\n‚îî   vname = \"parms\"\n[ Info: Inserting directed edge of type val from (Œ≤, r, Œº, K, Œ±) to parms.\n‚îå Info: dst vertex parms was not in G, and has been inserted.\n‚îî   vname = \"parms\"\n[ Info: Inserting directed edge of type comp from ScalingModel to parms.\n‚îå Info: dst vertex Œ≤ was not in G, and has been inserted.\n‚îî   vname = \"Œ≤\"\n[ Info: Inserting directed edge of type var from parms to Œ≤.\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :comp\n‚îÇ   weight = 2\n‚îÇ   type = :comp\n‚îÇ   src = \"ScalingModel\"\n‚îî   dst = \"parms\"\n‚îå Info: dst vertex r was not in G, and has been inserted.\n‚îî   vname = \"r\"\n[ Info: Inserting directed edge of type var from parms to r.\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :comp\n‚îÇ   weight = 3\n‚îÇ   type = :comp\n‚îÇ   src = \"ScalingModel\"\n‚îî   dst = \"parms\"\n‚îå Info: dst vertex Œº was not in G, and has been inserted.\n‚îî   vname = \"Œº\"\n[ Info: Inserting directed edge of type var from parms to Œº.\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :comp\n‚îÇ   weight = 4\n‚îÇ   type = :comp\n‚îÇ   src = \"ScalingModel\"\n‚îî   dst = \"parms\"\n‚îå Info: dst vertex K was not in G, and has been inserted.\n‚îî   vname = \"K\"\n[ Info: Inserting directed edge of type var from parms to K.\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :comp\n‚îÇ   weight = 5\n‚îÇ   type = :comp\n‚îÇ   src = \"ScalingModel\"\n‚îî   dst = \"parms\"\n‚îå Info: dst vertex Œ± was not in G, and has been inserted.\n‚îî   vname = \"Œ±\"\n[ Info: Inserting directed edge of type var from parms to Œ±.\n‚îå Info: dst vertex dS was not in G, and has been inserted.\n‚îî   vname = \"dS\"\n[ Info: Inserting directed edge of type output from ScalingModel to dS.\n‚îå Info: dst vertex r * (1 - S / K) * S - Œ≤ * S * I was not in G, and has been inserted.\n‚îî   vname = \"r * (1 - S / K) * S - Œ≤ * S * I\"\n[ Info: Inserting directed edge of type val from dS to r * (1 - S / K) * S - Œ≤ * S * I.\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :output\n‚îÇ   weight = 2\n‚îÇ   type = :output\n‚îÇ   src = \"ScalingModel\"\n‚îî   dst = \"dS\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"exp\"\n‚îÇ   weight = 2\n‚îÇ   type = \"exp\"\n‚îÇ   src = \"dS\"\n‚îî   dst = \"r * (1 - S / K) * S - Œ≤ * S * I\"\n‚îå Info: dst vertex - was not in G, and has been inserted.\n‚îî   vname = \"-\"\n[ Info: Inserting directed edge of type input from ScalingModel to -.\n‚îå Info: dst vertex Symbol[Symbol(\"r * (1 - S / K) * S\"), Symbol(\"Œ≤ * S * I\")] was not in G, and has been inserted.\n‚îî   vname = \"Symbol[Symbol(\\\"r * (1 - S / K) * S\\\"), Symbol(\\\"Œ≤ * S * I\\\")]\"\n[ Info: Inserting directed edge of type args from - to Symbol[Symbol(\"r * (1 - S / K) * S\"), Symbol(\"Œ≤ * S * I\")].\n‚îå Info: dst vertex dI was not in G, and has been inserted.\n‚îî   vname = \"dI\"\n[ Info: Inserting directed edge of type output from ScalingModel to dI.\n‚îå Info: dst vertex Œ≤ * S * I - (Œº + Œ±) * I was not in G, and has been inserted.\n‚îî   vname = \"Œ≤ * S * I - (Œº + Œ±) * I\"\n[ Info: Inserting directed edge of type val from dI to Œ≤ * S * I - (Œº + Œ±) * I.\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :output\n‚îÇ   weight = 2\n‚îÇ   type = :output\n‚îÇ   src = \"ScalingModel\"\n‚îî   dst = \"dI\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"exp\"\n‚îÇ   weight = 2\n‚îÇ   type = \"exp\"\n‚îÇ   src = \"dI\"\n‚îî   dst = \"Œ≤ * S * I - (Œº + Œ±) * I\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :input\n‚îÇ   weight = 2\n‚îÇ   type = :input\n‚îÇ   src = \"ScalingModel\"\n‚îî   dst = \"-\"\n‚îå Info: dst vertex Symbol[Symbol(\"Œ≤ * S * I\"), Symbol(\"(Œº + Œ±) * I\")] was not in G, and has been inserted.\n‚îî   vname = \"Symbol[Symbol(\\\"Œ≤ * S * I\\\"), Symbol(\\\"(Œº + Œ±) * I\\\")]\"\n[ Info: Inserting directed edge of type args from - to Symbol[Symbol(\"Œ≤ * S * I\"), Symbol(\"(Œº + Œ±) * I\")].\n‚îå Info: dst vertex du was not in G, and has been inserted.\n‚îî   vname = \"du\"\n[ Info: Inserting directed edge of type takes from ScalingModel to du.\n‚îå Info: dst vertex [dS, dI] was not in G, and has been inserted.\n‚îî   vname = \"[dS, dI]\"\n[ Info: Inserting directed edge of type val from du to [dS, dI].\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :has\n‚îÇ   weight = 2\n‚îÇ   type = :has\n‚îÇ   src = \"ScalingModel\"\n‚îî   dst = \"du\"\n‚îå Info: dst vertex collection was not in G, and has been inserted.\n‚îî   vname = \"collection\"\n[ Info: Inserting directed edge of type property from du to collection.\n‚îå Info: src vertex ScalingModel.micro_1(du, u, parms, time) was not in G, and has been inserted.\n‚îî   vname = \"ScalingModel.micro_1(du, u, parms, time)\"\n[ Info: Inserting directed edge of type destructure from ScalingModel.micro_1(du, u, parms, time) to (Œ≤, r, Œº, K, Œ±).\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"val\"\n‚îÇ   weight = 2\n‚îÇ   type = \"val\"\n‚îÇ   src = \"(Œ≤, r, Œº, K, Œ±)\"\n‚îî   dst = \"parms\"\n[ Info: Inserting directed edge of type comp from ScalingModel.micro_1(du, u, parms, time) to parms.\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"var\"\n‚îÇ   weight = 2\n‚îÇ   type = \"var\"\n‚îÇ   src = \"parms\"\n‚îî   dst = \"Œ≤\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :comp\n‚îÇ   weight = 2\n‚îÇ   type = :comp\n‚îÇ   src = \"ScalingModel.micro_1(du, u, parms, time)\"\n‚îî   dst = \"parms\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"var\"\n‚îÇ   weight = 2\n‚îÇ   type = \"var\"\n‚îÇ   src = \"parms\"\n‚îî   dst = \"r\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :comp\n‚îÇ   weight = 3\n‚îÇ   type = :comp\n‚îÇ   src = \"ScalingModel.micro_1(du, u, parms, time)\"\n‚îî   dst = \"parms\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"var\"\n‚îÇ   weight = 2\n‚îÇ   type = \"var\"\n‚îÇ   src = \"parms\"\n‚îî   dst = \"Œº\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :comp\n‚îÇ   weight = 4\n‚îÇ   type = :comp\n‚îÇ   src = \"ScalingModel.micro_1(du, u, parms, time)\"\n‚îî   dst = \"parms\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"var\"\n‚îÇ   weight = 2\n‚îÇ   type = \"var\"\n‚îÇ   src = \"parms\"\n‚îî   dst = \"K\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :comp\n‚îÇ   weight = 5\n‚îÇ   type = :comp\n‚îÇ   src = \"ScalingModel.micro_1(du, u, parms, time)\"\n‚îî   dst = \"parms\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"var\"\n‚îÇ   weight = 2\n‚îÇ   type = \"var\"\n‚îÇ   src = \"parms\"\n‚îî   dst = \"Œ±\"\n[ Info: Inserting directed edge of type output from ScalingModel.micro_1(du, u, parms, time) to dS.\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"val\"\n‚îÇ   weight = 3\n‚îÇ   type = \"val\"\n‚îÇ   src = \"dS\"\n‚îî   dst = \"r * (1 - S / K) * S - Œ≤ * S * I\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :output\n‚îÇ   weight = 2\n‚îÇ   type = :output\n‚îÇ   src = \"ScalingModel.micro_1(du, u, parms, time)\"\n‚îî   dst = \"dS\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"exp\"\n‚îÇ   weight = 4\n‚îÇ   type = \"exp\"\n‚îÇ   src = \"dS\"\n‚îî   dst = \"r * (1 - S / K) * S - Œ≤ * S * I\"\n[ Info: Inserting directed edge of type input from ScalingModel.micro_1(du, u, parms, time) to -.\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"args\"\n‚îÇ   weight = 2\n‚îÇ   type = \"args\"\n‚îÇ   src = \"-\"\n‚îî   dst = \"Symbol[Symbol(\\\"r * (1 - S / K) * S\\\"), Symbol(\\\"Œ≤ * S * I\\\")]\"\n[ Info: Inserting directed edge of type output from ScalingModel.micro_1(du, u, parms, time) to dI.\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"val\"\n‚îÇ   weight = 3\n‚îÇ   type = \"val\"\n‚îÇ   src = \"dI\"\n‚îî   dst = \"Œ≤ * S * I - (Œº + Œ±) * I\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :output\n‚îÇ   weight = 2\n‚îÇ   type = :output\n‚îÇ   src = \"ScalingModel.micro_1(du, u, parms, time)\"\n‚îî   dst = \"dI\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"exp\"\n‚îÇ   weight = 4\n‚îÇ   type = \"exp\"\n‚îÇ   src = \"dI\"\n‚îî   dst = \"Œ≤ * S * I - (Œº + Œ±) * I\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :input\n‚îÇ   weight = 2\n‚îÇ   type = :input\n‚îÇ   src = \"ScalingModel.micro_1(du, u, parms, time)\"\n‚îî   dst = \"-\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"args\"\n‚îÇ   weight = 2\n‚îÇ   type = \"args\"\n‚îÇ   src = \"-\"\n‚îî   dst = \"Symbol[Symbol(\\\"Œ≤ * S * I\\\"), Symbol(\\\"(Œº + Œ±) * I\\\")]\"\n[ Info: Inserting directed edge of type takes from ScalingModel.micro_1(du, u, parms, time) to du.\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"val\"\n‚îÇ   weight = 2\n‚îÇ   type = \"val\"\n‚îÇ   src = \"du\"\n‚îî   dst = \"[dS, dI]\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :has\n‚îÇ   weight = 2\n‚îÇ   type = :has\n‚îÇ   src = \"ScalingModel.micro_1(du, u, parms, time)\"\n‚îî   dst = \"du\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"property\"\n‚îÇ   weight = 2\n‚îÇ   type = \"property\"\n‚îÇ   src = \"du\"\n‚îî   dst = \"collection\"\n‚îå Info: Returning graph G\n‚îÇ   nedges = 24\n‚îî   cardinality = 20\n‚îå Info: Code graph 1 has v vertices and e edges.\n‚îÇ   v = 20\n‚îî   e = 24\n‚îå Info: Parsing julia script\n‚îî   file = \"../examples/epicookbook/src/SEIRmodel.jl\"\ns = \"# -*- coding: utf-8 -*-\\n# ---\\n# jupyter:\\n#   jupytext:\\n#     text_representation:\\n#       extension: .jl\\n#       format_name: light\\n#       format_version: '1.3'\\n#       jupytext_version: 0.8.6\\n#   kernelspec:\\n#     display_name: Julia 1.0.3\\n#     language: julia\\n#     name: julia-1.0\\n# ---\\n\\nmodule SEIRmodel\\nusing DifferentialEquations\\n\\n#Susceptible-exposed-infected-recovered model function\\nfunction seir_ode(dY,Y,p,t)\\n    #Infected per-Capita Rate\\n    Œ≤ = p[1]\\n    #Incubation Rate\\n    œÉ = p[2]\\n    #Recover per-capita rate\\n    Œ≥ = p[3]\\n    #Death Rate\\n    Œº = p[4]\\n\\n    #Susceptible Individual\\n    S = Y[1]\\n    #Exposed Individual\\n    E = Y[2]\\n    #Infected Individual\\n    I = Y[3]\\n    #Recovered Individual\\n    #R = Y[4]\\n\\n    dY[1] = Œº-Œ≤*S*I-Œº*S\\n    dY[2] = Œ≤*S*I-(œÉ+Œº)*E\\n    dY[3] = œÉ*E - (Œ≥+Œº)*I\\nend\\n\\n#Pram (Infected Rate, Incubation Rate, Recover Rate, Death Rate)\\npram=[520/365,1/60,1/30,774835/(65640000*365)]\\n#Initialize Param(Susceptible Individuals, Exposed Individuals, Infected Individuals)\\ninit=[0.8,0.1,0.1]\\ntspan=(0.0,365.0)\\n\\nseir_prob = ODEProblem(seir_ode,init,tspan,pram)\\n\\nsol=solve(seir_prob);\\n\\nusing Plots\\n\\nva = VectorOfArray(sol.u)\\ny = convert(Array,va)\\nR = ones(size(sol.t))' - sum(y,dims=1);\\n\\nplot(sol.t,[y',R'],xlabel=\\\"Time\\\",ylabel=\\\"Proportion\\\")\\n\\n\\n\\nend\\n\"\n[ Info: unknown expr type for metacollector\nexpr = :(function seir_ode(dY, Y, p, t)\n      #= none:22 =#\n      Œ≤ = p[1]\n      #= none:24 =#\n      œÉ = p[2]\n      #= none:26 =#\n      Œ≥ = p[3]\n      #= none:28 =#\n      Œº = p[4]\n      #= none:31 =#\n      S = Y[1]\n      #= none:33 =#\n      E = Y[2]\n      #= none:35 =#\n      I = Y[3]\n      #= none:39 =#\n      dY[1] = (Œº - Œ≤ * S * I) - Œº * S\n      #= none:40 =#\n      dY[2] = Œ≤ * S * I - (œÉ + Œº) * E\n      #= none:41 =#\n      dY[3] = œÉ * E - (Œ≥ + Œº) * I\n  end)\n[ Info: unknown expr type for metacollector\nexpr = :(plot(sol.t, [y', R'], xlabel=\"Time\", ylabel=\"Proportion\"))\n‚îå Info: script uses modules\n‚îÇ   modules =\n‚îÇ    2-element Array{Any,1}:\n‚îÇ     Any[:DifferentialEquations]\n‚îî     Any[:Plots]\n‚îå Info: script defines functions\n‚îÇ   funcs =\n‚îÇ    1-element Array{Any,1}:\n‚îÇ     :(seir_ode(dY, Y, p, t)) => quote\n‚îÇ        #= none:22 =#\n‚îÇ        Œ≤ = p[1]\n‚îÇ        #= none:24 =#\n‚îÇ        œÉ = p[2]\n‚îÇ        #= none:26 =#\n‚îÇ        Œ≥ = p[3]\n‚îÇ        #= none:28 =#\n‚îÇ        Œº = p[4]\n‚îÇ        #= none:31 =#\n‚îÇ        S = Y[1]\n‚îÇ        #= none:33 =#\n‚îÇ        E = Y[2]\n‚îÇ        #= none:35 =#\n‚îÇ        I = Y[3]\n‚îÇ        #= none:39 =#\n‚îÇ        dY[1] = (Œº - Œ≤ * S * I) - Œº * S\n‚îÇ        #= none:40 =#\n‚îÇ        dY[2] = Œ≤ * S * I - (œÉ + Œº) * E\n‚îÇ        #= none:41 =#\n‚îÇ        dY[3] = œÉ * E - (Œ≥ + Œº) * I\n‚îî    end\n‚îå Info: script defines glvariables\n‚îÇ   funcs =\n‚îÇ    8-element Array{Any,1}:\n‚îÇ          :pram => :([520 / 365, 1 / 60, 1 / 30, 774835 / (65640000 * 365)])\n‚îÇ          :init => :([0.8, 0.1, 0.1])\n‚îÇ         :tspan => :((0.0, 365.0))\n‚îÇ     :seir_prob => :(ODEProblem(seir_ode, init, tspan, pram))\n‚îÇ           :sol => :(solve(seir_prob))\n‚îÇ            :va => :(VectorOfArray(sol.u))\n‚îÇ             :y => :(convert(Array, va))\n‚îî             :R => :((ones(size(sol.t)))' - sum(y, dims=1))\nfuncdefs = Any[:(seir_ode(dY, Y, p, t))=>quote\n    #= none:22 =#\n    Œ≤ = p[1]\n    #= none:24 =#\n    œÉ = p[2]\n    #= none:26 =#\n    Œ≥ = p[3]\n    #= none:28 =#\n    Œº = p[4]\n    #= none:31 =#\n    S = Y[1]\n    #= none:33 =#\n    E = Y[2]\n    #= none:35 =#\n    I = Y[3]\n    #= none:39 =#\n    dY[1] = (Œº - Œ≤ * S * I) - Œº * S\n    #= none:40 =#\n    dY[2] = Œ≤ * S * I - (œÉ + Œº) * E\n    #= none:41 =#\n    dY[3] = œÉ * E - (Œ≥ + Œº) * I\nend]\n‚îå Info: local scope definitions\n‚îÇ   subdefs =\n‚îÇ    1-element Array{Any,1}:\n‚îî     :(seir_ode(dY, Y, p, t)) => MetaCollector{FuncCollector{Array{Any,1}},Array{Any,1},Array{Any,1},Array{Any,1}}(Any[:(Œ≤ = p[1]), :(œÉ = p[2]), :(Œ≥ = p[3]), :(Œº = p[4]), :(S = Y[1]), :(E = Y[2]), :(I = Y[3]), :(dY[1] = (Œº - Œ≤ * S * I) - Œº * S), :(dY[2] = Œ≤ * S * I - (œÉ + Œº) * E), :(dY[3] = œÉ * E - (Œ≥ + Œº) * I)], FuncCollector{Array{Any,1}}(Any[]), Any[:Œ≤=>:(p[1]), :œÉ=>:(p[2]), :Œ≥=>:(p[3]), :Œº=>:(p[4]), :S=>:(Y[1]), :E=>:(Y[2]), :I=>:(Y[3]), :(dY[1])=>:((Œº - Œ≤ * S * I) - Œº * S), :(dY[2])=>:(Œ≤ * S * I - (œÉ + Œº) * E), :(dY[3])=>:(œÉ * E - (Œ≥ + Œº) * I)], Any[])\n‚îå Info: seir_ode(dY, Y, p, t) uses modules\n‚îî   modules = 0-element Array{Any,1}\n‚îå Info: seir_ode(dY, Y, p, t) defines functions\n‚îî   funcs = 0-element Array{Any,1}\n‚îå Info: seir_ode(dY, Y, p, t) defines variables\n‚îÇ   funcs =\n‚îÇ    10-element Array{Any,1}:\n‚îÇ           :Œ≤ => :(p[1])\n‚îÇ           :œÉ => :(p[2])\n‚îÇ           :Œ≥ => :(p[3])\n‚îÇ           :Œº => :(p[4])\n‚îÇ           :S => :(Y[1])\n‚îÇ           :E => :(Y[2])\n‚îÇ           :I => :(Y[3])\n‚îÇ     :(dY[1]) => :((Œº - Œ≤ * S * I) - Œº * S)\n‚îÇ     :(dY[2]) => :(Œ≤ * S * I - (œÉ + Œº) * E)\n‚îî     :(dY[3]) => :(œÉ * E - (Œ≥ + Œº) * I)\n‚îå Info: Making edges\n‚îî   scope = :SEIRmodel\n(var, val) = (:Œ≤, :(p[1]))\n(var, val) = (:œÉ, :(p[2]))\n(var, val) = (:Œ≥, :(p[3]))\n(var, val) = (:Œº, :(p[4]))\n(var, val) = (:S, :(Y[1]))\n(var, val) = (:E, :(Y[2]))\n(var, val) = (:I, :(Y[3]))\n(var, val) = (:(dY[1]), :((Œº - Œ≤ * S * I) - Œº * S))\n(var, val) = (:(dY[2]), :(Œ≤ * S * I - (œÉ + Œº) * E))\n(var, val) = (:(dY[3]), :(œÉ * E - (Œ≥ + Œº) * I))\n‚îå Info: Making edges\n‚îî   scope = \"SEIRmodel.seir_ode(dY, Y, p, t)\"\n(var, val) = (:Œ≤, :(p[1]))\n(var, val) = (:œÉ, :(p[2]))\n(var, val) = (:Œ≥, :(p[3]))\n(var, val) = (:Œº, :(p[4]))\n(var, val) = (:S, :(Y[1]))\n(var, val) = (:E, :(Y[2]))\n(var, val) = (:I, :(Y[3]))\n(var, val) = (:(dY[1]), :((Œº - Œ≤ * S * I) - Œº * S))\n(var, val) = (:(dY[2]), :(Œ≤ * S * I - (œÉ + Œº) * E))\n(var, val) = (:(dY[3]), :(œÉ * E - (Œ≥ + Œº) * I))\n‚îå Info: Edges found\n‚îî   path = \"../examples/epicookbook/src/SEIRmodel.jl\"\n[ Info: The input graph contains 20 unique vertices\n‚îå Info: The input edge list refers to 37 unique vertices.\n‚îî   nv = 37\n‚îå Info: The size of the intersection of these two sets is: 1.\n‚îî   nv = 1\n‚îå Info: src vertex SEIRmodel was not in G, and has been inserted.\n‚îî   vname = \"SEIRmodel\"\n‚îå Info: dst vertex Œ≤ was not in G, and has been inserted.\n‚îî   vname = \"Œ≤\"\n[ Info: Inserting directed edge of type takes from SEIRmodel to Œ≤.\n‚îå Info: dst vertex p[1] was not in G, and has been inserted.\n‚îî   vname = \"p[1]\"\n[ Info: Inserting directed edge of type val from Œ≤ to p[1].\n‚îå Info: dst vertex œÉ was not in G, and has been inserted.\n‚îî   vname = \"œÉ\"\n[ Info: Inserting directed edge of type takes from SEIRmodel to œÉ.\n‚îå Info: dst vertex p[2] was not in G, and has been inserted.\n‚îî   vname = \"p[2]\"\n[ Info: Inserting directed edge of type val from œÉ to p[2].\n‚îå Info: dst vertex Œ≥ was not in G, and has been inserted.\n‚îî   vname = \"Œ≥\"\n[ Info: Inserting directed edge of type takes from SEIRmodel to Œ≥.\n‚îå Info: dst vertex p[3] was not in G, and has been inserted.\n‚îî   vname = \"p[3]\"\n[ Info: Inserting directed edge of type val from Œ≥ to p[3].\n‚îå Info: dst vertex Œº was not in G, and has been inserted.\n‚îî   vname = \"Œº\"\n[ Info: Inserting directed edge of type takes from SEIRmodel to Œº.\n‚îå Info: dst vertex p[4] was not in G, and has been inserted.\n‚îî   vname = \"p[4]\"\n[ Info: Inserting directed edge of type val from Œº to p[4].\n‚îå Info: dst vertex S was not in G, and has been inserted.\n‚îî   vname = \"S\"\n[ Info: Inserting directed edge of type takes from SEIRmodel to S.\n‚îå Info: dst vertex Y[1] was not in G, and has been inserted.\n‚îî   vname = \"Y[1]\"\n[ Info: Inserting directed edge of type val from S to Y[1].\n‚îå Info: dst vertex E was not in G, and has been inserted.\n‚îî   vname = \"E\"\n[ Info: Inserting directed edge of type takes from SEIRmodel to E.\n‚îå Info: dst vertex Y[2] was not in G, and has been inserted.\n‚îî   vname = \"Y[2]\"\n[ Info: Inserting directed edge of type val from E to Y[2].\n‚îå Info: dst vertex I was not in G, and has been inserted.\n‚îî   vname = \"I\"\n[ Info: Inserting directed edge of type takes from SEIRmodel to I.\n‚îå Info: dst vertex Y[3] was not in G, and has been inserted.\n‚îî   vname = \"Y[3]\"\n[ Info: Inserting directed edge of type val from I to Y[3].\n‚îå Info: dst vertex dY[1] was not in G, and has been inserted.\n‚îî   vname = \"dY[1]\"\n[ Info: Inserting directed edge of type output from SEIRmodel to dY[1].\n‚îå Info: dst vertex (Œº - Œ≤ * S * I) - Œº * S was not in G, and has been inserted.\n‚îî   vname = \"(Œº - Œ≤ * S * I) - Œº * S\"\n[ Info: Inserting directed edge of type val from dY[1] to (Œº - Œ≤ * S * I) - Œº * S.\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :output\n‚îÇ   weight = 2\n‚îÇ   type = :output\n‚îÇ   src = \"SEIRmodel\"\n‚îî   dst = \"dY[1]\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"exp\"\n‚îÇ   weight = 2\n‚îÇ   type = \"exp\"\n‚îÇ   src = \"dY[1]\"\n‚îî   dst = \"(Œº - Œ≤ * S * I) - Œº * S\"\n[ Info: Inserting directed edge of type input from SEIRmodel to -.\n‚îå Info: dst vertex Symbol[Symbol(\"Œº - Œ≤ * S * I\"), Symbol(\"Œº * S\")] was not in G, and has been inserted.\n‚îî   vname = \"Symbol[Symbol(\\\"Œº - Œ≤ * S * I\\\"), Symbol(\\\"Œº * S\\\")]\"\n[ Info: Inserting directed edge of type args from - to Symbol[Symbol(\"Œº - Œ≤ * S * I\"), Symbol(\"Œº * S\")].\n‚îå Info: dst vertex dY[2] was not in G, and has been inserted.\n‚îî   vname = \"dY[2]\"\n[ Info: Inserting directed edge of type output from SEIRmodel to dY[2].\n‚îå Info: dst vertex Œ≤ * S * I - (œÉ + Œº) * E was not in G, and has been inserted.\n‚îî   vname = \"Œ≤ * S * I - (œÉ + Œº) * E\"\n[ Info: Inserting directed edge of type val from dY[2] to Œ≤ * S * I - (œÉ + Œº) * E.\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :output\n‚îÇ   weight = 2\n‚îÇ   type = :output\n‚îÇ   src = \"SEIRmodel\"\n‚îî   dst = \"dY[2]\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"exp\"\n‚îÇ   weight = 2\n‚îÇ   type = \"exp\"\n‚îÇ   src = \"dY[2]\"\n‚îî   dst = \"Œ≤ * S * I - (œÉ + Œº) * E\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :input\n‚îÇ   weight = 2\n‚îÇ   type = :input\n‚îÇ   src = \"SEIRmodel\"\n‚îî   dst = \"-\"\n‚îå Info: dst vertex Symbol[Symbol(\"Œ≤ * S * I\"), Symbol(\"(œÉ + Œº) * E\")] was not in G, and has been inserted.\n‚îî   vname = \"Symbol[Symbol(\\\"Œ≤ * S * I\\\"), Symbol(\\\"(œÉ + Œº) * E\\\")]\"\n[ Info: Inserting directed edge of type args from - to Symbol[Symbol(\"Œ≤ * S * I\"), Symbol(\"(œÉ + Œº) * E\")].\n‚îå Info: dst vertex dY[3] was not in G, and has been inserted.\n‚îî   vname = \"dY[3]\"\n[ Info: Inserting directed edge of type output from SEIRmodel to dY[3].\n‚îå Info: dst vertex œÉ * E - (Œ≥ + Œº) * I was not in G, and has been inserted.\n‚îî   vname = \"œÉ * E - (Œ≥ + Œº) * I\"\n[ Info: Inserting directed edge of type val from dY[3] to œÉ * E - (Œ≥ + Œº) * I.\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :output\n‚îÇ   weight = 2\n‚îÇ   type = :output\n‚îÇ   src = \"SEIRmodel\"\n‚îî   dst = \"dY[3]\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"exp\"\n‚îÇ   weight = 2\n‚îÇ   type = \"exp\"\n‚îÇ   src = \"dY[3]\"\n‚îî   dst = \"œÉ * E - (Œ≥ + Œº) * I\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :input\n‚îÇ   weight = 3\n‚îÇ   type = :input\n‚îÇ   src = \"SEIRmodel\"\n‚îî   dst = \"-\"\n‚îå Info: dst vertex Symbol[Symbol(\"œÉ * E\"), Symbol(\"(Œ≥ + Œº) * I\")] was not in G, and has been inserted.\n‚îî   vname = \"Symbol[Symbol(\\\"œÉ * E\\\"), Symbol(\\\"(Œ≥ + Œº) * I\\\")]\"\n[ Info: Inserting directed edge of type args from - to Symbol[Symbol(\"œÉ * E\"), Symbol(\"(Œ≥ + Œº) * I\")].\n‚îå Info: src vertex SEIRmodel.seir_ode(dY, Y, p, t) was not in G, and has been inserted.\n‚îî   vname = \"SEIRmodel.seir_ode(dY, Y, p, t)\"\n[ Info: Inserting directed edge of type takes from SEIRmodel.seir_ode(dY, Y, p, t) to Œ≤.\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"val\"\n‚îÇ   weight = 2\n‚îÇ   type = \"val\"\n‚îÇ   src = \"Œ≤\"\n‚îî   dst = \"p[1]\"\n[ Info: Inserting directed edge of type takes from SEIRmodel.seir_ode(dY, Y, p, t) to œÉ.\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"val\"\n‚îÇ   weight = 2\n‚îÇ   type = \"val\"\n‚îÇ   src = \"œÉ\"\n‚îî   dst = \"p[2]\"\n[ Info: Inserting directed edge of type takes from SEIRmodel.seir_ode(dY, Y, p, t) to Œ≥.\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"val\"\n‚îÇ   weight = 2\n‚îÇ   type = \"val\"\n‚îÇ   src = \"Œ≥\"\n‚îî   dst = \"p[3]\"\n[ Info: Inserting directed edge of type takes from SEIRmodel.seir_ode(dY, Y, p, t) to Œº.\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"val\"\n‚îÇ   weight = 2\n‚îÇ   type = \"val\"\n‚îÇ   src = \"Œº\"\n‚îî   dst = \"p[4]\"\n[ Info: Inserting directed edge of type takes from SEIRmodel.seir_ode(dY, Y, p, t) to S.\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"val\"\n‚îÇ   weight = 2\n‚îÇ   type = \"val\"\n‚îÇ   src = \"S\"\n‚îî   dst = \"Y[1]\"\n[ Info: Inserting directed edge of type takes from SEIRmodel.seir_ode(dY, Y, p, t) to E.\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"val\"\n‚îÇ   weight = 2\n‚îÇ   type = \"val\"\n‚îÇ   src = \"E\"\n‚îî   dst = \"Y[2]\"\n[ Info: Inserting directed edge of type takes from SEIRmodel.seir_ode(dY, Y, p, t) to I.\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"val\"\n‚îÇ   weight = 2\n‚îÇ   type = \"val\"\n‚îÇ   src = \"I\"\n‚îî   dst = \"Y[3]\"\n[ Info: Inserting directed edge of type output from SEIRmodel.seir_ode(dY, Y, p, t) to dY[1].\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"val\"\n‚îÇ   weight = 3\n‚îÇ   type = \"val\"\n‚îÇ   src = \"dY[1]\"\n‚îî   dst = \"(Œº - Œ≤ * S * I) - Œº * S\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :output\n‚îÇ   weight = 2\n‚îÇ   type = :output\n‚îÇ   src = \"SEIRmodel.seir_ode(dY, Y, p, t)\"\n‚îî   dst = \"dY[1]\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"exp\"\n‚îÇ   weight = 4\n‚îÇ   type = \"exp\"\n‚îÇ   src = \"dY[1]\"\n‚îî   dst = \"(Œº - Œ≤ * S * I) - Œº * S\"\n[ Info: Inserting directed edge of type input from SEIRmodel.seir_ode(dY, Y, p, t) to -.\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"args\"\n‚îÇ   weight = 2\n‚îÇ   type = \"args\"\n‚îÇ   src = \"-\"\n‚îî   dst = \"Symbol[Symbol(\\\"Œº - Œ≤ * S * I\\\"), Symbol(\\\"Œº * S\\\")]\"\n[ Info: Inserting directed edge of type output from SEIRmodel.seir_ode(dY, Y, p, t) to dY[2].\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"val\"\n‚îÇ   weight = 3\n‚îÇ   type = \"val\"\n‚îÇ   src = \"dY[2]\"\n‚îî   dst = \"Œ≤ * S * I - (œÉ + Œº) * E\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :output\n‚îÇ   weight = 2\n‚îÇ   type = :output\n‚îÇ   src = \"SEIRmodel.seir_ode(dY, Y, p, t)\"\n‚îî   dst = \"dY[2]\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"exp\"\n‚îÇ   weight = 4\n‚îÇ   type = \"exp\"\n‚îÇ   src = \"dY[2]\"\n‚îî   dst = \"Œ≤ * S * I - (œÉ + Œº) * E\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :input\n‚îÇ   weight = 2\n‚îÇ   type = :input\n‚îÇ   src = \"SEIRmodel.seir_ode(dY, Y, p, t)\"\n‚îî   dst = \"-\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"args\"\n‚îÇ   weight = 2\n‚îÇ   type = \"args\"\n‚îÇ   src = \"-\"\n‚îî   dst = \"Symbol[Symbol(\\\"Œ≤ * S * I\\\"), Symbol(\\\"(œÉ + Œº) * E\\\")]\"\n[ Info: Inserting directed edge of type output from SEIRmodel.seir_ode(dY, Y, p, t) to dY[3].\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"val\"\n‚îÇ   weight = 3\n‚îÇ   type = \"val\"\n‚îÇ   src = \"dY[3]\"\n‚îî   dst = \"œÉ * E - (Œ≥ + Œº) * I\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :output\n‚îÇ   weight = 2\n‚îÇ   type = :output\n‚îÇ   src = \"SEIRmodel.seir_ode(dY, Y, p, t)\"\n‚îî   dst = \"dY[3]\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"exp\"\n‚îÇ   weight = 4\n‚îÇ   type = \"exp\"\n‚îÇ   src = \"dY[3]\"\n‚îî   dst = \"œÉ * E - (Œ≥ + Œº) * I\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = :input\n‚îÇ   weight = 3\n‚îÇ   type = :input\n‚îÇ   src = \"SEIRmodel.seir_ode(dY, Y, p, t)\"\n‚îî   dst = \"-\"\n‚îå Info: Incrementing weight of existing directed edge\n‚îÇ   edge_type = \"args\"\n‚îÇ   weight = 2\n‚îÇ   type = \"args\"\n‚îÇ   src = \"-\"\n‚îî   dst = \"Symbol[Symbol(\\\"œÉ * E\\\"), Symbol(\\\"(Œ≥ + Œº) * I\\\")]\"\n‚îå Info: Returning graph G\n‚îÇ   nedges = 59\n‚îî   cardinality = 45\n‚îå Info: Code graph 2 has v vertices and e edges.\n‚îÇ   v = 45\n‚îî   e = 59\n[ Info: All markdown and code files have been parsed; writing final knowledge graph to dot file\nProcess(`dot -Tsvg -O ../examples/epicookbook/data/dot_file_ex1.dot`, ProcessExited(0))","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"The extraction code will generate a dot file diagram of the edges in the graph.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Due to the fact that code extraction is a heuristic, there is some cleaning of the knowledge  graph required before it is ready for reasoning.","category":"page"},{"location":"example/#Reasoning","page":"Example","title":"Reasoning","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Once the information is extracted from the documentation and code, we can visualize the knowledge as a graph. Most edges of type cooccur are elided for clarity.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"(Image: Knowledge Graph from epicookbook)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"This knowledge graph contains all the connections we need to combine components across models. Once can view this combination as either a modification of one model by substituting components of another model, or as the automatic generation of a metamodel by synthesizing components from the knowledge graph into a single coherent model. Further theoretical analysis of metamodeling and model modification as mathematical problems is warranted to make these categories unambiguous and precisely defined.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Once we identify a subgraph of related components we can identify the graft point between the two models. We look for a common variable that is used in two models, specifically in a derivative calculation. We find the variable S which appears in dS and dY (as S=Y[1] and dY = derivative(Y)). The knowledge that dS, dY are derivatives comes from the background knowledge of modeling that comes from reading textbooks and general scientific knowledge, while the fact that S and Y[1] both appear in an expression mu-beta*S*I - mu*S comes from the specific documents and codebases under consideration by the metamodeler.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"(Image: Knowledge Subgraph showing model modification)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"This subgraph must then extend out to capture all of the relevant information such as the parameter sets encountered, the function calls that contain these variables and expressions. We have found the largest relevant subgraph for some unspecified definition of relevance. From this subgraph, a human modeler can easily instruct the SemanticModels system on how to combine the SEIRmodel and ScalingModel programs into a single model and generate a program to execute it.","category":"page"},{"location":"example/#Generation","page":"Example","title":"Generation","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Once reasoning is complete the graft.jl program will run over the extracted knowledge graph, and generate a new model. In this case we want to take the birth rate dynamics from the ScalingModel and add them to the SEIR model to create an SEIR+birth_rate model.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Here is the code that does the grafting.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"using Cassette\nusing DifferentialEquations\nusing SemanticModels.Parsers\nusing SemanticModels.Dubstep\n\n# source of original problem\ninclude(\"../examples/epicookbook/src/SEIRmodel.jl\")\n\n#the functions we want to modify\nseir_ode = SEIRmodel.seir_ode\n\n# source of the problem we want to take from\nexpr = parsefile(\"../examples/epicookbook/src/ScalingModel.jl\")","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Once you have identified the entry point to your model, you can identify pieces of another model that you want to graft onto it. This piece of the other model might take significant preparation in order to be ready to fit onto the base model. These transformations include changing variables, and other plumbing aspects. If you stick to taking whole functions and not expressions, this prep work is reduced.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"# Find the expression we want to graft\n#vital dynamics S rate expression\nvdsre = expr.args[3].args[5].args[2].args[4]\n@show popgrowth = vdsre.args[2].args[2]\nreplacevar(expr, old, new) = begin\n    dump(expr)\n    expr.args[3].args[3].args[3] = new\n    return expr\nend\npopgrowth = replacevar(popgrowth, :K,:N)\n\n# generate the function newfunc\n# this eval happens at the top level so should only happen once\nnewfunc = eval(:(fpopgrowth(r,S,N) = $popgrowth))\n\n# This is the new problem\n# notice the signature doesn't even match, we have added a new parameter\nfunction fprime(dY,Y,p,t, œµ)\n    #Infected per-Capita Rate\n    Œ≤ = p[1]\n    #Incubation Rate\n    œÉ = p[2]\n    #Recover per-capita rate\n    Œ≥ = p[3]\n    #Death Rate\n    Œº = p[4]\n\n    #Susceptible Individual\n    S = Y[1]\n    #Exposed Individual\n    E = Y[2]\n    #Infected Individual\n    I = Y[3]\n    #Recovered Individual\n    #R = Y[4]\n\n    # here is the graft point\n    dY[1] = Œº-Œ≤*S*I-Œº*S + newfunc(œµ, S, S+E+I)\n    dY[2] = Œ≤*S*I-(œÉ+Œº)*E\n    dY[3] = œÉ*E - (Œ≥+Œº)*I\nend","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Define the overdub behavior, all the fucntions needed to be defined at this point using run time values slows down overdub.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"function Cassette.overdub(ctx::Dubstep.GraftCtx, f::typeof(seir_ode), args...)\n    # this call matches the new signature\n    return Cassette.fallback(ctx, fprime, args..., ctx.metadata[:lambda])\nend","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"The last step is to run the new model!","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"#set up our modeling configuration\nfunction g()\n    #Pram (Infected Rate, Incubation Rate, Recover Rate, Death Rate)\n    pram=[520/365,1/60,1/30,774835/(65640000*365)]\n    #Initialize Param(Susceptible Individuals, Exposed Individuals, Infected Individuals)\n    init=[0.8,0.1,0.1]\n    tspan=(0.0,365.0)\n\n    seir_prob = ODEProblem(seir_ode,init,tspan,pram)\n\n    sol=solve(seir_prob);\nend\n\n# sweep over population growth rates\nfunction scalegrowth(Œª=1.0)\n    # ctx.metadata holds our new parameter\n    ctx = Dubstep.GraftCtx(metadata=Dict(:lambda=>Œª))\n    return Cassette.overdub(ctx, g)\nend\n\nprintln(\"S\\tI\\tR\")\nfor Œª in [1.0,1.1,1.2]\n    @time S,I,R = scalegrowth(Œª)(365)\n    println(\"$S\\t$I\\t$R\")\nend\n#it works!","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"julia> include(\"graft.jl\")\ns = \"# -*- coding: utf-8 -*-\\n# ---\\n# jupyter:\\n#   jupytext:\\n#     text_representation:\\n#       extension: .jl\\n#       format_name: light\\n#       format_version: '1.3'\\n#       jupytext_version: 0.8.6\\n#   kernelspec:\\n#     display_name: Julia 1.0.3\\n#     language: julia\\n#     name: julia-1.0\\n# ---\\n\\nmodule ScalingModel\\nusing DifferentialEquations\\n\\nfunction micro_1(du, u, parms, time)\\n    # PARAMETER DEFS\\n    # Œ≤ transmition rate\\n    # r net population growth rate\\n    # Œº hosts' natural mortality rate\\n    # Œö population size\\n    # Œ± disease induced mortality rate\\n\\n    Œ≤, r, Œº, K, Œ± = parms\\n    dS = r*(1-S/K)*S - Œ≤*S*I\\n    dI = Œ≤*S*I-(Œº+Œ±)*I\\n    du = [dS,dI]\\nend\\n\\n# +\\n# PARAMETER DEFS\\n# w and m are used to define the other parameters allometrically\\n\\nw = 1;\\nm = 10;\\nŒ≤ = 0.0247*m*w^0.44;\\nr = 0.6*w^-0.27;\\nŒº = 0.4*w^-0.26;\\nK = 16.2*w^-0.7;\\nŒ± = (m-1)*Œº;\\n# -\\n\\nparms = [Œ≤,r,Œº,K,Œ±];\\ninit = [K,1.];\\ntspan = (0.0,10.0);\\n\\nsir_prob = ODEProblem(micro_1,init,tspan,parms)\\n\\nsir_sol = solve(sir_prob);\\n\\nusing Plots\\n\\nplot(sir_sol,xlabel=\\\"Time\\\",ylabel=\\\"Number\\\")\\n\\nm = [5,10,20,40]\\nws = 10 .^collect(range(-3,length = 601,3))\\nŒ≤s = zeros(601,4)\\nfor i = 1:4\\n    Œ≤s[:,i] = 0.0247*m[i]*ws.^0.44\\nend\\nplot(ws,Œ≤s,xlabel=\\\"Weight\\\",ylabel=\\\"\\\\\\\\beta_min\\\", xscale=:log10,yscale=:log10, label=[\\\"m = 5\\\" \\\"m = 10\\\" \\\"m = 20\\\" \\\"m = 40\\\"],lw=3)\\n\\nend\\n\"\npopgrowth = (vdsre.args[2]).args[2] = :(r * (1 - S / K) * S)\nExpr\n  head: Symbol call\n  args: Array{Any}((4,))\n    1: Symbol *\n    2: Symbol r\n    3: Expr\n      head: Symbol call\n      args: Array{Any}((3,))\n        1: Symbol -\n        2: Int64 1\n        3: Expr\n          head: Symbol call\n          args: Array{Any}((3,))\n            1: Symbol /\n            2: Symbol S\n            3: Symbol K\n    4: Symbol S\nS\tI\tR\n 67.554431 seconds (125.80 M allocations: 6.555 GiB, 7.52% gc time)\n4.139701895048853e-5\t1.512940651164174\t1.2314284234326383\n  4.132043 seconds (1.85 M allocations: 33.602 MiB, 0.37% gc time)\n3.319429471438334e-5\t1.7926581454821442\t1.4394890708586585\n  4.294201 seconds (1.99 M allocations: 36.084 MiB, 0.54% gc time)\n2.7307348723966148e-5\t2.096234030610046\t1.6601782657100044","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"S I R\n4.139701895048853e-5 1.512940651164174 1.2314284234326383\n3.319429471438334e-5 1.7926581454821442 1.4394890708586585\n2.7307348723966148e-5 2.096234030610046 1.6601782657100044","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We can see from the model output that as the birth rate of the population increases, the size of the SEIR epidemic increases. This example illustrates how we can add capabilities  to models in a way that can augment the ability of scientists to conduct in silico experiments. This augmentation will ultimately enable a faster development of scientific ideas informed by data and simulation.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Hopefully, this example has shown you the goals and scope of this software, the remaining documentation details the various components essential to the creation of this demonstration.","category":"page"}]
}
