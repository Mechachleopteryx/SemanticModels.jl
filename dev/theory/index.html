<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Theory · SemanticModels</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>SemanticModels</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">SemanticModels.jl</a></li><li><a class="toctext" href="../usecases/">Intended Use Cases</a></li><li><a class="toctext" href="../news/">News</a></li><li><a class="toctext" href="../modeltools/">ModelTools</a></li><li><a class="toctext" href="../example/">Example</a></li><li class="current"><a class="toctext" href>Theory</a><ul class="internal"><li><a class="toctext" href="#What-is-a-model?-1">What is a model?</a></li><li><a class="toctext" href="#Analyzing-scientific-models-as-programs-1">Analyzing scientific models as programs</a></li><li><a class="toctext" href="#Categories-for-Science-1">Categories for Science</a></li><li><a class="toctext" href="#Models-in-the-Category-of-Types-1">Models in the Category of Types</a></li><li><a class="toctext" href="#Model-Augmentation-1">Model Augmentation</a></li><li><a class="toctext" href="#Up-Next-1">Up Next</a></li></ul></li><li><a class="toctext" href="../dubstep/">Dubstep</a></li><li><a class="toctext" href="../graph/">Knowledge Graphs</a></li><li><a class="toctext" href="../extraction/">Knowledge Extraction</a></li><li><a class="toctext" href="../validation/">Validation</a></li><li><a class="toctext" href="../library/">Library Reference</a></li><li><a class="toctext" href="../approach/">Approaches</a></li><li><a class="toctext" href="../slides/">Slides</a></li><li><a class="toctext" href="../FluModel/">Flu Model</a></li><li><a class="toctext" href="../contributing/">Contributing</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Theory</a></li></ul><a class="edit-page" href="https://github.com/jpfairbanks/SemanticModels.jl/blob/master/doc/src/theory.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Theory</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Semantic-Modeling-Theory-1" href="#Semantic-Modeling-Theory-1">Semantic Modeling Theory</a></h1><h2><a class="nav-anchor" id="What-is-a-model?-1" href="#What-is-a-model?-1">What is a model?</a></h2><p>The goal of science is to build models of natural phenomena that explain how they work. Scientists build these models by conducting data gathering phenomena and using math to represent these experiments. This sets up a balancing act between the two traits that make for a good scientific model,  it must first match the data collected and also be able to explain the phenomena.</p><p>We can think of fitting the data as a regression problem:</p><div>\[h^* = \min_{h\in {H}} \ell(h(x), y)\]</div><p>and the institutional process of discovery as</p><div>\[\max_{{H}\in \mathcal{M}} expl(h^*)\]</div><p>where <span>$expl$</span> is the explanatory power of a class of models <span>$H$</span>. The explanatory power is some combination of generalization, parsimony, and consistency with the fundamental principles of the field.</p><p>This formulation is notional in the current state of the art, because models are not a well parameterized space. The goal of this project is to identify subspaces that can be parameterized using algebraic structures and represent those subspace symbolically so that computers can represent them and perform optimization.</p><h2><a class="nav-anchor" id="Analyzing-scientific-models-as-programs-1" href="#Analyzing-scientific-models-as-programs-1">Analyzing scientific models as programs</a></h2><p>We can consider three different problems for semantic modeling</p><ol><li><em>Model Modification:</em> Given a model <span>$M$</span> and a transformation <span>$T$</span> construct a new model <span>$T(M)$</span>.</li><li><em>Metamodel construction:</em> Given a set of a possible component models <span>$\mathcal{M}$</span>, known independent variables <span>$\mathcal{I}$</span>, and a set of desired dependent variables <span>$V$</span>, and a set of rules for combining models <span>$R$</span>construct a combination of models <span>$m\in\mathcal{R(M)}$</span> that takes as input <span>$\mathcal{I}$</span> and evaluates the dependent variables <span>$V$</span>.</li><li><em>Model Validation:</em> Given a model <span>$M$</span> and a set of properties <span>$P$</span> and input <span>$x$</span>, determine if the model satisfies all properties <span>$P$</span> when evaluated on <span>$x$</span></li></ol><p>A model <span>$M=(D,R,f)$</span> is a tuple containing a set <span>$D$</span>, called the domain, and a set <span>$R$</span>, called the co-domain with a function <span>$f:D\mapsto R$</span>. If <span>$D$</span> is the cross product of sets <span>$D_1 \times D_2 \cdots D_k$</span> then the and <span>$f = f(x_1\dots x_k)$</span> where <span>$x$</span> are the independent variables of <span>$M$</span>. If <span>$R=R_1\times R_2\cdots r_d$</span> then <span>$R_i$</span> are the dependent variables of <span>$M$</span>. </p><p>A Modeling framework <span>$(U,M,R)$</span>is a universe of sets <span>$U$</span>, class of models <span>$\mathcal{M}$</span>, and a set of rules <span>$R$</span>. Such that the domains and co-domains of all models in <span>$\mathcal{M}$</span> are elements of <span>$\mathcal{U}$</span>, and the class of models is closed under composition when the rules are satisfied. If <span>$R(M_1, \dots M_n)$</span> then <span>$\circ\left(M_1\dots M_n\right)\in \mathcal{M}$</span>. Composition of models is defined as </p><div>\[\circ(M_1, \dots, M_n)=(D_1\times\dots\times D_{n-1}, R_1\times\dots\times R_{n-1}, \circ (f_1(x_1),\dots f_{n_1}(x_{n-1}))\]</div><p>In order to build a useful DAG, a class of models should contain models such as constants, identity, projections, boolean logic, arithmetic, and elementary functions.</p><p>We also need to handle the case of model identification. There are certain models within a framework that are essentially equivalent. For example if <span>$D_1$</span> and <span>$D_2$</span> are sets with homomorphism <span>$g:D_2\mapsto D_1$</span>, then <span>$M_1 = (D_1, R, f) = (D_2, R, f \circ g)$</span> are equivalent as models. In fact <span>$(D_2, D_1, g)$</span> should be included in the class of models in a modeling framework.</p><p>We need a good theoretical foundation for proving theorems about manipulating models and combining them. Categories for Science may be that foundation.</p><p>The work of Evan Patterson on building semantic representations of data science programs is particularly relevant to these modeling questions <a href="https://www.epatters.org/assets/papers/2018-semantic-enrichment-ijcai-demo.pdf &quot;Semantic Representations of Data Science Programs&quot;">SRDSP</a>. <a href="https://www.epatters.org/assets/papers/2018-semantic-enrichment-kdd.pdf &quot;Teaching machines to understand data science code by semantic enrichment of dataflow graphs&quot;">Patterson 2018</a> </p><h2><a class="nav-anchor" id="Categories-for-Science-1" href="#Categories-for-Science-1">Categories for Science</a></h2><p>Dan Spivak wrote a wonderful book, <a href="http://math.mit.edu/~dspivak/CT4S.pdf">CT4S</a>, on category theory for scientists based on his lectures at MIT.</p><blockquote><p>Data gathering is ubiquitous in science. Giant databases are currently being mined for unknown patterns, but in fact there are many (many) known patterns that simply have not been catalogued. Consider the well-known case of medical records. A patient’s medical history is often known by various individual doctor-offices but quite inadequately shared between them. Sharing medical records often means faxing a hand-written note or a filled-in house-created form between offices.</p><p>Similarly, in science there exists substantial expertise making brilliant connections between concepts, but it is being conveyed in silos of English prose known as journal articles. Every scientific journal article has a methods section, but it is almost impossible to read a methods section and subsequently repeat the experiment—the English language is inadequate to precisely and concisely convey what is being done</p></blockquote><p>This is the point of our project, to mine the code and docs for the information necessary to repeat and <em>expand</em> scientific knowledge. Reproducible research is focused on getting the code/data to be shared and runnable with VMs/Docker etc are doing the first step. Can I repeat your analysis? We want to push that to expanding.</p><h3><a class="nav-anchor" id="Ologs-1" href="#Ologs-1">Ologs</a></h3><p>Ontology logs are a diagrammatic approach to formalizing scientific methodologies. They can be used to precisely specify what a scientist is talking about (see <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0024274">Spivak Kent 2012</a> &quot;Ologs: A Categorical Framework for Knowledge Representation.&quot;). </p><p>An olog is composed of types (the boxes) and aspects (the edges). The labels on the edges is the name of the aspect. An aspect is valid if it is a function (1-many relation). </p><p><img src="../img/olog_birthday.png" alt="Birthday olog"/></p><p>We can represent an SIR model as an olog as shown below.</p><p><img src="../img/olog_sir.dot.png" alt="SIR olog"/></p><p>Another category theory representation without the human readable names used in an olog shows a simpler representation.</p><p><img src="../img/category_sir.dot.png" alt="SIR Category"/></p><h2><a class="nav-anchor" id="Models-in-the-Category-of-Types-1" href="#Models-in-the-Category-of-Types-1">Models in the Category of Types</a></h2><p>All programs in a strongly typed language have a set of types and functions that map values between those types. For example the Julia program</p><pre><code class="language-julia">a = 5.0
b = 1
c = 2*a
d = b + c</code></pre><p>Has the types <code>Int, Float</code> and functions <code>*, +</code> which are both binary functions. These types and functions can be represented as a category, where the objects are the types and the morphisms are the functions. We refer to the input type of a function as the domain and the output type as the codomain of the function. Multi-argument functions are represented with tuple types representing their argument. For example <code>+(a::Int,b::Int)::Int</code> is a function <span>$+: Int\times Int -&gt; Int$</span>. These type categories are well studied in the field of Functional Programming. We apply these categories to the study of mathematical models. </p><p>One can use a combination of static and dynamic analysis to extract this category representation from a program and use it to represent the model implemented by the code.</p><p>The most salient consequence of programming language theory is that the more information that a programmer can encode in the type system, the more helpful the programming language can be for improving performance, quality, and correctness.</p><p>We want to leverage the type system to verify the semantic integrity of a model. This is critical when pursuing automatic model modification. Model developers use any number of conventions to encode semantic constraints into their code for example, prefacing all variables that refer to time with a <code>t</code>, such as <code>t_start, s_end</code>. This semantic constraint that all variables named <code>t_</code> are temporal variables is not encoded in the type system because all those variables are still floats. Another example is that vectors of different lengths are incompatible. In a compartment model, the number of initial conditions must match the number of compartments, and the number of parameters may be different. For example in an SIR model there are 3 initial conditions, <span>$S,I,R$</span> and there are 2 parameters <span>$\beta, \gamma$</span>. These vectors are incompatible, you cannot perform arithmetic or comparisons on them directly. Most computational systems employed by scientists will use a runtime check on dimensions to prevent a program from crashing on malformed linear algebra. Scientists rely on this limited from of semantic integrity checking provided by the language. </p><p>Our goal is to extract and encode the maximum amount of information from scientific codes into the type system. The type system is analyzable as a category. Thus we can look at the category of types and analyze the integrety of the programs. For example if there are two types <span>$S,T$</span> and two functions <span>$f,g: S\arrow T$</span> such that <span>$Codom(f) = Codom(g)$</span> but <span>$Range(f) \cap Range(g)$</span>, then we say that the type system is ambiguous in that there are two functions that use disjoint subsets of their common codomain. In order to more fully encode program semantics into the type system, the programmer (or an automated system) should introduce new types to the program to represent these disjoint subsets. <img src="../img/types_sir.dot.png" alt="Ambiguous types in the SIR Model"/> Returning to the SIR model example, the <code>.param</code> and <code>.initial</code> functions both map <code>Problem</code> to <code>Vector{Float}</code> but have disjoint ranges. From our mathematical understanding of the model, we know that parameters and initial conditions are incompatible types of vectors, for one thing the output of <code>.param</code> is length 2 and the output of <code>.initial</code> is length 3. Any program analysis of the model will be hampered by the ambiguity introduced by using the same type to represent two different concepts. On the other hand, <code>.first</code> and <code>.second</code> have overlapping ranges and are comparable as times.</p><p><img src="../img/types_sir_unambig.dot.png" alt="Unambiguous types in the SIR Model"/></p><p>This is an example of how PL theory ideas can improve the analysis of computational models.</p><p>Teaching the type system about agent based modeling. In the example notebook <code>/examples/agenttypes2.jl</code> you can see how to embed model structure into the julia type system. That example uses two versions of the same agent based model of disease. In the first implementation, the agents have states represented by the julia type <code>:Symbol</code> with values <code>:S, :I, :R</code>, and in the second, more refined implementation, the agent&#39;s states are represented by the singleton types <code>Susceptible, Infected, Recovered</code> with values, <code>Susceptible(), Infected(), Recovered()</code>. The model is the same, but the type system contains more information about the execution of the model. For example the julia type system knows what the possible state transitions are based in the second implementation, while the first model has a black box of <code>:Symbol</code>s that are not distinguishable in program analysis. </p><p>The original type graph <span>$g$</span> shows how the model works. <img src="../img/exampletypegraph.dot.svg" alt="Using Symbol values to represent states"/></p><p>The refined model has a typegraph <span>$G$</span>, which includes the new singleton types as well as different tuple types.</p><p>We can establish a graph homomorphism <span>$\phi: G \to g$</span> such that <span>$\phi(v) = v$</span> for all <span>$v$</span> in <span>$V(g) \cap V(G)$</span>. The following figure shows this homomorphism by drawing vertices <span>$v\in G$</span> with the same color as <span>$\phi(v)\in g$</span>. <img src="../img/typegraphmorphism.dot.svg" alt="Using Symbol values to represent states"/> Within this typegraph we have a subgraph that contains the state transitions for the agents. We can draw this subgraph separately to show how the compiler has been taught to understand the semantics of the model. <img src="../img/type_DFA.dot.svg" alt="The typegraph understand the transition graph of the agents"/></p><p>The embedding of model semantics into the type system enables programs to reason over the behavior of the models.</p><h2><a class="nav-anchor" id="Model-Augmentation-1" href="#Model-Augmentation-1">Model Augmentation</a></h2><p>Scientists build novel models from old models and the approach provided by SemanticModels has several benefits. We think that</p><h3><a class="nav-anchor" id="Abstraction-1" href="#Abstraction-1">Abstraction</a></h3><p>Modeling operations have similarities across domains and we can build general model augmentations that let scientists translate operations from one domain to another. The code that defines transformations is also &quot;general modeling code&quot; so our abstraction is closed.</p><h3><a class="nav-anchor" id="Symbolification-1" href="#Symbolification-1">Symbolification</a></h3><p>The geometric perspective is really great for proving things about shapes, but developing algorithms requires adopting a symbolic perspectrive like algebra. Our example of polynomial regression connects here because we are able to write algorithms for model selection that leverage the symbolic nature of the transformations. In fact we can give examples of model selection in terms of <a href="https://en.wikipedia.org/wiki/Ideal_(ring_theory)">ideals</a>. The algebra of the transformation space is a place for algorithms on the model space.</p><ul><li>Open question: Can we lift the greatest common divisor of polynomials to be the &quot;greatest common submodel&quot; for least squares regression? If so, does the euclidean algorithm for GCD give a model selection algorithm?</li></ul><h3><a class="nav-anchor" id="Metaprogramming-for-Science-1" href="#Metaprogramming-for-Science-1">Metaprogramming for Science</a></h3><p>Scientific models are so diverse that we need the full flexibility of code as input for our modeling framework. This is somewhat inherent to the scientific process. Scientists who are pushing the field in modeling are often inventing or applying new algorithms that are capable of solving those models. Also the first formulation of a new model is not the most elegant and so we need to be able to operate on ad-hoc models before we understand the class of models well enough for an elegant formulation to get added to the modeling framework.</p><p>Metaprogramming is about writing programs that write programs, so it makes sense that metamodeling is about writing models that write models. In order to write models that can generate models, there needs to be a compact and parsimonious representation of the model for algorithms to manipulate. As we have seen in writing our post-hoc modeling framework, scientific models diverse and hard to summarize, however the transformations that can be applied to a model while preserving its validity within the class of models is often much more structured than the models themselves. This is why we think that metamodels will work on these transformations instead of on the models directly.</p><p>Again we look to our polynomial regression problem, with only two transformations you can generate the entire class of polynomial regression problems from a model that computes linear regression. Algorithms that work on the polynomial regression models directly would have to manage a lot of complexity around arguments, data flow, conditional logic, I/O. But in the transformation state there is just <code>f(x) -&gt; xf(x)</code> and <code>f(x) -&gt; f(x) + 1</code> which are simple transformations.</p><p>By representing complex models as transformations of a base model, under an algebra of transformations, we are able to make metaprogramming for science much easier.</p><h3><a class="nav-anchor" id="Model-Synthesis-1" href="#Model-Synthesis-1">Model Synthesis</a></h3><p>One goal of the program is to get to the point where we can automatically infer how to combine models based on what they compute. The idea of model circuits based on signal flow graphs (see #137) is that you can statically connect models with a wiring diagram and then evaluate the diagram to compute the combined model. General DAGs are hard to compose and are typically written with either a declarative DAG language or an imperative DAG building library.</p><p><a href="http://math.mit.edu/~dspivak/teaching/sp18/7Sketches.pdf">Fong and Spivak 2018</a> shows how to express signal processing and controls problems in a graphical language based on categories of products and permutations category or <em>props</em>.</p><p>In this category, computations can be specified with diagrams. The following figure shows a diagram representing a classical controls problem. These diagrams are shown to have the same functorial semantics as matrices. <img src="../img/controls_signal_graph.png" alt="Diagram for computing PID control"/></p><p>We think that the category theory approach of props is the right approach. This approach leads to diagrams with precise semantics for representing general purpose computation networks. These networks will be specified with code that combines the sum and product operation in a hierarchical expression just like regular code. Thus the code that makes the diagrams is a model that we can augment with our current ModelTools techniques.</p><p>These &quot;model circuits&quot; can thus be built out of code resulting from transformations on code that builds a base circuit. Which establishes tools for creating high level transformations on circuits. We can then define the input and output wires as the modeling concepts we know and want to know and then build algorithms for solving for the circuit that gets from the inputs to the outputs. We suspect a dynamic programming approach to recursively bring the inputs and outputs closer together will solve this problem. </p><p>Algorithms that do model synthesis will need to account for the conceptual knowledge that is captured in the text and documentation of scientific software. </p><p>Once we have a mathematically sound way to represent combinations of models, we must address the practical aspects of model synthesis. We are endeavoring to augment and automate scientific workflows by meaningfully pruning the set of possible metamodels. When scientists design models, they must confront:</p><ol><li>known unknowns: parameters or components they are aware of, but about which there may be uncertainty regarding</li></ol><p>values or best practices</p><ol><li>unknown unknowns: parameters or components the scientist does not yet know about, or deems unnecessary for the</li></ol><p>current modeling task</p><p>In a broader/more abstract sense, our potential contributions to &quot;AI for Science&quot; are related to <a href="https://en.wikipedia.org/wiki/Uncertainty_quantification">uncertainty quantification</a>, and we endeavor to help scientists assess and reduce both aleatoric and epistemic uncertainty. By working within a specific domain and comparing known existing models, we can help a scientist make progress on (1). By integrating across domains at semantically meaningful endpoints, we can help a scientist make progress on (2).</p><p>Some tasks that can be automated require understanding both the existing scientific software ecosystem and what the scientist it trying to compute. For example, scientific productivity can be enhanced by answering questions like, &quot;If I know X and Y but want to know Z, what software tools can solve this for me, and how do I use them?&quot; This is a task that <em>de novo</em> modeling frameworks cannot answer, because the existing literature was not developed with the new framework.</p><h2><a class="nav-anchor" id="Up-Next-1" href="#Up-Next-1">Up Next</a></h2><p>The ideas of representing models as categories and functors that preserve different aspects of the structures we have is compelling. We believe this is the best theoretical foundation for the model augmentation and model synthesis components of SemanticModels.</p><footer><hr/><a class="previous" href="../example/"><span class="direction">Previous</span><span class="title">Example</span></a><a class="next" href="../dubstep/"><span class="direction">Next</span><span class="title">Dubstep</span></a></footer></article></body></html>
