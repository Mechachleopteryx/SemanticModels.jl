# Model Validation with Dynamic Analysis

## Method for validating code
1. Collect a sample of known “good” inputs matched with their corresponding “good” outputs, and a sample of known “bad” inputs matched with their corresponding “bad” outputs.
    1. “Good” here is defined as: given these input(s), the model output(s)/prediction(s) correspond to expected or observed empirical reality, within an acceptable tolerance/range re: error. 
    1. Edge cases to note but not heavily consider @ this point:
        1. For “good” input to “bad” output, we just corrupt the “good” inputs at various points along the computation.
        1. If assumption that code is correct and does not contain bugs holds, then it is ok to assume we will not observe “bad” input to “good” output. 
1. Run the simulation to collect a sample of known good outputs.
1. Instrument the code to log all SSA assignments from the function calls
1. Train an RNN on the sequence of [(func, var, val)...] where the labels are “good input/bad input”
    1. By definition, any SSA “sentence” generated by a known “good” input is assumed to be “good”; thus, these labels essentially propagate down. 
1. Partial evaluations of RNN give you the “where did it go wrong. 

In step 1: for an analytically tractable model, we can generate an arbitrarily large collection of known good and bad inputs.

## IO interface

We need to build a Tensorflow.jl or Flux.jl RNN model that will work on sequences `[(func, var, val, type)]` and produce labels of good/bad

1. Traces will be communicated 1 trace per file
1. Each line is a tuple module.func, var,val,type with quotes as necessary for CSV
1. The files will be organized into folders program/{good,bad}/tracenumber.txt
1. Traces will be variable length
